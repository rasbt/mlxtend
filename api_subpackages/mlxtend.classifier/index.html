<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Sebastian Raschka">
        <link rel="canonical" href="http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.classifier/">
        <link rel="shortcut icon" href="../../favicon.ico">
        
        <title>Mlxtend.classifier - mlxtend</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="../../cinder/css/base.css" rel="stylesheet">
        <link href="../../cinder/css/bootstrap-custom.css" rel="stylesheet">
        <link href="../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../cinder/css/cinder.css" rel="stylesheet">
        <link href="../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../../cinder/css/highlight.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">mlxtend</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../USER_GUIDE_INDEX/">User Guide Index</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">classifier</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/classifier/Adaline/">Adaptive Linear Neuron -- Adaline</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/EnsembleVoteClassifier/">EnsembleVoteClassifier</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/LogisticRegression/">Logistic Regression</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/MultiLayerPerceptron/">Neural Network - Multilayer Perceptron</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/Perceptron/">Perceptron</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/SoftmaxRegression/">Softmax Regression</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/StackingClassifier/">StackingClassifier</a>
</li>
            
<li >
    <a href="../../user_guide/classifier/StackingCVClassifier/">StackingCVClassifier</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">cluster</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/cluster/Kmeans/">Kmeans</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">data</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/data/autompg_data/">Auto MPG</a>
</li>
            
<li >
    <a href="../../user_guide/data/boston_housing_data/">Boston Housing Data</a>
</li>
            
<li >
    <a href="../../user_guide/data/iris_data/">Iris Dataset</a>
</li>
            
<li >
    <a href="../../user_guide/data/loadlocal_mnist/">Load the MNIST Dataset from Local Files</a>
</li>
            
<li >
    <a href="../../user_guide/data/make_multiplexer_dataset/">Make Multiplexer Dataset</a>
</li>
            
<li >
    <a href="../../user_guide/data/mnist_data/">MNIST Dataset</a>
</li>
            
<li >
    <a href="../../user_guide/data/three_blobs_data/">Three Blobs Dataset</a>
</li>
            
<li >
    <a href="../../user_guide/data/wine_data/">Wine Dataset</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">evaluate</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/evaluate/bootstrap/">Bootstrap</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/bootstrap_point632_score/">bootstrap_point632_score</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/BootstrapOutOfBag/">BootstrapOutOfBag</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/cochrans_q/">Cochran's Q Test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/combined_ftest_5x2cv/">5x2cv combined *F* test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/confusion_matrix/">Confusion Matrix</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/feature_importance_permutation/">Feature Importance Permutation</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/ftest/">F-Test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/lift_score/">Lift Score</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/mcnemar_table/">Contigency Table for McNemar's Test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/mcnemar_tables/">Contigency Tables for McNemar's Test and Cochran's Q Test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/mcnemar/">McNemar's Test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/paired_ttest_5x2cv/">5x2cv paired *t* test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/paired_ttest_kfold_cv/">K-fold cross-validated paired *t* test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/paired_ttest_resampled/">Resampled paired *t* test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/permutation_test/">Permutation Test</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/PredefinedHoldoutSplit/">PredefinedHoldoutSplit</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/RandomHoldoutSplit/">RandomHoldoutSplit</a>
</li>
            
<li >
    <a href="../../user_guide/evaluate/scoring/">Scoring</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">feature_extraction</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/feature_extraction/LinearDiscriminantAnalysis/">Linear Discriminant Analysis</a>
</li>
            
<li >
    <a href="../../user_guide/feature_extraction/PrincipalComponentAnalysis/">Principal Component Analysis</a>
</li>
            
<li >
    <a href="../../user_guide/feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">feature_selection</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/feature_selection/ColumnSelector/">ColumnSelector</a>
</li>
            
<li >
    <a href="../../user_guide/feature_selection/ExhaustiveFeatureSelector/">Exhaustive Feature Selector</a>
</li>
            
<li >
    <a href="../../user_guide/feature_selection/SequentialFeatureSelector/">Sequential Feature Selector</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">file_io</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/file_io/find_filegroups/">Find Filegroups</a>
</li>
            
<li >
    <a href="../../user_guide/file_io/find_files/">Find Files</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">frequent_patterns</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/frequent_patterns/apriori/">Apriori</a>
</li>
            
<li >
    <a href="../../user_guide/frequent_patterns/association_rules/">Association rules</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">general concepts</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/general_concepts/activation-functions/">Activation Functions for Artificial Neural Networks</a>
</li>
            
<li >
    <a href="../../user_guide/general_concepts/gradient-optimization/">Gradient Descent and Stochastic Gradient Descent</a>
</li>
            
<li >
    <a href="../../user_guide/general_concepts/linear-gradient-derivative/">Deriving the Gradient Descent Rule for Linear Regression and Adaline</a>
</li>
            
<li >
    <a href="../../user_guide/general_concepts/regularization-linear/">Regularization of Generalized Linear Models</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">image</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/image/extract_face_landmarks/">Extract Face Landmarks</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">math</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/math/num_combinations/">Compute the Number of Combinations</a>
</li>
            
<li >
    <a href="../../user_guide/math/num_permutations/">Compute the Number of Permutations</a>
</li>
            
<li >
    <a href="../../user_guide/math/vectorspace_dimensionality/">Vectorspace Dimensionality</a>
</li>
            
<li >
    <a href="../../user_guide/math/vectorspace_orthonormalization/">Vectorspace Orthonormalization</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">plotting</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/plotting/category_scatter/">Scatterplot with Categories</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/checkerboard_plot/">Checkerboard Plot</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/ecdf/">Empirical Cumulative Distribution Function Plot</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/enrichment_plot/">Enrichment Plot</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/plot_confusion_matrix/">Confusion Matrix</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/plot_decision_regions/">Plotting Decision Regions</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/plot_learning_curves/">Plotting Learning Curves</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/plot_linear_regression/">Linear Regression Plot</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/plot_sequential_feature_selection/">Plot Sequential Feature Selection</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/scatterplotmatrix/">Scatter Plot Matrix</a>
</li>
            
<li >
    <a href="../../user_guide/plotting/stacked_barplot/">Stacked Barplot</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">preprocessing</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/preprocessing/CopyTransformer/">CopyTransformer</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/DenseTransformer/">DenseTransformer</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/MeanCenterer/">Mean Centerer</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/minmax_scaling/">MinMax Scaling</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/one-hot_encoding/">One hot encoding</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/shuffle_arrays_unison/">Shuffle Arrays in Unison</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/standardize/">Standardize</a>
</li>
            
<li >
    <a href="../../user_guide/preprocessing/TransactionEncoder/">TransactionEncoder</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">regressor</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/regressor/LinearRegression/">LinearRegression</a>
</li>
            
<li >
    <a href="../../user_guide/regressor/StackingCVRegressor/">StackingCVRegressor</a>
</li>
            
<li >
    <a href="../../user_guide/regressor/StackingRegressor/">StackingRegressor</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">text</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/text/generalize_names/">Generalize Names</a>
</li>
            
<li >
    <a href="../../user_guide/text/generalize_names_duplcheck/">Generalize Names & Duplicate Checking</a>
</li>
            
<li >
    <a href="../../user_guide/text/tokenizer/">Tokenizer</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">utils</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../user_guide/utils/Counter/">Counter</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li class="active">
    <a href="./">Mlxtend.classifier</a>
</li>
                                    
<li >
    <a href="../mlxtend.cluster/">Mlxtend.cluster</a>
</li>
                                    
<li >
    <a href="../mlxtend.data/">Mlxtend.data</a>
</li>
                                    
<li >
    <a href="../mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>
                                    
<li >
    <a href="../mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>
                                    
<li >
    <a href="../mlxtend.feature_selection/">Mlxtend.feature selection</a>
</li>
                                    
<li >
    <a href="../mlxtend.file_io/">Mlxtend.file io</a>
</li>
                                    
<li >
    <a href="../mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
</li>
                                    
<li >
    <a href="../mlxtend.plotting/">Mlxtend.plotting</a>
</li>
                                    
<li >
    <a href="../mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>
                                    
<li >
    <a href="../mlxtend.regressor/">Mlxtend.regressor</a>
</li>
                                    
<li >
    <a href="../mlxtend.text/">Mlxtend.text</a>
</li>
                                    
<li >
    <a href="../mlxtend.utils/">Mlxtend.utils</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../installation/">Installation</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../changelog.md">Release Notes</a>
</li>
                                    
<li >
    <a href="../../contributing.md">How To Contribute</a>
</li>
                                    
<li >
    <a href="../../contributors/">Contributors</a>
</li>
                                    
<li >
    <a href="../../license/">License</a>
</li>
                                    
<li >
    <a href="../../cite/">Citing Mlxtend</a>
</li>
                                    
<li >
    <a href="../../discuss/">Discuss</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../../user_guide/utils/Counter/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../mlxtend.cluster/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/rasbt/mlxtend/edit/master/docs/api_subpackages/mlxtend.classifier.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#adaline">Adaline</a></li>
            <li><a href="#methods">Methods</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause">License: BSD 3 clause</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_1">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_1">License: BSD 3 clause</a></li>
            <li><a href="#ensemblevoteclassifier">EnsembleVoteClassifier</a></li>
            <li><a href="#logisticregression">LogisticRegression</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_2">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_2">License: BSD 3 clause</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_3">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_3">License: BSD 3 clause</a></li>
            <li><a href="#multilayerperceptron">MultiLayerPerceptron</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_4">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_4">License: BSD 3 clause</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_5">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_5">License: BSD 3 clause</a></li>
            <li><a href="#perceptron">Perceptron</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_6">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_6">License: BSD 3 clause</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_7">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_7">License: BSD 3 clause</a></li>
            <li><a href="#softmaxregression">SoftmaxRegression</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_8">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_8">License: BSD 3 clause</a></li>
        <li class="main "><a href="#author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_9">Author: Gael Varoquaux &#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></li>
        <li class="main "><a href="#license-bsd-3-clause_9">License: BSD 3 clause</a></li>
            <li><a href="#stackingcvclassifier">StackingCVClassifier</a></li>
            <li><a href="#stackingclassifier">StackingClassifier</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p>mlxtend version: 0.14.0dev </p>
<h2 id="adaline">Adaline</h2>
<p><em>Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)</em></p>
<p>ADAptive LInear NEuron classifier.</p>
<p>Note that this implementation of Adaline expects binary class labels
in {0, 1}.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.01)</p>
<p>solver rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>minibatches</code> : int (default: None)</p>
<p>The number of minibatches for gradient-based optimization.
If None: Normal Equations (closed-form solution)
If 1: Gradient Descent learning
If len(y): Stochastic Gradient Descent (SGD) online learning
If 1 &lt; minibatches &lt; len(y): SGD Minibatch learning</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr if not solver='normal equation'
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>Sum of squared errors after each epoch.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/">http://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/</a></p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.'</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause">License: BSD 3 clause</h1>
</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_1">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_1">License: BSD 3 clause</h1>
<h2 id="ensemblevoteclassifier">EnsembleVoteClassifier</h2>
<p><em>EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, refit=True)</em></p>
<p>Soft Voting/Majority Rule classifier for scikit-learn estimators.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>clfs</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>VotingClassifier</code> will fit clones
of those original classifiers that will
be stored in the class attribute
<code>self.clfs_</code> if <code>refit=True</code> (default).</p>
</li>
<li>
<p><code>voting</code> : str, {'hard', 'soft'} (default='hard')</p>
<p>If 'hard', uses predicted class labels for majority rule voting.
Else if 'soft', predicts the class label based on the argmax of
the sums of the predicted probalities, which is recommended for
an ensemble of well-calibrated classifiers.</p>
</li>
<li>
<p><code>weights</code> : array-like, shape = [n_classifiers], optional (default=<code>None</code>)</p>
<p>Sequence of weights (<code>float</code> or <code>int</code>) to weight the occurances of
predicted class labels (<code>hard</code> voting) or class probabilities
before averaging (<code>soft</code> voting). Uses uniform weights if <code>None</code>.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the clf being fitted
- <code>verbose=2</code>: Prints info about the parameters of the clf being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying clf to
self.verbose - 2</p>
</li>
<li>
<p><code>refit</code> : bool (default: True)</p>
<p>Refits classifiers in <code>clfs</code> if True; uses references to the <code>clfs</code>,
otherwise (assumes that the classifiers were already fit).
Note: refit=False is incompatible to mist scikit-learn wrappers!
For instance, if any form of cross-validation is performed
this would require the re-fitting classifiers to training folds, which
would raise a NotFitterError if refit=False.
(New in mlxtend v0.6.)</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>classes_</code> : array-like, shape = [n_predictions]</p>
</li>
<li>
<p><code>clf</code> : array-like, shape = [n_predictions]</p>
<p>The unmodified input classifiers</p>
</li>
<li>
<p><code>clf_</code> : array-like, shape = [n_predictions]</p>
<p>Fitted clones of the input classifiers</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB
&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt; from mlxtend.sklearn import EnsembleVoteClassifier
&gt;&gt;&gt; clf1 = LogisticRegression(random_seed=1)
&gt;&gt;&gt; clf2 = RandomForestClassifier(random_seed=1)
&gt;&gt;&gt; clf3 = GaussianNB()
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2])
&gt;&gt;&gt; eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
... voting='hard', verbose=1)
&gt;&gt;&gt; eclf1 = eclf1.fit(X, y)
&gt;&gt;&gt; print(eclf1.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt; eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')
&gt;&gt;&gt; eclf2 = eclf2.fit(X, y)
&gt;&gt;&gt; print(eclf2.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt; eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
...                          voting='soft', weights=[2,1,1])
&gt;&gt;&gt; eclf3 = eclf3.fit(X, y)
&gt;&gt;&gt; print(eclf3.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt;
</code></pre>
<p>For more usage examples, please see
<a href="http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/">http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/</a></p>
<h3 id="methods_1">Methods</h3>
<hr>

<p><em>fit(X, y, sample_weight=None)</em></p>
<p>Learn weight coefficients from training data for each classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights passed as sample_weights to each regressor
in the regressors list as well as the meta_regressor.
Raises error if some regressor does not support
sample_weight in the fit() method.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict class labels for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>maj</code> : array-like, shape = [n_samples]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>avg</code> : array-like, shape = [n_samples, n_classes]</p>
<p>Weighted average probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Return class labels or probabilities for X for each estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>If</code>voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]</p>
<p>Class probabilties calculated by each classifier.</p>
</li>
<li>
<p><code>If</code>voting='hard'`` : array-like = [n_classifiers, n_samples]</p>
<p>Class labels predicted by each classifier.</p>
</li>
</ul>
<h2 id="logisticregression">LogisticRegression</h2>
<p><em>LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0)</em></p>
<p>Logistic regression classifier.</p>
<p>Note that this implementation of Logistic Regression
expects binary class labels in {0, 1}.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.01)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>l2_lambda</code> : float</p>
<p>Regularization parameter for L2 regularization.
No regularization if l2_lambda=0.0.</p>
</li>
<li>
<p><code>minibatches</code> : int (default: 1)</p>
<p>The number of minibatches for gradient-based optimization.
If 1: Gradient Descent learning
If len(y): Stochastic Gradient Descent (SGD) online learning
If 1 &lt; minibatches &lt; len(y): SGD Minibatch learning</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>List of floats with cross_entropy cost (sgd or gd) for every
epoch.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/">http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/</a></p>
<h3 id="methods_2">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.'</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_2">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_2">License: BSD 3 clause</h1>
</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities of X from the net input.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>Class 1 probability</code> : float</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_3">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_3">License: BSD 3 clause</h1>
<h2 id="multilayerperceptron">MultiLayerPerceptron</h2>
<p><em>MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0)</em></p>
<p>Multi-layer perceptron classifier with logistic sigmoid activations</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.5)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>hidden_layers</code> : list (default: [50])</p>
<p>Number of units per hidden layer. By default 50 units in the
first hidden layer. At the moment only 1 hidden layer is supported</p>
</li>
<li>
<p><code>n_classes</code> : int (default: None)</p>
<p>A positive integer to declare the number of class labels
if not all class labels are present in a partial training set.
Gets the number of class labels automatically if None.</p>
</li>
<li>
<p><code>l1</code> : float (default: 0.0)</p>
<p>L1 regularization strength</p>
</li>
<li>
<p><code>l2</code> : float (default: 0.0)</p>
<p>L2 regularization strength</p>
</li>
<li>
<p><code>momentum</code> : float (default: 0.0)</p>
<p>Momentum constant. Factor multiplied with the
gradient of the previous epoch t-1 to improve
learning speed
w(t) := w(t) - (grad(t) + momentum * grad(t-1))</p>
</li>
<li>
<p><code>decrease_const</code> : float (default: 0.0)</p>
<p>Decrease constant. Shrinks the learning rate
after each epoch via eta / (1 + epoch*decrease_const)</p>
</li>
<li>
<p><code>minibatches</code> : int (default: 1)</p>
<p>Divide the training data into <em>k</em> minibatches
for accelerated stochastic gradient descent learning.
Gradient Descent Learning if <code>minibatches</code> = 1
Stochastic Gradient Descent learning if <code>minibatches</code> = len(y)
Minibatch learning if <code>minibatches</code> &gt; 1</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape=[n_features, n_classes]</p>
<p>Weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1D-array, shape=[n_classes]</p>
<p>Bias units after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>List of floats; the mean categorical cross entropy
cost after each epoch.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/">http://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/</a></p>
<h3 id="methods_3">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.'</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_4">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_4">License: BSD 3 clause</h1>
</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities of X from the net input.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>Class probabilties</code> : array-like, shape= [n_samples, n_classes]</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_5">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_5">License: BSD 3 clause</h1>
<h2 id="perceptron">Perceptron</h2>
<p><em>Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)</em></p>
<p>Perceptron classifier.</p>
<p>Note that this implementation of the Perceptron expects binary class labels
in {0, 1}.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.1)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Number of passes over the training dataset.
Prior to each epoch, the dataset is shuffled to prevent cycles.</p>
</li>
<li>
<p><code>random_seed</code> : int</p>
<p>Random state for initializing random weights and shuffling.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>Number of misclassifications in every epoch.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/">http://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/</a></p>
<h3 id="methods_4">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.'</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_6">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_6">License: BSD 3 clause</h1>
</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_7">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_7">License: BSD 3 clause</h1>
<h2 id="softmaxregression">SoftmaxRegression</h2>
<p><em>SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0)</em></p>
<p>Softmax regression classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>eta</code> : float (default: 0.01)</p>
<p>Learning rate (between 0.0 and 1.0)</p>
</li>
<li>
<p><code>epochs</code> : int (default: 50)</p>
<p>Passes over the training dataset.
Prior to each epoch, the dataset is shuffled
if <code>minibatches &gt; 1</code> to prevent cycles in stochastic gradient descent.</p>
</li>
<li>
<p><code>l2</code> : float</p>
<p>Regularization parameter for L2 regularization.
No regularization if l2=0.0.</p>
</li>
<li>
<p><code>minibatches</code> : int (default: 1)</p>
<p>The number of minibatches for gradient-based optimization.
If 1: Gradient Descent learning
If len(y): Stochastic Gradient Descent (SGD) online learning
If 1 &lt; minibatches &lt; len(y): SGD Minibatch learning</p>
</li>
<li>
<p><code>n_classes</code> : int (default: None)</p>
<p>A positive integer to declare the number of class labels
if not all class labels are present in a partial training set.
Gets the number of class labels automatically if None.</p>
</li>
<li>
<p><code>random_seed</code> : int (default: None)</p>
<p>Set random state for shuffling and initializing the weights.</p>
</li>
<li>
<p><code>print_progress</code> : int (default: 0)</p>
<p>Prints progress in fitting to stderr.
0: No output
1: Epochs elapsed and cost
2: 1 plus time elapsed
3: 2 plus estimated time until completion</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>w_</code> : 2d-array, shape={n_features, 1}</p>
<p>Model weights after fitting.</p>
</li>
<li>
<p><code>b_</code> : 1d-array, shape={1,}</p>
<p>Bias unit after fitting.</p>
</li>
<li>
<p><code>cost_</code> : list</p>
<p>List of floats, the average cross_entropy for each epoch.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/">http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/</a></p>
<h3 id="methods_5">Methods</h3>
<hr>

<p><em>fit(X, y, init_params=True)</em></p>
<p>Learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>init_params</code> : bool (default: True)</p>
<p>Re-initializes model parameters prior to fitting.
Set False to continue training with weights from
a previous model fitting.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.'</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_8">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_8">License: BSD 3 clause</h1>
</li>
</ul>
<hr>

<p><em>predict(X)</em></p>
<p>Predict targets from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>target_values</code> : array-like, shape = [n_samples]</p>
<p>Predicted target values.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities of X from the net input.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>Class probabilties</code> : array-like, shape= [n_samples, n_classes]</li>
</ul>
<hr>

<p><em>score(X, y)</em></p>
<p>Compute the prediction accuracy</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values (true class labels).</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>acc</code> : float</p>
<p>The prediction accuracy as a float
between 0.0 and 1.0 (perfect score).</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<p>adapted from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py</p>
<h1 id="author-gael-varoquaux-amp103amp97amp101amp108amp46amp118amp97amp114amp111amp113amp117amp97amp117amp120amp64amp110amp111amp114amp109amp97amp108amp101amp115amp117amp112amp46amp111amp114amp103_9">Author: Gael Varoquaux <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;">&#103;&#97;&#101;&#108;&#46;&#118;&#97;&#114;&#111;&#113;&#117;&#97;&#117;&#120;&#64;&#110;&#111;&#114;&#109;&#97;&#108;&#101;&#115;&#117;&#112;&#46;&#111;&#114;&#103;</a></h1>
<h1 id="license-bsd-3-clause_9">License: BSD 3 clause</h1>
<h2 id="stackingcvclassifier">StackingCVClassifier</h2>
<p><em>StackingCVClassifier(classifiers, meta_classifier, use_probas=False, cv=2, use_features_in_secondary=False, stratify=True, shuffle=True, verbose=0, store_train_meta_features=False, use_clones=True)</em></p>
<p>A 'Stacking Cross-Validation' classifier for scikit-learn estimators.</p>
<p>New in mlxtend v0.4.3</p>
<p><strong>Notes</strong></p>
<p>The StackingCVClassifier uses scikit-learn's check_cv
internally, which doesn't support a random seed. Thus
NumPy's random seed need to be specified explicitely for
deterministic behavior, for instance, by setting
np.random.seed(RANDOM_SEED)
prior to fitting the StackingCVClassifier</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>classifiers</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>StackingCVClassifer</code> will fit clones
of these original classifiers that will
be stored in the class attribute <code>self.clfs_</code>.</p>
</li>
<li>
<p><code>meta_classifier</code> : object</p>
<p>The meta-classifier to be fitted on the ensemble of
classifiers</p>
</li>
<li>
<p><code>use_probas</code> : bool (default: False)</p>
<p>If True, trains meta-classifier based on predicted probabilities
instead of class labels.</p>
</li>
<li>
<p><code>cv</code> : int, cross-validation generator or an iterable, optional (default: 2)</p>
<p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:
- None, to use the default 2-fold cross validation,
- integer, to specify the number of folds in a <code>(Stratified)KFold</code>,
- An object to be used as a cross-validation generator.
- An iterable yielding train, test splits.
For integer/None inputs, it will use either a <code>KFold</code> or
<code>StratifiedKFold</code> cross validation depending the value of <code>stratify</code>
argument.</p>
</li>
<li>
<p><code>use_features_in_secondary</code> : bool (default: False)</p>
<p>If True, the meta-classifier will be trained both on the predictions
of the original classifiers and the original dataset.
If False, the meta-classifier will be trained only on the predictions
of the original classifiers.</p>
</li>
<li>
<p><code>stratify</code> : bool (default: True)</p>
<p>If True, and the <code>cv</code> argument is integer it will follow a stratified
K-Fold cross validation technique. If the <code>cv</code> argument is a specific
cross validation technique, this argument is omitted.</p>
</li>
<li>
<p><code>shuffle</code> : bool (default: True)</p>
<p>If True,  and the <code>cv</code> argument is integer, the training data will be
shuffled at fitting stage prior to cross-validation. If the <code>cv</code>
argument is a specific cross validation technique, this argument is
omitted.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the regressor being fitted
and which fold is currently being used for fitting
- <code>verbose=2</code>: Prints info about the parameters of the
regressor being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying regressor to
self.verbose - 2</p>
</li>
<li>
<p><code>store_train_meta_features</code> : bool (default: False)</p>
<p>If True, the meta-features computed from the training data used
for fitting the meta-classifier stored in the
<code>self.train_meta_features_</code> array, which can be
accessed after calling <code>fit</code>.</p>
</li>
<li>
<p><code>use_clones</code> : bool (default: True)</p>
<p>Clones the classifiers for stacking classification if True (default)
or else uses the original ones, which will be refitted on the dataset
upon calling the <code>fit</code> method. Hence, if use_clones=True, the original
input classifiers will remain unmodified upon using the
StackingCVClassifier's <code>fit</code> method.
Setting <code>use_clones=False</code> is
recommended if you are working with estimators that are supporting
the scikit-learn fit/predict API interface but are not compatible
to scikit-learn's <code>clone</code> function.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>clfs_</code> : list, shape=[n_classifiers]</p>
<p>Fitted classifiers (clones of the original classifiers)</p>
</li>
<li>
<p><code>meta_clf_</code> : estimator</p>
<p>Fitted meta-classifier (clone of the original meta-estimator)</p>
</li>
<li>
<p><code>train_meta_features</code> : numpy array, shape = [n_samples, n_classifiers]</p>
<p>meta-features for training data, where n_samples is the
number of samples
in training data and n_classifiers is the number of classfiers.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/</a></p>
<h3 id="methods_6">Methods</h3>
<hr>

<p><em>fit(X, y, groups=None, sample_weight=None)</em></p>
<p>Fit ensemble classifers and the meta-classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : numpy array, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>groups</code> : numpy array/None, shape = [n_samples]</p>
<p>The group that each sample belongs to. This is used by specific
folding strategies such as GroupKFold()</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights passed as sample_weights to each regressor
in the regressors list as well as the meta_regressor.
Raises error if some regressor does not support
sample_weight in the fit() method.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict target values for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>labels</code> : array-like, shape = [n_samples]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_meta_features(X)</em></p>
<p>Get meta-features of test-data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array, shape = [n_samples, n_features]</p>
<p>Test vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>meta-features</code> : numpy array, shape = [n_samples, n_classifiers]</p>
<p>Returns the meta-features for test data.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>proba</code> : array-like, shape = [n_samples, n_classes]</p>
<p>Probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<h2 id="stackingclassifier">StackingClassifier</h2>
<p><em>StackingClassifier(classifiers, meta_classifier, use_probas=False, average_probas=False, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True)</em></p>
<p>A Stacking classifier for scikit-learn estimators for classification.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>classifiers</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>StackingClassifer</code> will fit clones
of these original classifiers that will
be stored in the class attribute
<code>self.clfs_</code>.</p>
</li>
<li>
<p><code>meta_classifier</code> : object</p>
<p>The meta-classifier to be fitted on the ensemble of
classifiers</p>
</li>
<li>
<p><code>use_probas</code> : bool (default: False)</p>
<p>If True, trains meta-classifier based on predicted probabilities
instead of class labels.</p>
</li>
<li>
<p><code>average_probas</code> : bool (default: False)</p>
<p>Averages the probabilities as meta features if True.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the regressor being fitted
- <code>verbose=2</code>: Prints info about the parameters of the
regressor being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying regressor to
self.verbose - 2</p>
</li>
<li>
<p><code>use_features_in_secondary</code> : bool (default: False)</p>
<p>If True, the meta-classifier will be trained both on the predictions
of the original classifiers and the original dataset.
If False, the meta-classifier will be trained only on the predictions
of the original classifiers.</p>
</li>
<li>
<p><code>store_train_meta_features</code> : bool (default: False)</p>
<p>If True, the meta-features computed from the training data used
for fitting the meta-classifier stored in the
<code>self.train_meta_features_</code> array, which can be
accessed after calling <code>fit</code>.</p>
</li>
<li>
<p><code>use_clones</code> : bool (default: True)</p>
<p>Clones the classifiers for stacking classification if True (default)
or else uses the original ones, which will be refitted on the dataset
upon calling the <code>fit</code> method. Hence, if use_clones=True, the original
input classifiers will remain unmodified upon using the
StackingClassifier's <code>fit</code> method.
Setting <code>use_clones=False</code> is
recommended if you are working with estimators that are supporting
the scikit-learn fit/predict API interface but are not compatible
to scikit-learn's <code>clone</code> function.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>clfs_</code> : list, shape=[n_classifiers]</p>
<p>Fitted classifiers (clones of the original classifiers)</p>
</li>
<li>
<p><code>meta_clf_</code> : estimator</p>
<p>Fitted meta-classifier (clone of the original meta-estimator)</p>
</li>
<li>
<p><code>train_meta_features</code> : numpy array, shape = [n_samples, n_classifiers]</p>
<p>meta-features for training data, where n_samples is the
number of samples
in training data and n_classifiers is the number of classfiers.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/</a></p>
<h3 id="methods_7">Methods</h3>
<hr>

<p><em>fit(X, y, sample_weight=None)</em></p>
<p>Fit ensemble classifers and the meta-classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples] or [n_samples, n_outputs]</p>
<p>Target values.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights passed as sample_weights to each regressor
in the regressors list as well as the meta_regressor.
Raises error if some regressor does not support
sample_weight in the fit() method.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict target values for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>labels</code> : array-like, shape = [n_samples] or [n_samples, n_outputs]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_meta_features(X)</em></p>
<p>Get meta-features of test-data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array, shape = [n_samples, n_features]</p>
<p>Test vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>meta-features</code> : numpy array, shape = [n_samples, n_classifiers]</p>
<p>Returns the meta-features for test data.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>proba</code> : array-like, shape = [n_samples, n_classes] or a list of                 n_outputs of such arrays if n_outputs &gt; 1.</p>
<p>Probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2014-2018 <a href="http://sebastianraschka.com">Sebastian Raschka</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../mathjaxhelper.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
