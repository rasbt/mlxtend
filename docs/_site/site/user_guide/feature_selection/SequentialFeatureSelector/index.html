<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A library consisting of useful tools and extensions for the day-to-day data science tasks."> 
    <meta name="author" content="Sebastian Raschka"> 
    <link rel="canonical" href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/">
    <link rel="shortcut icon" href="../../../img/favicon.ico">

    <title>Sequential Feature Selector - mlxtend</title>

    <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../../css/base.css" rel="stylesheet">
    <link href="../../../css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">


    <link href="../../../cinder/css/base.css" rel="stylesheet">


    <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">


    <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">


    <link href="../../../cinder/css/cinder.css" rel="stylesheet">


    <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">


    <link href="../../../cinder/css/highlight.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
    ga('send', 'pageview');
    </script>
    
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            <a class="navbar-brand" href="../../..">mlxtend</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../USER_GUIDE_INDEX/">User Guide Index</a>
</li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">classifier</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../classifier/Adaline/">Adaptive Linear Neuron -- Adaline</a>
</li>

        
            
<li >
    <a href="../../classifier/EnsembleVoteClassifier/">EnsembleVoteClassifier</a>
</li>

        
            
<li >
    <a href="../../classifier/LogisticRegression/">Logistic Regression</a>
</li>

        
            
<li >
    <a href="../../classifier/MultiLayerPerceptron/">Neural Network - Multilayer Perceptron</a>
</li>

        
            
<li >
    <a href="../../classifier/Perceptron/">Perceptron</a>
</li>

        
            
<li >
    <a href="../../classifier/SoftmaxRegression/">Softmax Regression</a>
</li>

        
            
<li >
    <a href="../../classifier/StackingClassifier/">StackingClassifier</a>
</li>

        
            
<li >
    <a href="../../classifier/StackingCVClassifier/">StackingCVClassifier</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">cluster</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../cluster/Kmeans/">Kmeans</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">data</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../data/autompg_data/">Auto MPG</a>
</li>

        
            
<li >
    <a href="../../data/boston_housing_data/">Boston Housing Data</a>
</li>

        
            
<li >
    <a href="../../data/iris_data/">Iris Dataset</a>
</li>

        
            
<li >
    <a href="../../data/loadlocal_mnist/">Load the MNIST Dataset from Local Files</a>
</li>

        
            
<li >
    <a href="../../data/make_multiplexer_dataset/">Make Multiplexer Dataset</a>
</li>

        
            
<li >
    <a href="../../data/mnist_data/">MNIST Dataset</a>
</li>

        
            
<li >
    <a href="../../data/three_blobs_data/">Three Blobs Dataset</a>
</li>

        
            
<li >
    <a href="../../data/wine_data/">Wine Dataset</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">evaluate</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../evaluate/bootstrap/">Bootstrap</a>
</li>

        
            
<li >
    <a href="../../evaluate/bootstrap_point632_score/">bootstrap_point632_score</a>
</li>

        
            
<li >
    <a href="../../evaluate/BootstrapOutOfBag/">BootstrapOutOfBag</a>
</li>

        
            
<li >
    <a href="../../evaluate/cochrans_q/">Cochran's Q Test</a>
</li>

        
            
<li >
    <a href="../../evaluate/combined_ftest_5x2cv/">5x2cv combined *F* test</a>
</li>

        
            
<li >
    <a href="../../evaluate/confusion_matrix/">Confusion Matrix</a>
</li>

        
            
<li >
    <a href="../../evaluate/feature_importance_permutation/">Feature Importance Permutation</a>
</li>

        
            
<li >
    <a href="../../evaluate/ftest/">F-Test</a>
</li>

        
            
<li >
    <a href="../../evaluate/lift_score/">Lift Score</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar_table/">Contigency Table for McNemar's Test</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar_tables/">Contigency Tables for McNemar's Test and Cochran's Q Test</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar/">McNemar's Test</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_5x2cv/">5x2cv paired *t* test</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_kfold_cv/">K-fold cross-validated paired *t* test</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_resampled/">Resampled paired *t* test</a>
</li>

        
            
<li >
    <a href="../../evaluate/permutation_test/">Permutation Test</a>
</li>

        
            
<li >
    <a href="../../evaluate/PredefinedHoldoutSplit/">PredefinedHoldoutSplit</a>
</li>

        
            
<li >
    <a href="../../evaluate/RandomHoldoutSplit/">RandomHoldoutSplit</a>
</li>

        
            
<li >
    <a href="../../evaluate/scoring/">Scoring</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_extraction</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/">Linear Discriminant Analysis</a>
</li>

        
            
<li >
    <a href="../../feature_extraction/PrincipalComponentAnalysis/">Principal Component Analysis</a>
</li>

        
            
<li >
    <a href="../../feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_selection</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../ColumnSelector/">ColumnSelector</a>
</li>

        
            
<li >
    <a href="../ExhaustiveFeatureSelector/">Exhaustive Feature Selector</a>
</li>

        
            
<li class="active">
    <a href="./">Sequential Feature Selector</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">file_io</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../file_io/find_filegroups/">Find Filegroups</a>
</li>

        
            
<li >
    <a href="../../file_io/find_files/">Find Files</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">frequent_patterns</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../frequent_patterns/apriori/">Apriori</a>
</li>

        
            
<li >
    <a href="../../frequent_patterns/association_rules/">Association rules</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">general concepts</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../general_concepts/activation-functions/">Activation Functions for Artificial Neural Networks</a>
</li>

        
            
<li >
    <a href="../../general_concepts/gradient-optimization/">Gradient Descent and Stochastic Gradient Descent</a>
</li>

        
            
<li >
    <a href="../../general_concepts/linear-gradient-derivative/">Deriving the Gradient Descent Rule for Linear Regression and Adaline</a>
</li>

        
            
<li >
    <a href="../../general_concepts/regularization-linear/">Regularization of Generalized Linear Models</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">image</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../image/extract_face_landmarks/">Extract Face Landmarks</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">math</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../math/num_combinations/">Compute the Number of Combinations</a>
</li>

        
            
<li >
    <a href="../../math/num_permutations/">Compute the Number of Permutations</a>
</li>

        
            
<li >
    <a href="../../math/vectorspace_dimensionality/">Vectorspace Dimensionality</a>
</li>

        
            
<li >
    <a href="../../math/vectorspace_orthonormalization/">Vectorspace Orthonormalization</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">plotting</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../plotting/category_scatter/">Scatterplot with Categories</a>
</li>

        
            
<li >
    <a href="../../plotting/checkerboard_plot/">Checkerboard Plot</a>
</li>

        
            
<li >
    <a href="../../plotting/ecdf/">Empirical Cumulative Distribution Function Plot</a>
</li>

        
            
<li >
    <a href="../../plotting/enrichment_plot/">Enrichment Plot</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_confusion_matrix/">Confusion Matrix</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_decision_regions/">Plotting Decision Regions</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_learning_curves/">Plotting Learning Curves</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_linear_regression/">Linear Regression Plot</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_sequential_feature_selection/">Plot Sequential Feature Selection</a>
</li>

        
            
<li >
    <a href="../../plotting/scatterplotmatrix/">Scatter Plot Matrix</a>
</li>

        
            
<li >
    <a href="../../plotting/stacked_barplot/">Stacked Barplot</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">preprocessing</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../preprocessing/CopyTransformer/">CopyTransformer</a>
</li>

        
            
<li >
    <a href="../../preprocessing/DenseTransformer/">DenseTransformer</a>
</li>

        
            
<li >
    <a href="../../preprocessing/MeanCenterer/">Mean Centerer</a>
</li>

        
            
<li >
    <a href="../../preprocessing/minmax_scaling/">MinMax Scaling</a>
</li>

        
            
<li >
    <a href="../../preprocessing/one-hot_encoding/">One hot encoding</a>
</li>

        
            
<li >
    <a href="../../preprocessing/shuffle_arrays_unison/">Shuffle Arrays in Unison</a>
</li>

        
            
<li >
    <a href="../../preprocessing/standardize/">Standardize</a>
</li>

        
            
<li >
    <a href="../../preprocessing/TransactionEncoder/">TransactionEncoder</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">regressor</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../regressor/LinearRegression/">LinearRegression</a>
</li>

        
            
<li >
    <a href="../../regressor/StackingCVRegressor/">StackingCVRegressor</a>
</li>

        
            
<li >
    <a href="../../regressor/StackingRegressor/">StackingRegressor</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">text</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../text/generalize_names/">Generalize Names</a>
</li>

        
            
<li >
    <a href="../../text/generalize_names_duplcheck/">Generalize Names & Duplicate Checking</a>
</li>

        
            
<li >
    <a href="../../text/tokenizer/">Tokenizer</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">utils</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../utils/Counter/">Counter</a>
</li>

        
    </ul>
  </li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.classifier/">Mlxtend.classifier</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.cluster/">Mlxtend.cluster</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.data/">Mlxtend.data</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.feature_selection/">Mlxtend.feature selection</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.file_io/">Mlxtend.file io</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.image/">Mlxtend.image</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.plotting/">Mlxtend.plotting</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.regressor/">Mlxtend.regressor</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.text/">Mlxtend.text</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.utils/">Mlxtend.utils</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../../../installation/">Installation</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../CHANGELOG/">Release Notes</a>
</li>

                        
                            
<li >
    <a href="../../../CONTRIBUTING/">How To Contribute</a>
</li>

                        
                            
<li >
    <a href="../../../contributors/">Contributors</a>
</li>

                        
                            
<li >
    <a href="../../../license/">License</a>
</li>

                        
                            
<li >
    <a href="../../../cite/">Citing Mlxtend</a>
</li>

                        
                            
<li >
    <a href="../../../discuss/">Discuss</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fa fa-search"></i> Search
                        </a>
                    </li>

                <!--
                    <li >
                        <a rel="next" href="../ExhaustiveFeatureSelector/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../../file_io/find_filegroups/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>-->
                    <li>
                        <a href="https://github.com/rasbt/mlxtend"><i class="fa fa-github"></i> GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#sequential-feature-selector">Sequential Feature Selector</a></li>
            <li class="second-level"><a href="#overview">Overview</a></li>
                 <!-- 
                <li class="third-level"><a href="#sequential-forward-selection-sfs">Sequential Forward Selection (SFS)</a></li>
                <li class="third-level"><a href="#sequential-backward-selection-sbs">Sequential Backward Selection (SBS)</a></li>
                <li class="third-level"><a href="#sequential-backward-floating-selection-sbfs">Sequential Backward Floating Selection (SBFS)</a></li>
                <li class="third-level"><a href="#sequential-forward-floating-selection-sffs">Sequential Forward Floating Selection (SFFS)</a></li>
                <li class="third-level"><a href="#references">References</a></li>  -->
            <li class="second-level"><a href="#example-1-a-simple-sequential-forward-selection-example">Example 1 - A simple Sequential Forward Selection example</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-2-toggling-between-sfs-sbs-sffs-and-sbfs">Example 2 - Toggling between SFS, SBS, SFFS, and SBFS</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-3-visualizing-the-results-in-dataframes">Example 3 - Visualizing the results in DataFrames</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-4-plotting-the-results">Example 4 - Plotting the results</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-5-sequential-feature-selection-for-regression">Example 5 - Sequential Feature Selection for Regression</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-6-feature-selection-with-fixed-trainvalidation-splits">Example 6 -- Feature Selection with Fixed Train/Validation Splits</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-7-using-the-selected-feature-subset-for-making-new-predictions">Example 7 -- Using the Selected Feature Subset For Making New Predictions</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-8-sequential-feature-selection-and-gridsearch">Example 8 -- Sequential Feature Selection and GridSearch</a></li>
                 <!-- 
                <li class="third-level"><a href="#obtaining-the-best-k-feature-indices-after-gridsearch">Obtaining the best k feature indices after GridSearch</a></li>  -->
            <li class="second-level"><a href="#example-9-selecting-the-best-feature-combination-in-a-k-range">Example 9 -- Selecting the "best"  feature combination in a k-range</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-10-using-other-cross-validation-schemes">Example 10 -- Using other cross-validation schemes</a></li>
                 <!-- 
                <li class="third-level"><a href="#using-groupkfold-with-sequentialfeatureselector">Using GroupKFold with SequentialFeatureSelector</a></li>  -->
            <li class="second-level"><a href="#example-11-working-with-pandas-dataframes">Example 11 - Working with pandas DataFrames</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-12-using-pandas-dataframes">Example 12 - Using Pandas DataFrames</a></li>
                 <!--   -->
        <li class="first-level "><a href="#api">API</a></li>
            <li class="second-level"><a href="#methods">Methods</a></li>
                 <!--   -->
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="sequential-feature-selector">Sequential Feature Selector</h1>
<p>Implementation of <em>sequential feature algorithms</em> (SFAs) -- greedy search algorithms -- that have been developed as a suboptimal solution to the computationally often not feasible exhaustive search.</p>
<blockquote>
<p>from mlxtend.feature_selection import SequentialFeatureSelector</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial <em>d</em>-dimensional feature space to a <em>k</em>-dimensional feature subspace where <em>k &lt; d</em>. The motivation behind feature selection algorithms is to automatically select a subset of features that is most relevant to the problem. The goal of feature selection is two-fold: We want to improve the computational efficiency and reduce the generalization error of the model by removing irrelevant features or noise. A wrapper approach such as sequential feature selection is especially useful if embedded feature selection -- for example, a regularization penalty like LASSO -- is not applicable.</p>
<p>In a nutshell, SFAs remove or add one feature at the time based on the classifier performance until a feature subset of the desired size <em>k</em> is reached. There are 4 different flavors of SFAs available via the <code>SequentialFeatureSelector</code>:</p>
<ol>
<li>Sequential Forward Selection (SFS)</li>
<li>Sequential Backward Selection (SBS)</li>
<li>Sequential Forward Floating Selection (SFFS)</li>
<li>Sequential Backward Floating Selection (SBFS)</li>
</ol>
<p>The <strong><em>floating</em></strong> variants, SFFS and SBFS, can be considered as extensions to the simpler SFS and SBS algorithms. The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled. It is important to emphasize that this step is conditional and only occurs if the resulting feature subset is assessed as "better" by the criterion function after removal (or addition) of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion steps if the algorithm gets stuck in cycles.  </p>
<hr />
<p>How is this different from <em>Recursive Feature Elimination</em> (RFE)  -- e.g., as implemented in <code>sklearn.feature_selection.RFE</code>? RFE is computationally less complex using the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.</p>
<hr />
<p>The SFAs  are outlined in pseudo code below:</p>
<h3 id="sequential-forward-selection-sfs">Sequential Forward Selection (SFS)</h3>
<p><strong>Input:</strong> <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The <strong><em>SFS</em></strong> algorithm takes the whole <script type="math/tex">d</script>-dimensional feature set as input.</li>
</ul>
<p><strong>Output:</strong> <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>SFS returns a subset of features; the number of selected features <script type="math/tex">k</script>, where <script type="math/tex">k < d</script>, has to be specified <em>a priori</em>.</li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = \emptyset</script>, <script type="math/tex">k = 0</script>
</p>
<ul>
<li>We initialize the algorithm with an empty set <script type="math/tex">\emptyset</script> ("null set") so that <script type="math/tex">k = 0</script> (where <script type="math/tex">k</script> is the size of the subset).</li>
</ul>
<p><strong>Step 1 (Inclusion):</strong>  </p>
<p>
<script type="math/tex">x^+ = \text{ arg max } J(x_k + x), \text{ where }  x \in Y - X_k</script>
<br />
<script type="math/tex">X_{k+1} = X_k + x^+</script>
<br />
<script type="math/tex">k = k + 1</script>
<br />
<em>Go to Step 1</em> </p>
<ul>
<li>in this step, we add an additional feature, <script type="math/tex">x^+</script>, to our feature subset <script type="math/tex">X_k</script>.</li>
<li>
<script type="math/tex">x^+</script> is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to <script type="math/tex">X_k</script>.</li>
<li>We repeat this procedure until the termination criterion is satisfied.</li>
</ul>
<p><strong>Termination:</strong> <script type="math/tex">k = p</script>
</p>
<ul>
<li>We add features from the feature subset <script type="math/tex">X_k</script> until the feature subset of size <script type="math/tex">k</script> contains the number of desired features <script type="math/tex">p</script> that we specified <em>a priori</em>.</li>
</ul>
<h3 id="sequential-backward-selection-sbs">Sequential Backward Selection (SBS)</h3>
<p><strong>Input:</strong> the set of all features, <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The SBS algorithm takes the whole feature set as input.</li>
</ul>
<p><strong>Output:</strong> <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>SBS returns a subset of features; the number of selected features <script type="math/tex">k</script>, where <script type="math/tex">k < d</script>, has to be specified <em>a priori</em>.</li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = Y</script>, <script type="math/tex">k = d</script>
</p>
<ul>
<li>We initialize the algorithm with the given feature set so that the <script type="math/tex">k = d</script>.</li>
</ul>
<p><strong>Step 1 (Exclusion):</strong>  </p>
<p>
<script type="math/tex">x^- = \text{ arg max } J(x_k - x), \text{  where } x \in X_k</script>
<br />
<script type="math/tex">X_{k-1} = X_k - x^-</script>
<br />
<script type="math/tex">k = k - 1</script>
<br />
<em>Go to Step 1</em>  </p>
<ul>
<li>In this step, we remove a feature, <script type="math/tex">x^-</script> from our feature subset <script type="math/tex">X_k</script>.</li>
<li>
<script type="math/tex">x^-</script> is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from <script type="math/tex">X_k</script>.</li>
<li>We repeat this procedure until the termination criterion is satisfied.</li>
</ul>
<p><strong>Termination:</strong> <script type="math/tex">k = p</script>
</p>
<ul>
<li>We add features from the feature subset <script type="math/tex">X_k</script> until the feature subset of size <script type="math/tex">k</script> contains the number of desired features <script type="math/tex">p</script> that we specified <em>a priori</em>.</li>
</ul>
<h3 id="sequential-backward-floating-selection-sbfs">Sequential Backward Floating Selection (SBFS)</h3>
<p><strong>Input:</strong> the set of all features, <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The SBFS algorithm takes the whole feature set as input.</li>
</ul>
<p><strong>Output:</strong> <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>SBFS returns a subset of features; the number of selected features <script type="math/tex">k</script>, where <script type="math/tex">k < d</script>, has to be specified <em>a priori</em>.</li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = Y</script>, <script type="math/tex">k = d</script>
</p>
<ul>
<li>We initialize the algorithm with the given feature set so that the <script type="math/tex">k = d</script>.</li>
</ul>
<p><strong>Step 1 (Exclusion):</strong>  </p>
<p>
<script type="math/tex">x^- = \text{ arg max } J(x_k - x), \text{  where } x \in X_k</script>
<br />
<script type="math/tex">X_{k-1} = X_k - x^-</script>
<br />
<script type="math/tex">k = k - 1</script>
<br />
<em>Go to Step 2</em>  </p>
<ul>
<li>In this step, we remove a feature, <script type="math/tex">x^-</script> from our feature subset <script type="math/tex">X_k</script>.</li>
<li>
<script type="math/tex">x^-</script> is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from <script type="math/tex">X_k</script>.</li>
</ul>
<p><strong>Step 2 (Conditional Inclusion):</strong><br />
<br>
<script type="math/tex">x^+ = \text{ arg max } J(x_k + x), \text{ where } x \in Y - X_k</script>
<br />
<em>if J(x_k + x) &gt; J(x_k + x)</em>:  <br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">X_{k+1} = X_k + x^+</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">k = k + 1</script>
<br />
<em>Go to Step 1</em>  </p>
<ul>
<li>In Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature <script type="math/tex">x^+</script> for which the performance improvement is maximized. If <script type="math/tex">k = 2</script> or an improvement cannot be made (i.e., such feature <script type="math/tex">x^+</script> cannot be found), go back to step 1; else, repeat this step.</li>
</ul>
<p><strong>Termination:</strong> <script type="math/tex">k = p</script>
</p>
<ul>
<li>We add features from the feature subset <script type="math/tex">X_k</script> until the feature subset of size <script type="math/tex">k</script> contains the number of desired features <script type="math/tex">p</script> that we specified <em>a priori</em>.</li>
</ul>
<h3 id="sequential-forward-floating-selection-sffs">Sequential Forward Floating Selection (SFFS)</h3>
<p><strong>Input:</strong> the set of all features, <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The <strong><em>SFFS</em></strong> algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions (<strong><em>d = 10</em></strong>).
<br><br></li>
</ul>
<p><strong>Output:</strong> a subset of features, <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>The returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space (<strong><em>k = 5, d = 10</em></strong>).
<br><br></li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = Y</script>, <script type="math/tex">k = d</script>
</p>
<ul>
<li>We initialize the algorithm with an empty set ("null set") so that the <strong><em>k = 0</em></strong> (where <strong><em>k</em></strong> is the size of the subset)
<br><br></li>
</ul>
<p><strong>Step 1 (Inclusion):</strong><br />
<br>
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">x^+ = \text{ arg max } J(x_k + x), \text{ where }  x \in Y - X_k</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">X_{k+1} = X_k + x^+</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">k = k + 1</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;<em>Go to Step 2</em><br />
<br> <br>
<strong>Step 2 (Conditional Exclusion):</strong><br />
<br>
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">x^- = \text{ arg max } J(x_k - x), \text{ where } x \in X_k</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;<script type="math/tex">if \; J(x_k - x) > J(x_k - x)</script>:  <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">X_{k-1} = X_k - x^- </script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">k = k - 1</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;<em>Go to Step 1</em>  </p>
<ul>
<li>In step 1, we include the feature from the <strong><em>feature space</em></strong> that leads to the best performance increase for our <strong><em>feature subset</em></strong> (assessed by the <strong><em>criterion function</em></strong>). Then, we go over to step 2</li>
<li>
<p>In step 2, we only remove a feature if the resulting subset would gain an increase in performance. If <script type="math/tex">k = 2</script> or an improvement cannot be made (i.e., such feature <script type="math/tex">x^+</script> cannot be found), go back to step 1; else, repeat this step.</p>
</li>
<li>
<p>Steps 1 and 2 are repeated until the <strong>Termination</strong> criterion is reached.
<br><br></p>
</li>
</ul>
<p><strong>Termination:</strong> stop when <strong><em>k</em></strong> equals the number of desired features</p>
<h3 id="references">References</h3>
<ul>
<li>
<p>Ferri, F. J., Pudil P., Hatef, M., Kittler, J. (1994). <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=sbajBQAAQBAJ&amp;oi=fnd&amp;pg=PA403&amp;dq=comparative+study+of+techniques+for+large+scale&amp;ots=KdIOYpA8wj&amp;sig=hdOsBP1HX4hcDjx4RLg_chheojc#v=onepage&amp;q=comparative%20study%20of%20techniques%20for%20large%20scale&amp;f=false"><em>"Comparative study of techniques for large-scale feature selection."</em></a> Pattern Recognition in Practice IV : 403-413.</p>
</li>
<li>
<p>Pudil, P., Novovičová, J., &amp; Kittler, J. (1994). <a href="http://www.sciencedirect.com/science/article/pii/0167865594901279"><em>"Floating search methods in feature selection."</em></a> Pattern recognition letters 15.11 (1994): 1119-1125.</p>
</li>
</ul>
<h2 id="example-1-a-simple-sequential-forward-selection-example">Example 1 - A simple Sequential Forward Selection example</h2>
<p>Initializing a simple classifier from scikit-learn:</p>
<pre><code class="python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target
knn = KNeighborsClassifier(n_neighbors=4)
</code></pre>

<p>We start by selection the "best" 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we set <code>forward=True</code> and <code>floating=False</code>. By choosing <code>cv=0</code>, we don't perform any cross-validation, therefore, the performance (here: <code>'accuracy'</code>) is computed entirely on the training set. </p>
<pre><code class="python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           cv=0)

sfs1 = sfs1.fit(X, y)
</code></pre>

<pre><code>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2018-05-06 12:49:16] Features: 1/3 -- score: 0.96[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2018-05-06 12:49:16] Features: 2/3 -- score: 0.973333333333[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2018-05-06 12:49:16] Features: 3/3 -- score: 0.973333333333
</code></pre>
<p>Via the <code>subsets_</code> attribute, we can take a look at the selected feature indices at each step:</p>
<pre><code class="python">sfs1.subsets_
</code></pre>

<pre><code>{1: {'avg_score': 0.95999999999999996,
  'cv_scores': array([ 0.96]),
  'feature_idx': (3,),
  'feature_names': ('3',)},
 2: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.97333333]),
  'feature_idx': (2, 3),
  'feature_names': ('2', '3')},
 3: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.97333333]),
  'feature_idx': (1, 2, 3),
  'feature_names': ('1', '2', '3')}}
</code></pre>
<p>Note that the 'feature_names' entry is simply a string representation of the 'feature_idx' in this case. Optionally, we can provide custom feature names via the <code>fit</code> method's <code>custom_feature_names</code> parameter:</p>
<pre><code class="python">feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width')
sfs1 = sfs1.fit(X, y, custom_feature_names=feature_names)
sfs1.subsets_
</code></pre>

<pre><code>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2018-05-06 12:49:16] Features: 1/3 -- score: 0.96[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2018-05-06 12:49:16] Features: 2/3 -- score: 0.973333333333[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2018-05-06 12:49:16] Features: 3/3 -- score: 0.973333333333




{1: {'avg_score': 0.95999999999999996,
  'cv_scores': array([ 0.96]),
  'feature_idx': (3,),
  'feature_names': ('petal width',)},
 2: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.97333333]),
  'feature_idx': (2, 3),
  'feature_names': ('petal length', 'petal width')},
 3: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.97333333]),
  'feature_idx': (1, 2, 3),
  'feature_names': ('sepal width', 'petal length', 'petal width')}}
</code></pre>
<p>Furthermore, we can access the indices of the 3 best features directly via the <code>k_feature_idx_</code> attribute:</p>
<pre><code class="python">sfs1.k_feature_idx_
</code></pre>

<pre><code>(1, 2, 3)
</code></pre>
<p>And similarly, to obtain the names of these features, given that we provided an argument to the <code>custom_feature_names</code> parameter, we can refer to the <code>sfs1.k_feature_names_</code> attribute:</p>
<pre><code class="python">sfs1.k_feature_names_
</code></pre>

<pre><code>('sepal width', 'petal length', 'petal width')
</code></pre>
<p>Finally, the prediction score for these 3 features can be accesses via <code>k_score_</code>:</p>
<pre><code class="python">sfs1.k_score_
</code></pre>

<pre><code>0.97333333333333338
</code></pre>
<h2 id="example-2-toggling-between-sfs-sbs-sffs-and-sbfs">Example 2 - Toggling between SFS, SBS, SFFS, and SBFS</h2>
<p>Using the <code>forward</code> and <code>floating</code> parameters, we can toggle between SFS, SBS, SFFS, and SBFS as shown below. Note that we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via <code>n_jobs=-1</code>, we choose to run the cross-validation on all our available CPU cores.</p>
<pre><code class="python"># Sequential Forward Selection
sfs = SFS(knn, 
          k_features=3, 
          forward=True, 
          floating=False, 
          scoring='accuracy',
          cv=4,
          n_jobs=-1)
sfs = sfs.fit(X, y)

print('\nSequential Forward Selection (k=3):')
print(sfs.k_feature_idx_)
print('CV Score:')
print(sfs.k_score_)

###################################################

# Sequential Backward Selection
sbs = SFS(knn, 
          k_features=3, 
          forward=False, 
          floating=False, 
          scoring='accuracy',
          cv=4,
          n_jobs=-1)
sbs = sbs.fit(X, y)

print('\nSequential Backward Selection (k=3):')
print(sbs.k_feature_idx_)
print('CV Score:')
print(sbs.k_score_)

###################################################

# Sequential Forward Floating Selection
sffs = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=True, 
           scoring='accuracy',
           cv=4,
           n_jobs=-1)
sffs = sffs.fit(X, y)

print('\nSequential Forward Floating Selection (k=3):')
print(sffs.k_feature_idx_)
print('CV Score:')
print(sffs.k_score_)

###################################################

# Sequential Backward Floating Selection
sbfs = SFS(knn, 
           k_features=3, 
           forward=False, 
           floating=True, 
           scoring='accuracy',
           cv=4,
           n_jobs=-1)
sbfs = sbfs.fit(X, y)

print('\nSequential Backward Floating Selection (k=3):')
print(sbfs.k_feature_idx_)
print('CV Score:')
print(sbfs.k_score_)
</code></pre>

<pre><code>Sequential Forward Selection (k=3):
(1, 2, 3)
CV Score:
0.972756410256

Sequential Backward Selection (k=3):
(1, 2, 3)
CV Score:
0.972756410256

Sequential Forward Floating Selection (k=3):
(1, 2, 3)
CV Score:
0.972756410256

Sequential Backward Floating Selection (k=3):
(1, 2, 3)
CV Score:
0.972756410256
</code></pre>
<p>In this simple scenario, selecting the best 3 features out of the 4 available features in the Iris set, we end up with similar results regardless of which sequential selection algorithms we used.</p>
<h2 id="example-3-visualizing-the-results-in-dataframes">Example 3 - Visualizing the results in DataFrames</h2>
<p>For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the <code>get_metric_dict</code> method of the SequentialFeatureSelector object. The columns <code>std_dev</code> and <code>std_err</code> represent the standard deviation and standard errors of the cross-validation scores, respectively.</p>
<p>Below, we see the DataFrame of the Sequential Forward Selector from Example 2:</p>
<pre><code class="python">import pandas as pd
pd.DataFrame.from_dict(sfs.get_metric_dict()).T
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>avg_score</th>
      <th>ci_bound</th>
      <th>cv_scores</th>
      <th>feature_idx</th>
      <th>feature_names</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.952991</td>
      <td>0.0660624</td>
      <td>[0.974358974359, 0.948717948718, 0.88888888888...</td>
      <td>(3,)</td>
      <td>(3,)</td>
      <td>0.0412122</td>
      <td>0.0237939</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.959936</td>
      <td>0.0494801</td>
      <td>[0.974358974359, 0.948717948718, 0.91666666666...</td>
      <td>(2, 3)</td>
      <td>(2, 3)</td>
      <td>0.0308676</td>
      <td>0.0178214</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.972756</td>
      <td>0.0315204</td>
      <td>[0.974358974359, 1.0, 0.944444444444, 0.972222...</td>
      <td>(1, 2, 3)</td>
      <td>(1, 2, 3)</td>
      <td>0.0196636</td>
      <td>0.0113528</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now, let's compare it to the Sequential Backward Selector:</p>
<pre><code class="python">pd.DataFrame.from_dict(sbs.get_metric_dict()).T
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>avg_score</th>
      <th>ci_bound</th>
      <th>cv_scores</th>
      <th>feature_idx</th>
      <th>feature_names</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>0.972756</td>
      <td>0.0315204</td>
      <td>[0.974358974359, 1.0, 0.944444444444, 0.972222...</td>
      <td>(1, 2, 3)</td>
      <td>(1, 2, 3)</td>
      <td>0.0196636</td>
      <td>0.0113528</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.952991</td>
      <td>0.0372857</td>
      <td>[0.974358974359, 0.948717948718, 0.91666666666...</td>
      <td>(0, 1, 2, 3)</td>
      <td>(0, 1, 2, 3)</td>
      <td>0.0232602</td>
      <td>0.0134293</td>
    </tr>
  </tbody>
</table>
</div>

<p>We can see that both SFS and SBFS found the same "best" 3 features, however, the intermediate steps where obviously different.</p>
<p>The <code>ci_bound</code> column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the <code>confidence_interval</code> parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:</p>
<pre><code class="python">pd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>avg_score</th>
      <th>ci_bound</th>
      <th>cv_scores</th>
      <th>feature_idx</th>
      <th>feature_names</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>0.972756</td>
      <td>0.0242024</td>
      <td>[0.974358974359, 1.0, 0.944444444444, 0.972222...</td>
      <td>(1, 2, 3)</td>
      <td>(1, 2, 3)</td>
      <td>0.0196636</td>
      <td>0.0113528</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.952991</td>
      <td>0.0286292</td>
      <td>[0.974358974359, 0.948717948718, 0.91666666666...</td>
      <td>(0, 1, 2, 3)</td>
      <td>(0, 1, 2, 3)</td>
      <td>0.0232602</td>
      <td>0.0134293</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="example-4-plotting-the-results">Example 4 - Plotting the results</h2>
<p>After importing the little helper function <a href="../../plotting/plot_sequential_feature_selection/"><code>plotting.plot_sequential_feature_selection</code></a>, we can also visualize the results using matplotlib figures.</p>
<pre><code class="python">from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt

sfs = SFS(knn, 
          k_features=4, 
          forward=True, 
          floating=False, 
          scoring='accuracy',
          verbose=2,
          cv=5)

sfs = sfs.fit(X, y)

fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')

plt.ylim([0.8, 1])
plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()
</code></pre>

<pre><code>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2018-05-06 12:49:18] Features: 1/4 -- score: 0.96[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2018-05-06 12:49:18] Features: 2/4 -- score: 0.966666666667[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2018-05-06 12:49:18] Features: 3/4 -- score: 0.953333333333[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished

[2018-05-06 12:49:18] Features: 4/4 -- score: 0.973333333333
</code></pre>
<p><img alt="png" src="../SequentialFeatureSelector_files/SequentialFeatureSelector_43_1.png" /></p>
<h2 id="example-5-sequential-feature-selection-for-regression">Example 5 - Sequential Feature Selection for Regression</h2>
<p>Similar to the classification examples above, the <code>SequentialFeatureSelector</code> also supports scikit-learn's estimators
for regression.</p>
<pre><code class="python">from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston.data, boston.target

lr = LinearRegression()

sfs = SFS(lr, 
          k_features=13, 
          forward=True, 
          floating=False, 
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(X, y)
fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')

plt.title('Sequential Forward Selection (w. StdErr)')
plt.grid()
plt.show()
</code></pre>

<p><img alt="png" src="../SequentialFeatureSelector_files/SequentialFeatureSelector_46_0.png" /></p>
<h2 id="example-6-feature-selection-with-fixed-trainvalidation-splits">Example 6 -- Feature Selection with Fixed Train/Validation Splits</h2>
<p>If you do not wish to use cross-validation (here: k-fold cross-validation, i.e., rotating training and validation folds), you can use the <code>PredefinedHoldoutSplit</code> class to specify your own, fixed training and validation split.</p>
<pre><code class="python">from sklearn.datasets import load_iris
from mlxtend.evaluate import PredefinedHoldoutSplit
import numpy as np


iris = load_iris()
X = iris.data
y = iris.target

rng = np.random.RandomState(123)
my_validation_indices = rng.permutation(np.arange(150))[:30]
print(my_validation_indices)
</code></pre>

<pre><code>[ 72 112 132  88  37 138  87  42   8  90 141  33  59 116 135 104  36  13
  63  45  28 133  24 127  46  20  31 121 117   4]
</code></pre>
<pre><code class="python">from sklearn.neighbors import KNeighborsClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as SFS



knn = KNeighborsClassifier(n_neighbors=4)
piter = PredefinedHoldoutSplit(my_validation_indices)

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           cv=piter)

sfs1 = sfs1.fit(X, y)
</code></pre>

<pre><code>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2018-09-24 02:31:21] Features: 1/3 -- score: 0.9666666666666667[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2018-09-24 02:31:21] Features: 2/3 -- score: 0.9666666666666667[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2018-09-24 02:31:21] Features: 3/3 -- score: 0.9666666666666667
</code></pre>
<h2 id="example-7-using-the-selected-feature-subset-for-making-new-predictions">Example 7 -- Using the Selected Feature Subset For Making New Predictions</h2>
<pre><code class="python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)

knn = KNeighborsClassifier(n_neighbors=4)
</code></pre>

<pre><code class="python"># Select the &quot;best&quot; three features via
# 5-fold cross-validation on the training set.

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=5)
sfs1 = sfs1.fit(X_train, y_train)
</code></pre>

<pre><code class="python">print('Selected features:', sfs1.k_feature_idx_)
</code></pre>

<pre><code>Selected features: (1, 2, 3)
</code></pre>
<pre><code class="python"># Generate the new subsets based on the selected features
# Note that the transform call is equivalent to
# X_train[:, sfs1.k_feature_idx_]

X_train_sfs = sfs1.transform(X_train)
X_test_sfs = sfs1.transform(X_test)

# Fit the estimator using the new feature subset
# and make a prediction on the test data
knn.fit(X_train_sfs, y_train)
y_pred = knn.predict(X_test_sfs)

# Compute the accuracy of the prediction
acc = float((y_test == y_pred).sum()) / y_pred.shape[0]
print('Test set accuracy: %.2f %%' % (acc * 100))
</code></pre>

<pre><code>Test set accuracy: 96.00 %
</code></pre>
<h2 id="example-8-sequential-feature-selection-and-gridsearch">Example 8 -- Sequential Feature Selection and GridSearch</h2>
<pre><code class="python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)
</code></pre>

<p>Use scikit-learn's <code>GridSearch</code> to tune the hyperparameters inside and outside the <code>SequentialFeatureSelector</code>:</p>
<pre><code class="python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import mlxtend

knn = KNeighborsClassifier(n_neighbors=2)

sfs1 = SFS(estimator=knn, 
           k_features=3,
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=5)

pipe = Pipeline([('sfs', sfs1), 
                 ('knn', knn)])

param_grid = [
  {'sfs__k_features': [1, 2, 3, 4],
   'sfs__estimator__n_neighbors': [1, 2, 3, 4]}
  ]

gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=5,  
                  refit=False)

# run gridearch
gs = gs.fit(X_train, y_train)
</code></pre>

<p>... and the "best" parameters determined by GridSearch are ...</p>
<pre><code class="python">print(&quot;Best parameters via GridSearch&quot;, gs.best_params_)
</code></pre>

<pre><code>Best parameters via GridSearch {'sfs__estimator__n_neighbors': 1, 'sfs__k_features': 3}
</code></pre>
<h4 id="obtaining-the-best-k-feature-indices-after-gridsearch">Obtaining the best <em>k</em> feature indices after GridSearch</h4>
<p>If we are interested in the best <em>k</em> feature indices via <code>SequentialFeatureSelection.k_feature_idx_</code>, we have to initialize a <code>GridSearchCV</code> object with <code>refit=True</code>. Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline.</p>
<pre><code class="python">gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=5, 
                  refit=True)
gs = gs.fit(X_train, y_train)
</code></pre>

<p>After running the grid search, we can access the individual pipeline objects of the <code>best_estimator_</code> via the <code>steps</code> attribute.</p>
<pre><code class="python">gs.best_estimator_.steps
</code></pre>

<pre><code>[('sfs', SequentialFeatureSelector(clone_estimator=True, cv=5,
               estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
             metric_params=None, n_jobs=1, n_neighbors=1, p=2,
             weights='uniform'),
               floating=False, forward=True, k_features=3, n_jobs=1,
               pre_dispatch='2*n_jobs', scoring='accuracy', verbose=0)),
 ('knn',
  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
             metric_params=None, n_jobs=1, n_neighbors=2, p=2,
             weights='uniform'))]
</code></pre>
<p>Via sub-indexing, we can then obtain the best-selected feature subset:</p>
<pre><code class="python">print('Best features:', gs.best_estimator_.steps[0][1].k_feature_idx_)
</code></pre>

<pre><code>Best features: (0, 1, 3)
</code></pre>
<p>During cross-validation, this feature combination had a CV accuracy of:</p>
<pre><code class="python">print('Best score:', gs.best_score_)
</code></pre>

<pre><code>Best score: 0.94
</code></pre>
<pre><code class="python">gs.best_params_
</code></pre>

<pre><code>{'sfs__estimator__n_neighbors': 1, 'sfs__k_features': 3}
</code></pre>
<p><strong>Alternatively</strong>, if we can set the "best grid search parameters" in our pipeline manually if we ran <code>GridSearchCV</code> with <code>refit=False</code>. It should yield the same results:</p>
<pre><code class="python">pipe.set_params(**gs.best_params_).fit(X_train, y_train)
print('Best features:', pipe.steps[0][1].k_feature_idx_)
</code></pre>

<pre><code>Best features: (0, 1, 3)
</code></pre>
<h2 id="example-9-selecting-the-best-feature-combination-in-a-k-range">Example 9 -- Selecting the "best"  feature combination in a k-range</h2>
<p>If <code>k_features</code> is set to to a tuple <code>(min_k, max_k)</code> (new in 0.4.2), the SFS will now select the best feature combination that it discovered by iterating from <code>k=1</code> to <code>max_k</code> (forward), or <code>max_k</code> to <code>min_k</code> (backward). The size of the returned feature subset is then within <code>max_k</code> to <code>min_k</code>, depending on which combination scored best during cross validation.</p>
<pre><code class="python">X.shape
</code></pre>

<pre><code>(150, 4)
</code></pre>
<pre><code class="python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier
from mlxtend.data import wine_data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

X, y = wine_data()
X_train, X_test, y_train, y_test= train_test_split(X, y, 
                                                   stratify=y,
                                                   test_size=0.3,
                                                   random_state=1)

knn = KNeighborsClassifier(n_neighbors=2)

sfs1 = SFS(estimator=knn, 
           k_features=(3, 10),
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=5)

pipe = make_pipeline(StandardScaler(), sfs1)

pipe.fit(X_train, y_train)

print('best combination (ACC: %.3f): %s\n' % (sfs1.k_score_, sfs1.k_feature_idx_))
print('all subsets:\n', sfs1.subsets_)
plot_sfs(sfs1.get_metric_dict(), kind='std_err');
</code></pre>

<pre><code>best combination (ACC: 0.992): (0, 1, 2, 3, 6, 8, 9, 10, 11, 12)

all subsets:
 {1: {'feature_idx': (6,), 'cv_scores': array([ 0.84615385,  0.6       ,  0.88      ,  0.79166667,  0.875     ]), 'avg_score': 0.7985641025641026, 'feature_names': ('6',)}, 2: {'feature_idx': (6, 9), 'cv_scores': array([ 0.92307692,  0.88      ,  1.        ,  0.95833333,  0.91666667]), 'avg_score': 0.93561538461538463, 'feature_names': ('6', '9')}, 3: {'feature_idx': (6, 9, 12), 'cv_scores': array([ 0.92307692,  0.92      ,  0.96      ,  1.        ,  0.95833333]), 'avg_score': 0.95228205128205123, 'feature_names': ('6', '9', '12')}, 4: {'feature_idx': (3, 6, 9, 12), 'cv_scores': array([ 0.96153846,  0.96      ,  0.96      ,  1.        ,  0.95833333]), 'avg_score': 0.96797435897435891, 'feature_names': ('3', '6', '9', '12')}, 5: {'feature_idx': (3, 6, 9, 10, 12), 'cv_scores': array([ 0.92307692,  0.96      ,  1.        ,  1.        ,  1.        ]), 'avg_score': 0.97661538461538466, 'feature_names': ('3', '6', '9', '10', '12')}, 6: {'feature_idx': (2, 3, 6, 9, 10, 12), 'cv_scores': array([ 0.92307692,  0.96      ,  1.        ,  0.95833333,  1.        ]), 'avg_score': 0.96828205128205125, 'feature_names': ('2', '3', '6', '9', '10', '12')}, 7: {'feature_idx': (0, 2, 3, 6, 9, 10, 12), 'cv_scores': array([ 0.92307692,  0.92      ,  1.        ,  1.        ,  1.        ]), 'avg_score': 0.96861538461538466, 'feature_names': ('0', '2', '3', '6', '9', '10', '12')}, 8: {'feature_idx': (0, 2, 3, 6, 8, 9, 10, 12), 'cv_scores': array([ 1.  ,  0.92,  1.  ,  1.  ,  1.  ]), 'avg_score': 0.98399999999999999, 'feature_names': ('0', '2', '3', '6', '8', '9', '10', '12')}, 9: {'feature_idx': (0, 2, 3, 6, 8, 9, 10, 11, 12), 'cv_scores': array([ 1.  ,  0.92,  1.  ,  1.  ,  1.  ]), 'avg_score': 0.98399999999999999, 'feature_names': ('0', '2', '3', '6', '8', '9', '10', '11', '12')}, 10: {'feature_idx': (0, 1, 2, 3, 6, 8, 9, 10, 11, 12), 'cv_scores': array([ 1.  ,  0.96,  1.  ,  1.  ,  1.  ]), 'avg_score': 0.99199999999999999, 'feature_names': ('0', '1', '2', '3', '6', '8', '9', '10', '11', '12')}}
</code></pre>
<p><img alt="png" src="../SequentialFeatureSelector_files/SequentialFeatureSelector_77_1.png" /></p>
<h2 id="example-10-using-other-cross-validation-schemes">Example 10 -- Using other cross-validation schemes</h2>
<p>In addition to standard k-fold and stratified k-fold, other cross validation schemes can be used with <code>SequentialFeatureSelector</code>. For example, <code>GroupKFold</code> or <code>LeaveOneOut</code> cross-validation from scikit-learn. </p>
<h4 id="using-groupkfold-with-sequentialfeatureselector">Using GroupKFold with SequentialFeatureSelector</h4>
<pre><code class="python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier
from mlxtend.data import iris_data
from sklearn.model_selection import GroupKFold
import numpy as np

X, y = iris_data()
groups = np.arange(len(y)) // 10
print('groups: {}'.format(groups))
</code></pre>

<pre><code>groups: [ 0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2
  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4
  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7
  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9
 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12
 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14]
</code></pre>
<p>Calling the <code>split()</code> method of a scikit-learn cross-validator object will return a generator that yields train, test splits.</p>
<pre><code class="python">cv_gen = GroupKFold(4).split(X, y, groups)
cv_gen
</code></pre>

<pre><code>&lt;generator object _BaseKFold.split at 0x1a1a041200&gt;
</code></pre>
<p>The <code>cv</code> parameter of <code>SequentialFeatureSelector</code> must be either an <code>int</code> or an iterable yielding train, test splits. This iterable can be constructed by passing the train, test split generator to the built-in <code>list()</code> function. </p>
<pre><code class="python">cv = list(cv_gen)
</code></pre>

<pre><code class="python">knn = KNeighborsClassifier(n_neighbors=2)
sfs = SFS(estimator=knn, 
          k_features=2,
          scoring='accuracy',
          cv=cv)

sfs.fit(X, y)

print('best combination (ACC: %.3f): %s\n' % (sfs.k_score_, sfs.k_feature_idx_))
</code></pre>

<pre><code>best combination (ACC: 0.940): (2, 3)
</code></pre>
<h2 id="example-11-working-with-pandas-dataframes">Example 11 - Working with pandas DataFrames</h2>
<h2 id="example-12-using-pandas-dataframes">Example 12 - Using Pandas DataFrames</h2>
<p>Optionally, we can also use pandas DataFrames and pandas Series as input to the <code>fit</code> function. In this case, the column names of the pandas DataFrame will be used as feature names. However, note that if <code>custom_feature_names</code> are provided in the fit function, these <code>custom_feature_names</code> take precedence over the DataFrame column-based feature names.</p>
<pre><code class="python">import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from mlxtend.feature_selection import SequentialFeatureSelector as SFS


iris = load_iris()
X = iris.data
y = iris.target
knn = KNeighborsClassifier(n_neighbors=4)

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=0)
</code></pre>

<pre><code class="python">X_df = pd.DataFrame(X, columns=['sepal len', 'petal len',
                                'sepal width', 'petal width'])
X_df.head()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal len</th>
      <th>petal len</th>
      <th>sepal width</th>
      <th>petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<p>Also, the target array, <code>y</code>, can be optionally be cast as a Series:</p>
<pre><code class="python">y_series = pd.Series(y)
y_series.head()
</code></pre>

<pre><code>0    0
1    0
2    0
3    0
4    0
dtype: int64
</code></pre>
<pre><code class="python">sfs1 = sfs1.fit(X_df, y_series)
</code></pre>

<p>Note that the only difference of passing a pandas DataFrame as input is that the sfs1.subsets_ array will now contain a new column, </p>
<pre><code class="python">sfs1.subsets_
</code></pre>

<pre><code>{1: {'avg_score': 0.95999999999999996,
  'cv_scores': array([ 0.96]),
  'feature_idx': (3,),
  'feature_names': ('petal width',)},
 2: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.97333333]),
  'feature_idx': (2, 3),
  'feature_names': ('sepal width', 'petal width')},
 3: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.97333333]),
  'feature_idx': (1, 2, 3),
  'feature_names': ('petal len', 'sepal width', 'petal width')}}
</code></pre>
<p>In mlxtend version &gt;= 0.13 pandas DataFrames are supported as feature inputs to the <code>SequentianFeatureSelector</code> instead of NumPy arrays or other NumPy-like array types.</p>
<h1 id="api">API</h1>
<p><em>SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True)*</p>
<p>Sequential Feature Selection for Classification and Regression.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>k_features</code> : int or tuple or str (default: 1)</p>
<p>Number of features to select,
where k_features &lt; the full feature set.
New in 0.4.2: A tuple containing a min and max value can be provided,
and the SFS will consider return any feature combination between
min and max that scored highest in cross-validtion. For example,
the tuple (1, 4) will return any combination from
1 up to 4 features instead of a fixed number of features k.
New in 0.8.0: A string argument "best" or "parsimonious".
If "best" is provided, the feature selector will return the
feature subset with the best cross-validation performance.
If "parsimonious" is provided as an argument, the smallest
feature subset that is within one standard error of the
cross-validation performance will be selected.</p>
</li>
<li>
<p><code>forward</code> : bool (default: True)</p>
<p>Forward selection if True,
backward selection otherwise</p>
</li>
<li>
<p><code>floating</code> : bool (default: False)</p>
<p>Adds a conditional exclusion/inclusion if True.</p>
</li>
<li>
<p><code>verbose</code> : int (default: 0), level of verbosity to use in logging.</p>
<p>If 0, no output,
if 1 number of features in current set, if 2 detailed logging i
ncluding timestamp and cv scores at step.</p>
</li>
<li>
<p><code>scoring</code> : str, callable, or None (default: None)</p>
<p>If None (default), uses 'accuracy' for sklearn classifiers
and 'r2' for sklearn regressors.
If str, uses a sklearn scoring metric string identifier, for example
{accuracy, f1, precision, recall, roc_auc} for classifiers,
{'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',
'median_absolute_error', 'r2'} for regressors.
If a callable object or function is provided, it has to be conform with
sklearn's signature <code>scorer(estimator, X, y)</code>; see
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
for more information.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Integer or iterable yielding train, test splits. If cv is an integer
and <code>estimator</code> is a classifier (or y consists of integer class
labels) stratified k-fold. Otherwise regular k-fold cross-validation
is performed. No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>k_feature_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>k_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>k_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
sequential selection, where the dictionary keys are
the lengths k of these feature subsets. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v 0.13.0.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/">http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/</a></p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, custom_feature_names=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for y.</p>
</li>
<li>
<p><code>custom_feature_names</code> : None or tuple (default: tuple)</p>
<p>Custom feature names for <code>self.k_feature_names</code> and
<code>self.subsets_[i]['feature_names']</code>.
(new in v 0.13.0)</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, </em><em>fit_params)</em></p>
<p>Fit to training data then reduce X to its most important features.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.
New in v 0.13.0: a pandas Series are now also accepted as
argument for y.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Reduced feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Reduce X to its most important features.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Reduced feature subset of X, shape={n_samples, k_features}</p></div>
        
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Copyright &copy; 2014-2019 <a href="http://sebastianraschka.com">Sebastian Raschka</a><br></small>
        
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="../../../js/jquery-1.10.2.min.js"></script>
    <script src="../../../js/bootstrap-3.0.3.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '../../..';
    </script>
    <script data-main="../../../mkdocs/js/search.js" src="../../../mkdocs/js/require.js"></script>
    <script src="../../../js/base.js"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="../../../mathjaxhelper.js"></script>
    <script src="../../../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>
