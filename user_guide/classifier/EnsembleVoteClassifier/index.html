<!DOCTYPE html>
<html lang="en">

<head>
    <page.page.meta charset="utf-8">
    <page.page.meta http-equiv="X-UA-Compatible" page.content="IE=edge">
    <page.page.meta name="viewport" page.content="width=device-width, initial-scale=1.0">
    <page.page.meta name="description" page.content="A library consisting of useful tools and extensions for the day-to-day data science tasks."> 
    <page.page.meta name="author" page.content="Sebastian Raschka"> 
    <link rel="canonical" href="http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/"> 
    <link rel="shortcut icon" href="../../../img/favicon.ico">

    <title>EnsembleVoteClassifier - mlxtend</title>

    <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../../css/base.css" rel="stylesheet">
    <link href="../../../css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">

    <!--
    <link href="../../../extra.css" rel="stylesheet">-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
    ga('send', 'pageview');
    </script>
    
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="../../..">mlxtend</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../USER_GUIDE_INDEX/">USER GUIDE INDEX</a>
</li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">classifier</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../Adaline/">Adaline</a>
</li>

        
            
<li class="active">
    <a href="./">EnsembleVoteClassifier</a>
</li>

        
            
<li >
    <a href="../LogisticRegression/">LogisticRegression</a>
</li>

        
            
<li >
    <a href="../MultiLayerPerceptron/">MultiLayerPerceptron</a>
</li>

        
            
<li >
    <a href="../Perceptron/">Perceptron</a>
</li>

        
            
<li >
    <a href="../SoftmaxRegression/">SoftmaxRegression</a>
</li>

        
            
<li >
    <a href="../StackingClassifier/">StackingClassifier</a>
</li>

        
            
<li >
    <a href="../StackingCVClassifier/">StackingCVClassifier</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">cluster</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../cluster/Kmeans/">Kmeans</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">data</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../data/autompg_data/">Autompg data</a>
</li>

        
            
<li >
    <a href="../../data/boston_housing_data/">Boston housing data</a>
</li>

        
            
<li >
    <a href="../../data/iris_data/">Iris data</a>
</li>

        
            
<li >
    <a href="../../data/loadlocal_mnist/">Loadlocal mnist</a>
</li>

        
            
<li >
    <a href="../../data/make_multiplexer_dataset/">Make multiplexer dataset</a>
</li>

        
            
<li >
    <a href="../../data/mnist_data/">Mnist data</a>
</li>

        
            
<li >
    <a href="../../data/three_blobs_data/">Three blobs data</a>
</li>

        
            
<li >
    <a href="../../data/wine_data/">Wine data</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">evaluate</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../evaluate/bootstrap/">Bootstrap</a>
</li>

        
            
<li >
    <a href="../../evaluate/bootstrap_point632_score/">Bootstrap point632 score</a>
</li>

        
            
<li >
    <a href="../../evaluate/BootstrapOutOfBag/">BootstrapOutOfBag</a>
</li>

        
            
<li >
    <a href="../../evaluate/cochrans_q/">Cochrans q</a>
</li>

        
            
<li >
    <a href="../../evaluate/confusion_matrix/">Confusion matrix</a>
</li>

        
            
<li >
    <a href="../../evaluate/lift_score/">Lift score</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar_table/">Mcnemar table</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar_tables/">Mcnemar tables</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar/">Mcnemar</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_5x2cv/">Paired ttest 5x2cv</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_kfold_cv/">Paired ttest kfold cv</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_resampled/">Paired ttest resampled</a>
</li>

        
            
<li >
    <a href="../../evaluate/permutation_test/">Permutation test</a>
</li>

        
            
<li >
    <a href="../../evaluate/scoring/">Scoring</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_extraction</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/">LinearDiscriminantAnalysis</a>
</li>

        
            
<li >
    <a href="../../feature_extraction/PrincipalComponentAnalysis/">PrincipalComponentAnalysis</a>
</li>

        
            
<li >
    <a href="../../feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_selection</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../feature_selection/ColumnSelector/">ColumnSelector</a>
</li>

        
            
<li >
    <a href="../../feature_selection/ExhaustiveFeatureSelector/">ExhaustiveFeatureSelector</a>
</li>

        
            
<li >
    <a href="../../feature_selection/SequentialFeatureSelector/">SequentialFeatureSelector</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">file_io</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../file_io/find_filegroups/">Find filegroups</a>
</li>

        
            
<li >
    <a href="../../file_io/find_files/">Find files</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">frequent_patterns</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../frequent_patterns/apriori/">Apriori</a>
</li>

        
            
<li >
    <a href="../../frequent_patterns/association_rules/">Association rules</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">general concepts</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../general_concepts/activation-functions/">Activation functions</a>
</li>

        
            
<li >
    <a href="../../general_concepts/gradient-optimization/">Gradient optimization</a>
</li>

        
            
<li >
    <a href="../../general_concepts/linear-gradient-derivative/">Linear gradient derivative</a>
</li>

        
            
<li >
    <a href="../../general_concepts/regularization-linear/">Regularization linear</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">math</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../math/num_combinations/">Num combinations</a>
</li>

        
            
<li >
    <a href="../../math/num_permutations/">Num permutations</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">plotting</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../plotting/category_scatter/">Category scatter</a>
</li>

        
            
<li >
    <a href="../../plotting/checkerboard_plot/">Checkerboard plot</a>
</li>

        
            
<li >
    <a href="../../plotting/ecdf/">Ecdf</a>
</li>

        
            
<li >
    <a href="../../plotting/enrichment_plot/">Enrichment plot</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_confusion_matrix/">Plot confusion matrix</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_decision_regions/">Plot decision regions</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_learning_curves/">Plot learning curves</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_linear_regression/">Plot linear regression</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_sequential_feature_selection/">Plot sequential feature selection</a>
</li>

        
            
<li >
    <a href="../../plotting/stacked_barplot/">Stacked barplot</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">preprocessing</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../preprocessing/CopyTransformer/">CopyTransformer</a>
</li>

        
            
<li >
    <a href="../../preprocessing/DenseTransformer/">DenseTransformer</a>
</li>

        
            
<li >
    <a href="../../preprocessing/MeanCenterer/">MeanCenterer</a>
</li>

        
            
<li >
    <a href="../../preprocessing/minmax_scaling/">Minmax scaling</a>
</li>

        
            
<li >
    <a href="../../preprocessing/one-hot_encoding/">One hot encoding</a>
</li>

        
            
<li >
    <a href="../../preprocessing/shuffle_arrays_unison/">Shuffle arrays unison</a>
</li>

        
            
<li >
    <a href="../../preprocessing/standardize/">Standardize</a>
</li>

        
            
<li >
    <a href="../../preprocessing/TransactionEncoder/">TransactionEncoder</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">regressor</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../regressor/LinearRegression/">LinearRegression</a>
</li>

        
            
<li >
    <a href="../../regressor/StackingCVRegressor/">StackingCVRegressor</a>
</li>

        
            
<li >
    <a href="../../regressor/StackingRegressor/">StackingRegressor</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">text</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../text/generalize_names/">Generalize names</a>
</li>

        
            
<li >
    <a href="../../text/generalize_names_duplcheck/">Generalize names duplcheck</a>
</li>

        
            
<li >
    <a href="../../text/tokenizer/">Tokenizer</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">utils</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../utils/Counter/">Counter</a>
</li>

        
    </ul>
  </li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.classifier/">Mlxtend.classifier</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.cluster/">Mlxtend.cluster</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.data/">Mlxtend.data</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.feature_selection/">Mlxtend.feature selection</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.file_io/">Mlxtend.file io</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.plotting/">Mlxtend.plotting</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.regressor/">Mlxtend.regressor</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.text/">Mlxtend.text</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.utils/">Mlxtend.utils</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../../../installation/">Installation</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../changelog/">Release Notes</a>
</li>

                        
                            
<li >
    <a href="../../../contributing/">How To Contribute</a>
</li>

                        
                            
<li >
    <a href="../../../contributors/">Contributors</a>
</li>

                        
                            
<li >
    <a href="../../../license/">License</a>
</li>

                        
                            
<li >
    <a href="../../../cite/">Citing Mlxtend</a>
</li>

                        
                            
<li >
    <a href="../../../discuss/">Discuss</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>
            

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                
                    <li >
                        <a rel="next" href="../Adaline/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../LogisticRegression/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                


                    <li>
                        <a href="https://github.com/rasbt/mlxtend">
                                <i class="fa fa-github"></i>
                            GitHub
                        </a>
                    </li>



                
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#ensemblevoteclassifier">EnsembleVoteClassifier</a></li>
        
    
        <li class="main "><a href="#overview">Overview</a></li>
        
            <li><a href="#majority-voting-hard-voting">Majority Voting / Hard Voting</a></li>
                <!--  -->
        
            <li><a href="#weighted-majority-vote">Weighted Majority Vote</a></li>
                <!--  -->
        
            <li><a href="#soft-voting">Soft Voting</a></li>
                <!--  -->
        
            <li><a href="#references">References</a></li>
                <!--  -->
        
            <li><a href="#example-1-classifying-iris-flowers-using-different-classification-models">Example 1 -  Classifying Iris Flowers Using Different Classification Models</a></li>
                <!-- 
                    <li><a href="#plotting-decision-regions">Plotting Decision Regions</a></li>
             -->
        
            <li><a href="#example-2-grid-search">Example 2 - Grid Search</a></li>
                <!--  -->
        
            <li><a href="#example-3-majority-voting-with-classifiers-trained-on-different-feature-subsets">Example 3 - Majority voting with classifiers trained on different feature subsets</a></li>
                <!-- 
                    <li><a href="#manual-approach">Manual Approach</a></li>
             -->
        
            <li><a href="#example-5-using-pre-fitted-classifiers">Example 5 - Using Pre-fitted Classifiers</a></li>
                <!--  -->
        
            <li><a href="#example-6-ensembles-of-classifiers-that-operate-on-different-feature-subsets">Example 6 - Ensembles of Classifiers that Operate on Different Feature Subsets</a></li>
                <!--  -->
        
    
        <li class="main "><a href="#api">API</a></li>
        
            <li><a href="#methods">Methods</a></li>
                <!--  -->
        
    
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="ensemblevoteclassifier">EnsembleVoteClassifier</h1>
<p>Implementation of a majority voting <code>EnsembleVoteClassifier</code> for classification.</p>
<blockquote>
<p>from mlxtend.classifier import EnsembleVoteClassifier</p>
</blockquote>
<h1 id="overview">Overview</h1>
<p>The <code>EnsembleVoteClassifier</code> is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting. (For simplicity, we will refer to both majority and plurality voting as majority voting.)</p>
<p><img alt="" src="../EnsembleVoteClassifier_files/voting.png" /></p>
<p>The <code>EnsembleVoteClassifier</code> implements "hard" and "soft" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities (only recommended if the classifiers are well-calibrated).</p>
<p><img alt="" src="../EnsembleVoteClassifier_files/majority_voting.png" /></p>
<p><strong>Note</strong></p>
<p>If you are interested in using the <code>EnsembleVoteClassifier</code>, please note that it is now also available through scikit learn (&gt;0.17) as <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"><code>VotingClassifier</code></a>.</p>
<h3 id="majority-voting-hard-voting">Majority Voting / Hard Voting</h3>
<p>Hard voting is the simplest case of majority voting. Here, we predict the class label <script type="math/tex">\hat{y}</script> via majority (plurality) voting of each classifier <script type="math/tex">C_j</script>:</p>
<p>
<script type="math/tex; mode=display">\hat{y}=mode\{C_1(\mathbf{x}), C_2(\mathbf{x}), ..., C_m(\mathbf{x})\}</script>
</p>
<p>Assuming that we combine three classifiers that classify a training sample as follows:</p>
<ul>
<li>classifier 1 -&gt; class 0</li>
<li>classifier 2 -&gt; class 0</li>
<li>classifier 3 -&gt; class 1</li>
</ul>
<p>
<script type="math/tex; mode=display">\hat{y}=mode\{0, 0, 1\} = 0</script>
</p>
<p>Via majority vote, we would we would classify the sample as "class 0."</p>
<h3 id="weighted-majority-vote">Weighted Majority Vote</h3>
<p>In addition to the simple majority vote (hard voting) as described in the previous section, we can compute a weighted majority vote by associating a weight <script type="math/tex">w_j</script> with classifier <script type="math/tex">C_j</script>:</p>
<p>
<script type="math/tex; mode=display">\hat{y} = \arg \max_i \sum^{m}_{j=1} w_j \chi_A \big(C_j(\mathbf{x})=i\big),</script>
</p>
<p>where <script type="math/tex">\chi_A</script> is the characteristic function <script type="math/tex">[C_j(\mathbf{x}) = i \; \in A]</script>, and <script type="math/tex">A</script> is the set of unique class labels. </p>
<p>Continuing with the example from the previous section</p>
<ul>
<li>classifier 1 -&gt; class 0</li>
<li>classifier 2 -&gt; class 0</li>
<li>classifier 3 -&gt; class 1</li>
</ul>
<p>assigning the weights {0.2, 0.2, 0.6} would yield a prediction <script type="math/tex">\hat{y} = 1</script>:</p>
<p>
<script type="math/tex; mode=display">\arg \max_i [0.2 \times i_0 + 0.2 \times i_0 + 0.6 \times i_1] = 1</script>
</p>
<h3 id="soft-voting">Soft Voting</h3>
<p>In soft voting, we predict the class labels based on the predicted probabilities <script type="math/tex">p</script> for classifier -- this approach is only recommended if the classifiers are well-calibrated.</p>
<p>
<script type="math/tex; mode=display">\hat{y} = \arg \max_i \sum^{m}_{j=1} w_j p_{ij},</script>
</p>
<p>where <script type="math/tex">w_j</script> is the weight that can be assigned to the <script type="math/tex">j</script>th classifier.</p>
<p>Assuming the example in the previous section was a binary classification task with class labels <script type="math/tex">i \in \{0, 1\}</script>, our ensemble could make the following prediction:</p>
<ul>
<li>
<script type="math/tex">C_1(\mathbf{x}) \rightarrow [0.9, 0.1]</script>
</li>
<li>
<script type="math/tex">C_2(\mathbf{x}) \rightarrow [0.8, 0.2]</script>
</li>
<li>
<script type="math/tex">C_3(\mathbf{x}) \rightarrow [0.4, 0.6]</script>
</li>
</ul>
<p>Using uniform weights, we compute the average probabilities:</p>
<p>
<script type="math/tex; mode=display">p(i_0 \mid \mathbf{x}) = \frac{0.9 + 0.8 + 0.4}{3} = 0.7 \\\\
p(i_1 \mid \mathbf{x}) = \frac{0.1 + 0.2 + 0.6}{3} = 0.3</script>
</p>
<p>
<script type="math/tex; mode=display">\hat{y} = \arg \max_i \big[p(i_0 \mid \mathbf{x}), p(i_1 \mid \mathbf{x}) \big] = 0</script>
</p>
<p>However, assigning the weights {0.1, 0.1, 0.8} would yield a prediction <script type="math/tex">\hat{y} = 1</script>:</p>
<p>
<script type="math/tex; mode=display">p(i_0 \mid \mathbf{x}) = {0.1 \times 0.9 + 0.1 \times 0.8 + 0.8 \times  0.4} = 0.49 \\\\
p(i_1 \mid \mathbf{x}) = {0.1 \times 0.1 + 0.2 \times 0.1 + 0.8 \times 0.6} = 0.51</script>
</p>
<p>
<script type="math/tex; mode=display">\hat{y} = \arg \max_i \big[p(i_0 \mid \mathbf{x}), p(i_1 \mid \mathbf{x}) \big] = 1</script>
</p>
<h3 id="references">References</h3>
<ul>
<li>[1] S. Raschka. <a href="https://github.com/rasbt/python-machine-learning-book">Python Machine Learning</a>. Packt Publishing Ltd., 2015.</li>
</ul>
<h2 id="example-1-classifying-iris-flowers-using-different-classification-models">Example 1 -  Classifying Iris Flowers Using Different Classification Models</h2>
<pre><code class="python">from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target
</code></pre>

<pre><code class="python">from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
import numpy as np

clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()

print('5-fold cross validation:\n')

labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes']

for clf, label in zip([clf1, clf2, clf3], labels):

    scores = model_selection.cross_val_score(clf, X, y, 
                                              cv=5, 
                                              scoring='accuracy')
    print(&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;
          % (scores.mean(), scores.std(), label))
</code></pre>

<pre><code>5-fold cross validation:

Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
Accuracy: 0.93 (+/- 0.05) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [Naive Bayes]
</code></pre>
<pre><code class="python">from mlxtend.classifier import EnsembleVoteClassifier

eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])

labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']
for clf, label in zip([clf1, clf2, clf3, eclf], labels):

    scores = model_selection.cross_val_score(clf, X, y, 
                                              cv=5, 
                                              scoring='accuracy')
    print(&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot; 
          % (scores.mean(), scores.std(), label))
</code></pre>

<pre><code>Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
Accuracy: 0.93 (+/- 0.05) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [Naive Bayes]
Accuracy: 0.95 (+/- 0.05) [Ensemble]
</code></pre>
<h4 id="plotting-decision-regions">Plotting Decision Regions</h4>
<pre><code class="python">import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions
import matplotlib.gridspec as gridspec
import itertools

gs = gridspec.GridSpec(2, 2)

fig = plt.figure(figsize=(10,8))

labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']
for clf, lab, grd in zip([clf1, clf2, clf3, eclf],
                         labels,
                         itertools.product([0, 1], repeat=2)):

    clf.fit(X, y)
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(X=X, y=y, clf=clf)
    plt.title(lab)
</code></pre>

<p><img alt="png" src="../EnsembleVoteClassifier_files/EnsembleVoteClassifier_24_0.png" /></p>
<pre><code class="python">import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions
import matplotlib.gridspec as gridspec
import itertools

gs = gridspec.GridSpec(2, 2)

fig = plt.figure(figsize=(10,8))

labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']
for clf, lab, grd in zip([clf1, clf2, clf3, eclf],
                         labels,
                         itertools.product([0, 1], repeat=2)):

    clf.fit(X, y)
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(X=X, y=y, clf=clf)
    plt.title(lab)
</code></pre>

<p><img alt="png" src="../EnsembleVoteClassifier_files/EnsembleVoteClassifier_25_0.png" /></p>
<h2 id="example-2-grid-search">Example 2 - Grid Search</h2>
<pre><code class="python">from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target
</code></pre>

<pre><code class="python">from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
from mlxtend.classifier import EnsembleVoteClassifier

clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')

params = {'logisticregression__C': [1.0, 100.0],
          'randomforestclassifier__n_estimators': [20, 200],}

grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
grid.fit(iris.data, iris.target)

cv_keys = ('mean_test_score', 'std_test_score', 'params')

for r, _ in enumerate(grid.cv_results_['mean_test_score']):
    print(&quot;%0.3f +/- %0.2f %r&quot;
          % (grid.cv_results_[cv_keys[0]][r],
             grid.cv_results_[cv_keys[1]][r] / 2.0,
             grid.cv_results_[cv_keys[2]][r]))
</code></pre>

<pre><code>0.953 +/- 0.01 {'logisticregression__C': 1.0, 'randomforestclassifier__n_estimators': 20}
0.960 +/- 0.01 {'logisticregression__C': 1.0, 'randomforestclassifier__n_estimators': 200}
0.960 +/- 0.01 {'logisticregression__C': 100.0, 'randomforestclassifier__n_estimators': 20}
0.953 +/- 0.02 {'logisticregression__C': 100.0, 'randomforestclassifier__n_estimators': 200}
</code></pre>
<p><strong>Note</strong>: If the <code>EnsembleClassifier</code> is initialized with multiple similar estimator objects, the estimator names are modified with consecutive integer indices, for example:</p>
<pre><code class="python">clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
eclf = EnsembleVoteClassifier(clfs=[clf1, clf1, clf2], 
                              voting='soft')

params = {'logisticregression-1__C': [1.0, 100.0],
          'logisticregression-2__C': [1.0, 100.0],
          'randomforestclassifier__n_estimators': [20, 200],}

grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
grid = grid.fit(iris.data, iris.target)
</code></pre>

<p><strong>Note</strong></p>
<p>The <code>EnsembleVoteClass</code> also enables grid search over the <code>clfs</code> argument. However, due to the current implementation of <code>GridSearchCV</code> in scikit-learn, it is not possible to search over both, differenct classifiers and classifier parameters at the same time. For instance, while the following parameter dictionary works</p>
<pre><code>params = {'randomforestclassifier__n_estimators': [1, 100],
'clfs': [(clf1, clf1, clf1), (clf2, clf3)]}
</code></pre>
<p>it will use the instance settings of <code>clf1</code>, <code>clf2</code>, and <code>clf3</code> and not overwrite it with the <code>'n_estimators'</code> settings from <code>'randomforestclassifier__n_estimators': [1, 100]</code>.</p>
<h2 id="example-3-majority-voting-with-classifiers-trained-on-different-feature-subsets">Example 3 - Majority voting with classifiers trained on different feature subsets</h2>
<p>Feature selection algorithms implemented in scikit-learn as well as the <code>SequentialFeatureSelector</code> implement a <code>transform</code> method that passes the reduced feature subset to the next item in a <code>Pipeline</code>.</p>
<p>For example, the method</p>
<pre><code>def transform(self, X):
    return X[:, self.k_feature_idx_]
</code></pre>
<p>returns the best feature columns, <code>k_feature_idx_</code>, given a dataset X.</p>
<p>Thus, we simply need to construct a <code>Pipeline</code> consisting of the feature selector and the classifier in order to select different feature subsets for different algorithms. During <code>fitting</code>, the optimal feature subsets are automatically determined via the <code>GridSearchCV</code> object, and by calling <code>predict</code>, the fitted feature selector in the pipeline only passes these columns along, which resulted in the best performance for the respective classifier.</p>
<pre><code class="python">from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data[:, :], iris.target

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
from mlxtend.classifier import EnsembleVoteClassifier
from sklearn.pipeline import Pipeline
from mlxtend.feature_selection import SequentialFeatureSelector

clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()

# Creating a feature-selection-classifier pipeline

sfs1 = SequentialFeatureSelector(clf1, 
                                 k_features=4,
                                 forward=True, 
                                 floating=False, 
                                 scoring='accuracy',
                                 verbose=0,
                                 cv=0)

clf1_pipe = Pipeline([('sfs', sfs1),
                      ('logreg', clf1)])

eclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3], 
                              voting='soft')


params = {'pipeline__sfs__k_features': [1, 2, 3],
          'pipeline__logreg__C': [1.0, 100.0],
          'randomforestclassifier__n_estimators': [20, 200]}

grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
grid.fit(iris.data, iris.target)



cv_keys = ('mean_test_score', 'std_test_score', 'params')

for r, _ in enumerate(grid.cv_results_['mean_test_score']):
    print(&quot;%0.3f +/- %0.2f %r&quot;
          % (grid.cv_results_[cv_keys[0]][r],
             grid.cv_results_[cv_keys[1]][r] / 2.0,
             grid.cv_results_[cv_keys[2]][r]))
</code></pre>

<pre><code>0.953 +/- 0.01 {'pipeline__sfs__k_features': 1, 'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20}
0.947 +/- 0.02 {'pipeline__sfs__k_features': 1, 'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200}
0.953 +/- 0.01 {'pipeline__sfs__k_features': 2, 'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20}
0.947 +/- 0.02 {'pipeline__sfs__k_features': 2, 'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200}
0.953 +/- 0.01 {'pipeline__sfs__k_features': 3, 'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 20}
0.953 +/- 0.02 {'pipeline__sfs__k_features': 3, 'pipeline__logreg__C': 1.0, 'randomforestclassifier__n_estimators': 200}
0.947 +/- 0.02 {'pipeline__sfs__k_features': 1, 'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20}
0.953 +/- 0.02 {'pipeline__sfs__k_features': 1, 'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200}
0.947 +/- 0.02 {'pipeline__sfs__k_features': 2, 'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20}
0.947 +/- 0.02 {'pipeline__sfs__k_features': 2, 'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200}
0.960 +/- 0.01 {'pipeline__sfs__k_features': 3, 'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 20}
0.953 +/- 0.02 {'pipeline__sfs__k_features': 3, 'pipeline__logreg__C': 100.0, 'randomforestclassifier__n_estimators': 200}
</code></pre>
<p>The best parameters determined via GridSearch are:</p>
<pre><code class="python">grid.best_params_
</code></pre>

<pre><code>{'pipeline__logreg__C': 100.0,
 'pipeline__sfs__k_features': 3,
 'randomforestclassifier__n_estimators': 20}
</code></pre>
<p>Now, we assign these parameters to the ensemble voting classifier, fit the models on the complete training set, and perform a prediction on 3 samples from the Iris dataset.</p>
<pre><code class="python">eclf = eclf.set_params(**grid.best_params_)
eclf.fit(X, y).predict(X[[1, 51, 149]])
</code></pre>

<pre><code>array([0, 1, 2])
</code></pre>
<h4 id="manual-approach">Manual Approach</h4>
<p>Alternatively, we can select different columns "manually" using the <code>ColumnSelector</code> object. In this example, we select only the first (sepal length) and third (petal length) column for the logistic regression classifier (<code>clf1</code>).</p>
<pre><code class="python">from mlxtend.feature_selection import ColumnSelector


col_sel = ColumnSelector(cols=[0, 2])

clf1_pipe = Pipeline([('sel', col_sel),
                      ('logreg', clf1)])

eclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3],
                              voting='soft')
eclf.fit(X, y).predict(X[[1, 51, 149]])
</code></pre>

<pre><code>array([0, 1, 2])
</code></pre>
<p>Furthermore, we can fit the <code>SequentialFeatureSelector</code> separately, outside the grid search hyperparameter optimization pipeline. Here, we determine the best features first, and then we construct a pipeline using these "fixed," best features as seed for the <code>ColumnSelector</code>:</p>
<pre><code class="python">sfs1 = SequentialFeatureSelector(clf1, 
                                 k_features=2,
                                 forward=True, 
                                 floating=False, 
                                 scoring='accuracy',
                                 verbose=1,
                                 cv=0)

sfs1.fit(X, y)

print('Best features', sfs1.k_feature_idx_)

col_sel = ColumnSelector(cols=sfs1.k_feature_idx_)

clf1_pipe = Pipeline([('sel', col_sel),
                      ('logreg', clf1)])
</code></pre>

<pre><code>Features: 2/2

Best features (0, 2)
</code></pre>
<pre><code class="python">eclf = EnsembleVoteClassifier(clfs=[clf1_pipe, clf2, clf3], 
                              voting='soft')
eclf.fit(X, y).predict(X[[1, 51, 149]])
</code></pre>

<pre><code>array([0, 1, 2])
</code></pre>
<h2 id="example-5-using-pre-fitted-classifiers">Example 5 - Using Pre-fitted Classifiers</h2>
<pre><code class="python">from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target
</code></pre>

<p>Assume that we previously fitted our classifiers:</p>
<pre><code class="python">from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
import numpy as np

clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()

for clf in (clf1, clf2, clf3):
    clf.fit(X, y)
</code></pre>

<p>By setting <code>refit=False</code>, the <code>EnsembleVoteClassifier</code> will not re-fit these classifers to save computational time:</p>
<pre><code class="python">from mlxtend.classifier import EnsembleVoteClassifier
import copy
eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1], refit=False)

labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']

eclf.fit(X, y)

print('accuracy:', np.mean(y == eclf.predict(X)))
</code></pre>

<pre><code>accuracy: 0.973333333333
</code></pre>
<p>However, please note that <code>refit=False</code> is incompatible to any form of cross-validation that is done in e.g., <code>model_selection.cross_val_score</code> or <code>model_selection.GridSearchCV</code>, etc., since it would require the classifiers to be refit to the training folds. Thus, only use <code>refit=False</code> if you want to make a prediction directly without cross-validation.</p>
<h2 id="example-6-ensembles-of-classifiers-that-operate-on-different-feature-subsets">Example 6 - Ensembles of Classifiers that Operate on Different Feature Subsets</h2>
<p>If desired, the different classifiers can be fit to different subsets of features in the training dataset. The following example illustrates how this can be done on a technical level using scikit-learn pipelines and the <code>ColumnSelector</code>:</p>
<pre><code class="python">from sklearn.datasets import load_iris
from mlxtend.classifier import EnsembleVoteClassifier
from mlxtend.feature_selection import ColumnSelector
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

iris = load_iris()
X = iris.data
y = iris.target

pipe1 = make_pipeline(ColumnSelector(cols=(0, 2)),
                      LogisticRegression())
pipe2 = make_pipeline(ColumnSelector(cols=(1, 2, 3)),
                      LogisticRegression())

eclf = EnsembleVoteClassifier(clfs=[pipe1, pipe2])

eclf.fit(X, y)
</code></pre>

<pre><code>EnsembleVoteClassifier(clfs=[Pipeline(steps=[('columnselector', ColumnSelector(cols=(0, 2))), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='libl...='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))])],
            refit=True, verbose=0, voting='hard', weights=None)
</code></pre>
<h1 id="api">API</h1>
<p><em>EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, refit=True)</em></p>
<p>Soft Voting/Majority Rule classifier for scikit-learn estimators.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>clfs</code> : array-like, shape = [n_classifiers]</p>
<p>A list of classifiers.
Invoking the <code>fit</code> method on the <code>VotingClassifier</code> will fit clones
of those original classifiers that will
be stored in the class attribute
<code>self.clfs_</code> if <code>refit=True</code> (default).</p>
</li>
<li>
<p><code>voting</code> : str, {'hard', 'soft'} (default='hard')</p>
<p>If 'hard', uses predicted class labels for majority rule voting.
Else if 'soft', predicts the class label based on the argmax of
the sums of the predicted probalities, which is recommended for
an ensemble of well-calibrated classifiers.</p>
</li>
<li>
<p><code>weights</code> : array-like, shape = [n_classifiers], optional (default=<code>None</code>)</p>
<p>Sequence of weights (<code>float</code> or <code>int</code>) to weight the occurances of
predicted class labels (<code>hard</code> voting) or class probabilities
before averaging (<code>soft</code> voting). Uses uniform weights if <code>None</code>.</p>
</li>
<li>
<p><code>verbose</code> : int, optional (default=0)</p>
<p>Controls the verbosity of the building process.
- <code>verbose=0</code> (default): Prints nothing
- <code>verbose=1</code>: Prints the number &amp; name of the clf being fitted
- <code>verbose=2</code>: Prints info about the parameters of the clf being fitted
- <code>verbose&gt;2</code>: Changes <code>verbose</code> param of the underlying clf to
self.verbose - 2</p>
</li>
<li>
<p><code>refit</code> : bool (default: True)</p>
<p>Refits classifiers in <code>clfs</code> if True; uses references to the <code>clfs</code>,
otherwise (assumes that the classifiers were already fit).
Note: refit=False is incompatible to mist scikit-learn wrappers!
For instance, if any form of cross-validation is performed
this would require the re-fitting classifiers to training folds, which
would raise a NotFitterError if refit=False.
(New in mlxtend v0.6.)</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>classes_</code> : array-like, shape = [n_predictions]</p>
</li>
<li>
<p><code>clf</code> : array-like, shape = [n_predictions]</p>
<p>The unmodified input classifiers</p>
</li>
<li>
<p><code>clf_</code> : array-like, shape = [n_predictions]</p>
<p>Fitted clones of the input classifiers</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB
&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt; from mlxtend.sklearn import EnsembleVoteClassifier
&gt;&gt;&gt; clf1 = LogisticRegression(random_seed=1)
&gt;&gt;&gt; clf2 = RandomForestClassifier(random_seed=1)
&gt;&gt;&gt; clf3 = GaussianNB()
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2])
&gt;&gt;&gt; eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
... voting='hard', verbose=1)
&gt;&gt;&gt; eclf1 = eclf1.fit(X, y)
&gt;&gt;&gt; print(eclf1.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt; eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')
&gt;&gt;&gt; eclf2 = eclf2.fit(X, y)
&gt;&gt;&gt; print(eclf2.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt; eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
...                          voting='soft', weights=[2,1,1])
&gt;&gt;&gt; eclf3 = eclf3.fit(X, y)
&gt;&gt;&gt; print(eclf3.predict(X))
[1 1 1 2 2 2]
&gt;&gt;&gt;
</code></pre>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y)</em></p>
<p>Learn weight coefficients from training data for each classifier.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y=None, </em><em>fit_params)</em></p>
<p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : numpy array of shape [n_samples, n_features]</p>
<p>Training set.</p>
</li>
<li>
<p><code>y</code> : numpy array of shape [n_samples]</p>
<p>Target values.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_new</code> : numpy array of shape [n_samples, n_features_new]</p>
<p>Transformed array.</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Return estimator parameter names for GridSearch support.</p>
<hr>

<p><em>predict(X)</em></p>
<p>Predict class labels for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>maj</code> : array-like, shape = [n_samples]</p>
<p>Predicted class labels.</p>
</li>
</ul>
<hr>

<p><em>predict_proba(X)</em></p>
<p>Predict class probabilities for X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>avg</code> : array-like, shape = [n_samples, n_classes]</p>
<p>Weighted average probability for each class per sample.</p>
</li>
</ul>
<hr>

<p><em>score(X, y, sample_weight=None)</em></p>
<p>Returns the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : array-like, shape = (n_samples, n_features)</p>
<p>Test samples.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>
<p>True labels for X.</p>
</li>
<li>
<p><code>sample_weight</code> : array-like, shape = [n_samples], optional</p>
<p>Sample weights.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>score</code> : float</p>
<p>Mean accuracy of self.predict(X) wrt. y.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Return class labels or probabilities for X for each estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>If</code>voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes]</p>
<p>Class probabilties calculated by each classifier.</p>
</li>
<li>
<p><code>If</code>voting='hard'`` : array-like = [n_classifiers, n_samples]</p>
<p>Class labels predicted by each classifier.</p>
</li>
</ul></div>
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Copyright &copy; 2014-2018 <a href="http://sebastianraschka.com">Sebastian Raschka</a><br></small>
        
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="../../../js/jquery-1.10.2.min.js"></script>
    <script src="../../../js/bootstrap-3.0.3.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '../../..';
    </script>
    <script data-main="../../../mkdocs/js/search.js" src="../../../mkdocs/js/require.js"></script>
    <script src="../../../js/base.js"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="../../../mathjaxhelper.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>
