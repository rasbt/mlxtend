<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A library consisting of useful tools and extensions for the day-to-day data science tasks."> 
    <meta name="author" content="Sebastian Raschka"> 
    <link rel="canonical" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/">
    <link rel="shortcut icon" href="../../../img/favicon.ico">

    <title>StackingClassifier: Simple stacking - mlxtend</title>

    <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../../css/base.css" rel="stylesheet">
    <link href="../../../css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">


    <link href="../../../cinder/css/base.css" rel="stylesheet">


    <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">


    <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">


    <link href="../../../cinder/css/cinder.css" rel="stylesheet">


    <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">


    <link href="../../../cinder/css/highlight.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
    ga('send', 'pageview');
    </script>
    
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            <a class="navbar-brand" href="../../..">mlxtend</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../USER_GUIDE_INDEX/">User Guide Index</a>
</li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">classifier</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../Adaline/">Adaline: Adaptive Linear Neuron Classifier</a>
</li>

        
            
<li >
    <a href="../EnsembleVoteClassifier/">EnsembleVoteClassifier: A majority voting classifier</a>
</li>

        
            
<li >
    <a href="../LogisticRegression/">LogisticRegression: A binary classifier</a>
</li>

        
            
<li >
    <a href="../MultiLayerPerceptron/">MultilayerPerceptron: A simple multilayer neural network</a>
</li>

        
            
<li >
    <a href="../OneRClassifier/">OneRClassifier: One Rule (OneR) method for classfication</a>
</li>

        
            
<li >
    <a href="../Perceptron/">Perceptron: A simple binary classifier</a>
</li>

        
            
<li >
    <a href="../SoftmaxRegression/">SoftmaxRegression: Multiclass version of logistic regression</a>
</li>

        
            
<li class="active">
    <a href="./">StackingClassifier: Simple stacking</a>
</li>

        
            
<li >
    <a href="../StackingCVClassifier/">StackingCVClassifier: Stacking with cross-validation</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">cluster</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../cluster/Kmeans/">Kmeans: k-means clustering</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">data</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../data/autompg_data/">autompg_data: The Auto-MPG dataset for regression</a>
</li>

        
            
<li >
    <a href="../../data/boston_housing_data/">boston_housing_data: The Boston housing dataset for regression</a>
</li>

        
            
<li >
    <a href="../../data/iris_data/">iris_data: The 3-class iris dataset for classification</a>
</li>

        
            
<li >
    <a href="../../data/loadlocal_mnist/">loadlocal_mnist: A function for loading MNIST from the original ubyte files</a>
</li>

        
            
<li >
    <a href="../../data/make_multiplexer_dataset/">make_multiplexer_dataset: A function for creating multiplexer data</a>
</li>

        
            
<li >
    <a href="../../data/mnist_data/">mnist_data: A subset of the MNIST dataset for classification</a>
</li>

        
            
<li >
    <a href="../../data/three_blobs_data/">three_blobs_data: The synthetic blobs for classification</a>
</li>

        
            
<li >
    <a href="../../data/wine_data/">wine_data: A 3-class wine dataset for classification</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">evaluate</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../evaluate/accuracy_score/">accuracy_score: Computing standard, balanced, and per-class accuracy</a>
</li>

        
            
<li >
    <a href="../../evaluate/bias_variance_decomp/">bias_variance_decomp: Bias-variance decomposition for classification and regression losses</a>
</li>

        
            
<li >
    <a href="../../evaluate/bootstrap/">bootstrap: The ordinary nonparametric boostrap for arbitrary parameters</a>
</li>

        
            
<li >
    <a href="../../evaluate/bootstrap_point632_score/">bootstrap_point632_score: The .632 and .632+ boostrap for classifier evaluation</a>
</li>

        
            
<li >
    <a href="../../evaluate/BootstrapOutOfBag/">BootstrapOutOfBag: A scikit-learn compatible version of the out-of-bag bootstrap</a>
</li>

        
            
<li >
    <a href="../../evaluate/cochrans_q/">cochrans_q: Cochran's Q test for comparing multiple classifiers</a>
</li>

        
            
<li >
    <a href="../../evaluate/combined_ftest_5x2cv/">combined_ftest_5x2cv: 5x2cv combined *F* test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../evaluate/confusion_matrix/">confusion_matrix: creating a confusion matrix for model evaluation</a>
</li>

        
            
<li >
    <a href="../../evaluate/create_counterfactual/">create_counterfactual: Interpreting models via counterfactuals</a>
</li>

        
            
<li >
    <a href="../../evaluate/feature_importance_permutation/">feature_importance_permutation: Estimate feature importance via feature permutation.</a>
</li>

        
            
<li >
    <a href="../../evaluate/ftest/">ftest: F-test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../evaluate/GroupTimeSeriesSplit/">GroupTimeSeriesSplit: A scikit-learn compatible version of the time series validation with groups</a>
</li>

        
            
<li >
    <a href="../../evaluate/lift_score/">lift_score: Lift score for classification and association rule mining</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar_table/">mcnemar_table: Ccontingency table for McNemar's test</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar_tables/">mcnemar_tables: contingency tables for McNemar's test and Cochran's Q test</a>
</li>

        
            
<li >
    <a href="../../evaluate/mcnemar/">mcnemar: McNemar's test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_5x2cv/">paired_ttest_5x2cv: 5x2cv paired *t* test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_kfold_cv/">paired_ttest_kfold_cv: K-fold cross-validated paired *t* test</a>
</li>

        
            
<li >
    <a href="../../evaluate/paired_ttest_resampled/">paired_ttest_resample: Resampled paired *t* test</a>
</li>

        
            
<li >
    <a href="../../evaluate/permutation_test/">permutation_test: Permutation test for hypothesis testing</a>
</li>

        
            
<li >
    <a href="../../evaluate/PredefinedHoldoutSplit/">PredefinedHoldoutSplit: Utility for the holdout method compatible with scikit-learn</a>
</li>

        
            
<li >
    <a href="../../evaluate/RandomHoldoutSplit/">RandomHoldoutSplit: split a dataset into a train and validation subset for validation</a>
</li>

        
            
<li >
    <a href="../../evaluate/scoring/">scoring: computing various performance metrics</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_extraction</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/">LinearDiscriminantAnalysis: Linear discriminant analysis for dimensionality reduction</a>
</li>

        
            
<li >
    <a href="../../feature_extraction/PrincipalComponentAnalysis/">PrincipalComponentAnalysis: Principal component analysis (PCA) for dimensionality reduction</a>
</li>

        
            
<li >
    <a href="../../feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_selection</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../feature_selection/ColumnSelector/">ColumnSelector: Scikit-learn utility function to select specific columns in a pipeline</a>
</li>

        
            
<li >
    <a href="../../feature_selection/ExhaustiveFeatureSelector/">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</a>
</li>

        
            
<li >
    <a href="../../feature_selection/SequentialFeatureSelector/">SequentialFeatureSelector: The popular forward and backward feature selection approaches incl. floating variants</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">file_io</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../file_io/find_filegroups/">find_filegroups: Find files that only differ via their file extensions</a>
</li>

        
            
<li >
    <a href="../../file_io/find_files/">find_files: Find files based on substring matches</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">frequent_patterns</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../frequent_patterns/apriori/">Apriori</a>
</li>

        
            
<li >
    <a href="../../frequent_patterns/association_rules/">Association rules</a>
</li>

        
            
<li >
    <a href="../../frequent_patterns/fpgrowth/">Fpgrowth</a>
</li>

        
            
<li >
    <a href="../../frequent_patterns/fpmax/">Fpmax</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">image</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../image/extract_face_landmarks/">extract_face_landmarks: extract 68 landmark features from face images</a>
</li>

        
            
<li >
    <a href="../../image/eyepad_align/">EyepadAlign:  align face images based on eye location</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">math</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../math/num_combinations/">num_combinations: combinations for creating subsequences of *k* elements</a>
</li>

        
            
<li >
    <a href="../../math/num_permutations/">num_permutations: number of permutations for creating subsequences of *k* elements</a>
</li>

        
            
<li >
    <a href="../../math/vectorspace_dimensionality/">vectorspace_dimensionality: compute the number of dimensions that a set of vectors spans</a>
</li>

        
            
<li >
    <a href="../../math/vectorspace_orthonormalization/">vectorspace_orthonormalization: Converts a set of linearly independent vectors to a set of orthonormal basis vectors</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">plotting</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../plotting/category_scatter/">Scategory_scatter: Create a scatterplot with categories in different colors</a>
</li>

        
            
<li >
    <a href="../../plotting/checkerboard_plot/">checkerboard_plot: Create a checkerboard plot in matplotlib</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_pca_correlation_graph/">plot_pca_correlation_graph: plot correlations between original features and principal components</a>
</li>

        
            
<li >
    <a href="../../plotting/ecdf/">ecdf: Create an empirical cumulative distribution function plot</a>
</li>

        
            
<li >
    <a href="../../plotting/enrichment_plot/">enrichment_plot: create an enrichment plot for cumulative counts</a>
</li>

        
            
<li >
    <a href="../../plotting/heatmap/">heatmap: Create a heatmap in matplotlib</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_confusion_matrix/">plot_confusion_matrix: Visualize confusion matrices</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_decision_regions/">plot_decision_regions: Visualize the decision regions of a classifier</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_learning_curves/">plot_learning_curves: Plot learning curves from training and test sets</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_linear_regression/">plot_linear_regression: A quick way for plotting linear regression fits</a>
</li>

        
            
<li >
    <a href="../../plotting/plot_sequential_feature_selection/">plot_sequential_feature_selection: Visualize selected feature subset performances from the SequentialFeatureSelector</a>
</li>

        
            
<li >
    <a href="../../plotting/scatterplotmatrix/">scatterplotmatrix: visualize datasets via a scatter plot matrix</a>
</li>

        
            
<li >
    <a href="../../plotting/scatter_hist/">scatter_hist: create a scatter histogram plot</a>
</li>

        
            
<li >
    <a href="../../plotting/stacked_barplot/">stacked_barplot: Plot stacked bar plots in matplotlib</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">preprocessing</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../preprocessing/CopyTransformer/">CopyTransformer: A function that creates a copy of the input array in a scikit-learn pipeline</a>
</li>

        
            
<li >
    <a href="../../preprocessing/DenseTransformer/">DenseTransformer: Transforms a sparse into a dense NumPy array, e.g., in a scikit-learn pipeline</a>
</li>

        
            
<li >
    <a href="../../preprocessing/MeanCenterer/">MeanCenterer: column-based mean centering on a NumPy array</a>
</li>

        
            
<li >
    <a href="../../preprocessing/minmax_scaling/">MinMaxScaling: Min-max scaling fpr pandas DataFrames and NumPy arrays</a>
</li>

        
            
<li >
    <a href="../../preprocessing/one-hot_encoding/">One hot encoding</a>
</li>

        
            
<li >
    <a href="../../preprocessing/shuffle_arrays_unison/">shuffle_arrays_unison: shuffle arrays in a consistent fashion</a>
</li>

        
            
<li >
    <a href="../../preprocessing/standardize/">standardize: A function to standardize columns in a 2D NumPy array</a>
</li>

        
            
<li >
    <a href="../../preprocessing/TransactionEncoder/">TransactionEncoder</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">regressor</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../regressor/LinearRegression/">LinearRegression: An implementation of ordinary least-squares linear regression</a>
</li>

        
            
<li >
    <a href="../../regressor/StackingCVRegressor/">StackingCVRegressor: stacking with cross-validation for regression</a>
</li>

        
            
<li >
    <a href="../../regressor/StackingRegressor/">StackingRegressor: a simple stacking implementation for regression</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">text</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../text/generalize_names/">generalize_names: convert names into a generalized format</a>
</li>

        
            
<li >
    <a href="../../text/generalize_names_duplcheck/">generalize_names_duplcheck: Generalize names while preventing duplicates among different names</a>
</li>

        
            
<li >
    <a href="../../text/tokenizer/">tokenizer_emoticons: tokenizers for emoticons</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">utils</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../utils/Counter/">Counter: A simple progress counter</a>
</li>

        
    </ul>
  </li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.classifier/">Mlxtend.classifier</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.cluster/">Mlxtend.cluster</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.data/">Mlxtend.data</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.feature_selection/">Mlxtend.feature selection</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.file_io/">Mlxtend.file io</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.image/">Mlxtend.image</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.plotting/">Mlxtend.plotting</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.regressor/">Mlxtend.regressor</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.text/">Mlxtend.text</a>
</li>

                        
                            
<li >
    <a href="../../../api_subpackages/mlxtend.utils/">Mlxtend.utils</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../../../installation/">Installation</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../CHANGELOG/">Release Notes</a>
</li>

                        
                            
<li >
    <a href="../../../Code-of-Conduct/">Code of Conduct</a>
</li>

                        
                            
<li >
    <a href="../../../CONTRIBUTING/">How To Contribute</a>
</li>

                        
                            
<li >
    <a href="../../../contributors/">Contributors</a>
</li>

                        
                            
<li >
    <a href="../../../license/">License</a>
</li>

                        
                            
<li >
    <a href="../../../cite/">Citing Mlxtend</a>
</li>

                        
                            
<li >
    <a href="../../../discuss/">Discuss</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fa fa-search"></i> Search
                        </a>
                    </li>

                <!--
                    <li >
                        <a rel="next" href="../SoftmaxRegression/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../StackingCVClassifier/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>-->
                    <li>
                        <a href="https://github.com/rasbt/mlxtend"><i class="fa fa-github"></i> GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#stackingclassifier-simple-stacking">StackingClassifier: Simple stacking</a></li>
        <li class="first-level "><a href="#overview">Overview</a></li>
            <li class="second-level"><a href="#references">References</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-1-simple-stacked-classification">Example 1 - Simple Stacked Classification</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-2-using-probabilities-as-meta-features">Example 2 - Using Probabilities as Meta-Features</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-3-stacked-classification-and-gridsearch">Example 3 - Stacked Classification and GridSearch</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-4-stacking-of-classifiers-that-operate-on-different-feature-subsets">Example 4 - Stacking of Classifiers that Operate on Different Feature Subsets</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-5-using-pre-fitted-classifiers">Example 5 - Using Pre-fitted Classifiers</a></li>
                 <!--   -->
            <li class="second-level"><a href="#example-6-roc-curve-with-decision_function">Example 6 -- ROC Curve with decision_function</a></li>
                 <!--   -->
        <li class="first-level "><a href="#api">API</a></li>
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="stackingclassifier-simple-stacking">StackingClassifier: Simple stacking</h1>
<p>An ensemble-learning meta-classifier for stacking.</p>
<blockquote>
<p>from mlxtend.classifier import StackingClassifier</p>
</blockquote>
<h1 id="overview">Overview</h1>
<p>Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble.
The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.</p>
<p><img alt="" src="../StackingClassifier_files/stackingclassification_overview.png" /></p>
<p>The algorithm can be summarized as follows (source: [1]):</p>
<p><img alt="" src="../StackingClassifier_files/stacking_algorithm.png" /></p>
<p><strong>Please note that this type of Stacking is prone to overfitting due to information leakage. The related <a href="../StackingCVClassifier/">StackingCVClassifier.md</a> does not derive the predictions for the 2nd-level classifier from the same datast that was used for training the level-1 classifiers and is recommended instead.</strong></p>
<h3 id="references">References</h3>
<ul>
<li>[1] Tang, J., S. Alelyani, and H. Liu. "<a href="https://books.google.com/books?id=nwQZCwAAQBAJ&amp;lpg=PA500&amp;dq=stacking%20classifier%20subsets&amp;pg=PA499#v=onepage&amp;q&amp;f=false">Data Classification: Algorithms and Applications.</a>" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500.</li>
<li>[2] Wolpert, David H. "<a href="http://www.sciencedirect.com/science/article/pii/S0893608005800231">Stacked generalization.</a>" Neural networks 5.2 (1992): 241-259.</li>
</ul>
<h2 id="example-1-simple-stacked-classification">Example 1 - Simple Stacked Classification</h2>
<pre><code class="language-python">from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target
</code></pre>
<pre><code class="language-python">from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
from mlxtend.classifier import StackingClassifier
import numpy as np
import warnings

warnings.simplefilter('ignore')

clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], 
                          meta_classifier=lr)

print('3-fold cross validation:\n')

for clf, label in zip([clf1, clf2, clf3, sclf], 
                      ['KNN', 
                       'Random Forest', 
                       'Naive Bayes',
                       'StackingClassifier']):

    scores = model_selection.cross_val_score(clf, X, y, 
                                              cv=3, scoring='accuracy')
    print(&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot; 
          % (scores.mean(), scores.std(), label))
</code></pre>
<pre><code>3-fold cross validation:

Accuracy: 0.91 (+/- 0.01) [KNN]
Accuracy: 0.95 (+/- 0.01) [Random Forest]
Accuracy: 0.91 (+/- 0.02) [Naive Bayes]
Accuracy: 0.95 (+/- 0.02) [StackingClassifier]
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions
import matplotlib.gridspec as gridspec
import itertools

gs = gridspec.GridSpec(2, 2)

fig = plt.figure(figsize=(10,8))

for clf, lab, grd in zip([clf1, clf2, clf3, sclf], 
                         ['KNN', 
                          'Random Forest', 
                          'Naive Bayes',
                          'StackingClassifier'],
                          itertools.product([0, 1], repeat=2)):

    clf.fit(X, y)
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(X=X, y=y, clf=clf)
    plt.title(lab)
</code></pre>
<p><img alt="png" src="../StackingClassifier_files/StackingClassifier_14_0.png" /></p>
<h2 id="example-2-using-probabilities-as-meta-features">Example 2 - Using Probabilities as Meta-Features</h2>
<p>Alternatively, the class-probabilities of the first-level classifiers can be used to train the meta-classifier (2nd-level classifier) by setting <code>use_probas=True</code>. If <code>average_probas=True</code>, the probabilities of the level-1 classifiers are averaged, if <code>average_probas=False</code>, the probabilities are stacked (recommended). For example, in a 3-class setting with 2 level-1 classifiers, these classifiers may make the following "probability" predictions for 1 training sample:</p>
<ul>
<li>classifier 1: [0.2, 0.5, 0.3]</li>
<li>classifier 2: [0.3, 0.4, 0.4]</li>
</ul>
<p>If <code>average_probas=True</code>, the meta-features would be:</p>
<ul>
<li>[0.25, 0.45, 0.35]</li>
</ul>
<p>In contrast, using <code>average_probas=False</code> results in k features where, k = [n_classes * n_classifiers], by stacking these level-1 probabilities:</p>
<ul>
<li>[0.2, 0.5, 0.3, 0.3, 0.4, 0.4]</li>
</ul>
<pre><code class="language-python">clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],
                          use_probas=True,
                          average_probas=False,
                          meta_classifier=lr)

print('3-fold cross validation:\n')

for clf, label in zip([clf1, clf2, clf3, sclf], 
                      ['KNN', 
                       'Random Forest', 
                       'Naive Bayes',
                       'StackingClassifier']):

    scores = model_selection.cross_val_score(clf, X, y, 
                                              cv=3, scoring='accuracy')
    print(&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot; 
          % (scores.mean(), scores.std(), label))
</code></pre>
<pre><code>3-fold cross validation:

Accuracy: 0.91 (+/- 0.01) [KNN]
Accuracy: 0.95 (+/- 0.01) [Random Forest]
Accuracy: 0.91 (+/- 0.02) [Naive Bayes]
Accuracy: 0.92 (+/- 0.02) [StackingClassifier]
</code></pre>
<h2 id="example-3-stacked-classification-and-gridsearch">Example 3 - Stacked Classification and GridSearch</h2>
<p>The stack allows tuning hyper parameters of the base and meta models! A full list of tunable parameters can be obtained via <code>estimator.get_params().keys()</code>.</p>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from mlxtend.classifier import StackingClassifier

# Initializing models

clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], 
                          meta_classifier=lr)

params = {'kneighborsclassifier__n_neighbors': [1, 5],
          'randomforestclassifier__n_estimators': [10, 50],
          'meta_classifier__C': [0.1, 10.0]}

grid = GridSearchCV(estimator=sclf, 
                    param_grid=params, 
                    cv=5,
                    refit=True)
grid.fit(X, y)

cv_keys = ('mean_test_score', 'std_test_score', 'params')

for r, _ in enumerate(grid.cv_results_['mean_test_score']):
    print(&quot;%0.3f +/- %0.2f %r&quot;
          % (grid.cv_results_[cv_keys[0]][r],
             grid.cv_results_[cv_keys[1]][r] / 2.0,
             grid.cv_results_[cv_keys[2]][r]))

print('Best parameters: %s' % grid.best_params_)
print('Accuracy: %.2f' % grid.best_score_)
</code></pre>
<pre><code>0.933 +/- 0.03 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}
0.940 +/- 0.02 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}
0.927 +/- 0.03 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}
0.947 +/- 0.02 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
0.947 +/- 0.02 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}
0.947 +/- 0.02 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}
0.933 +/- 0.02 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}
0.940 +/- 0.02 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
Best parameters: {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
Accuracy: 0.95
</code></pre>
<p>In case we are planning to use a regression algorithm multiple times, all we need to do is to add an additional number suffix in the parameter grid as shown below:</p>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# Initializing models

clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3], 
                          meta_classifier=lr)

params = {'kneighborsclassifier-1__n_neighbors': [1, 5],
          'kneighborsclassifier-2__n_neighbors': [1, 5],
          'randomforestclassifier__n_estimators': [10, 50],
          'meta_classifier__C': [0.1, 10.0]}

grid = GridSearchCV(estimator=sclf, 
                    param_grid=params, 
                    cv=5,
                    refit=True)
grid.fit(X, y)

cv_keys = ('mean_test_score', 'std_test_score', 'params')

for r, _ in enumerate(grid.cv_results_['mean_test_score']):
    print(&quot;%0.3f +/- %0.2f %r&quot;
          % (grid.cv_results_[cv_keys[0]][r],
             grid.cv_results_[cv_keys[1]][r] / 2.0,
             grid.cv_results_[cv_keys[2]][r]))

print('Best parameters: %s' % grid.best_params_)
print('Accuracy: %.2f' % grid.best_score_)
</code></pre>
<pre><code>0.933 +/- 0.03 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}
0.940 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}
0.927 +/- 0.03 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}
0.947 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
0.940 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}
0.947 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}
0.933 +/- 0.03 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}
0.953 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
0.940 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}
0.947 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}
0.933 +/- 0.03 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}
0.953 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
0.953 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}
0.953 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}
0.933 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}
0.940 +/- 0.02 {'kneighborsclassifier-1__n_neighbors': 5, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
Best parameters: {'kneighborsclassifier-1__n_neighbors': 1, 'kneighborsclassifier-2__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}
Accuracy: 0.95
</code></pre>
<p><strong>Note</strong></p>
<p>The <code>StackingClassifier</code> also enables grid search over the <code>classifiers</code> argument. When there are level-mixed hyperparameters, GridSearchCV will try to replace hyperparameters in a top-down order, i.e., classifers -&gt; single base classifier -&gt; classifier hyperparameter. For instance,  given a hyperparameter grid such as</p>
<pre><code>params = {'randomforestclassifier__n_estimators': [1, 100],
'classifiers': [(clf1, clf1, clf1), (clf2, clf3)]}
</code></pre>
<p>it will first use the instance settings of either (clf1, clf1, clf1) or (clf2, clf3). Then it will replace the <code>'n_estimators'</code> settings for a matching classifier based on <code>'randomforestclassifier__n_estimators': [1, 100]</code>.</p>
<h2 id="example-4-stacking-of-classifiers-that-operate-on-different-feature-subsets">Example 4 - Stacking of Classifiers that Operate on Different Feature Subsets</h2>
<p>The different level-1 classifiers can be fit to different subsets of features in the training dataset. The following example illustrates how this can be done on a technical level using scikit-learn pipelines and the <code>ColumnSelector</code>:</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
from mlxtend.classifier import StackingClassifier
from mlxtend.feature_selection import ColumnSelector
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

iris = load_iris()
X = iris.data
y = iris.target

pipe1 = make_pipeline(ColumnSelector(cols=(0, 2)),
                      LogisticRegression())
pipe2 = make_pipeline(ColumnSelector(cols=(1, 2, 3)),
                      LogisticRegression())

sclf = StackingClassifier(classifiers=[pipe1, pipe2], 
                          meta_classifier=LogisticRegression())

sclf.fit(X, y)
</code></pre>
<pre><code>StackingClassifier(average_probas=False,
                   classifiers=[Pipeline(memory=None,
                                         steps=[('columnselector',
                                                 ColumnSelector(cols=(0, 2),
                                                                drop_axis=False)),
                                                ('logisticregression',
                                                 LogisticRegression(C=1.0,
                                                                    class_weight=None,
                                                                    dual=False,
                                                                    fit_intercept=True,
                                                                    intercept_scaling=1,
                                                                    l1_ratio=None,
                                                                    max_iter=100,
                                                                    multi_class='auto',
                                                                    n_jobs=None,
                                                                    penalty='l2',
                                                                    random_state=None,
                                                                    sol...
                   meta_classifier=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=100,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=None,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   store_train_meta_features=False, use_clones=True,
                   use_features_in_secondary=False, use_probas=False,
                   verbose=0)
</code></pre>
<h2 id="example-5-using-pre-fitted-classifiers">Example 5 - Using Pre-fitted Classifiers</h2>
<p>Assume that we previously fitted our classifiers:</p>
<pre><code class="language-python">from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
import numpy as np

clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()

for clf in (clf1, clf2, clf3):
    clf.fit(X, y)
</code></pre>
<p>By setting <code>fit_base_estimators=False</code>, it will enforce <code>use_clones</code> to be False and the <code>StackingClassifier</code> will not re-fit these classifers to save computational time:</p>
<pre><code class="language-python">from mlxtend.classifier import StackingClassifier
import copy
sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], 
                          meta_classifier=lr, fit_base_estimators=False)

labels = ['KNN', 'Random Forest', 'Naive Bayes', 'StackingClassifier']

sclf.fit(X, y)

print('accuracy:', np.mean(y == sclf.predict(X)))
</code></pre>
<pre><code>Warning: enforce use_clones to be False
accuracy: 1.0
</code></pre>
<p>However, please note that <code>fit_base_estimators=False</code> is incompatible to any form of cross-validation that is done in e.g., <code>model_selection.cross_val_score</code> or <code>model_selection.GridSearchCV</code>, etc., since it would require the classifiers to be refit to the training folds. Thus, only use <code>fit_base_estimators=False</code> if you want to make a prediction directly without cross-validation.</p>
<h2 id="example-6-roc-curve-with-decision_function">Example 6 -- ROC Curve with <code>decision_function</code></h2>
<p>Like other scikit-learn classifiers, the <code>StackingCVClassifier</code> has an <code>decision_function</code> method that can be used for plotting ROC curves. Note that the <code>decision_function</code> expects and requires the meta-classifier to implement a <code>decision_function</code>.</p>
<pre><code class="language-python">from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from mlxtend.classifier import StackingCVClassifier
from sklearn.metrics import roc_curve, auc
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier


iris = datasets.load_iris()
X, y = iris.data[:, [0, 1]], iris.target


# Binarize the output
y = label_binarize(y, classes=[0, 1, 2])
n_classes = y.shape[1]



RANDOM_SEED = 42


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=RANDOM_SEED)

clf1 =  LogisticRegression()
clf2 = RandomForestClassifier(random_state=RANDOM_SEED)
clf3 = SVC(random_state=RANDOM_SEED)
lr = LogisticRegression()


sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],
                          meta_classifier=lr)


# Learn to predict each class against the other
classifier = OneVsRestClassifier(sclf)
</code></pre>
<p><strong>Using <code>predict_proba()</code></strong></p>
<pre><code class="language-python">y_score = classifier.fit(X_train, y_train).predict_proba(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])

plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc=&quot;lower right&quot;)
plt.show()
</code></pre>
<p><img alt="png" src="../StackingClassifier_files/StackingClassifier_37_0.png" /></p>
<p><strong>Using <code>decision_function()</code></strong></p>
<pre><code class="language-python">y_score = classifier.fit(X_train, y_train).decision_function(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])

plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc=&quot;lower right&quot;)
plt.show()
</code></pre>
<p><img alt="png" src="../StackingClassifier_files/StackingClassifier_39_0.png" /></p>
<h1 id="api">API</h1></div>
        
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Copyright &copy; 2014-2020 <a href="http://sebastianraschka.com">Sebastian Raschka</a><br></small>
        
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="../../../js/jquery-1.10.2.min.js"></script>
    <script src="../../../js/bootstrap-3.0.3.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '../../..';
    </script>
    <script data-main="../../../mkdocs/js/search.js" src="../../../mkdocs/js/require.js"></script>
    <script src="../../../js/base.js"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="../../../mathjaxhelper.js"></script>
    <script src="../../../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>
