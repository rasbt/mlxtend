<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Sebastian Raschka">
        <link rel="canonical" href="https://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>feature_importance_permutation: Estimate feature importance via feature permutation. - mlxtend</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/color-brewer.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <link href="../../../cinder/css/base.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../cinder/css/cinder.css" rel="stylesheet">
        <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../../../cinder/css/highlight.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', "UA-38457794-2", "rasbt.github.io/mlxtend/");
            ga('send', 'pageview');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">mlxtend</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../../.." class="nav-link">Home</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">User Guide</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../USER_GUIDE_INDEX/" class="dropdown-item">User Guide Index</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">classifier</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../classifier/Adaline/" class="dropdown-item">Adaline: Adaptive Linear Neuron Classifier</a>
</li>
            
<li>
    <a href="../../classifier/EnsembleVoteClassifier/" class="dropdown-item">EnsembleVoteClassifier: A majority voting classifier</a>
</li>
            
<li>
    <a href="../../classifier/LogisticRegression/" class="dropdown-item">LogisticRegression: A binary classifier</a>
</li>
            
<li>
    <a href="../../classifier/MultiLayerPerceptron/" class="dropdown-item">MultilayerPerceptron: A simple multilayer neural network</a>
</li>
            
<li>
    <a href="../../classifier/OneRClassifier/" class="dropdown-item">OneRClassifier: One Rule (OneR) method for classification</a>
</li>
            
<li>
    <a href="../../classifier/Perceptron/" class="dropdown-item">Perceptron: A simple binary classifier</a>
</li>
            
<li>
    <a href="../../classifier/SoftmaxRegression/" class="dropdown-item">SoftmaxRegression: Multiclass version of logistic regression</a>
</li>
            
<li>
    <a href="../../classifier/StackingClassifier/" class="dropdown-item">StackingClassifier: Simple stacking</a>
</li>
            
<li>
    <a href="../../classifier/StackingCVClassifier/" class="dropdown-item">StackingCVClassifier: Stacking with cross-validation</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">cluster</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../cluster/Kmeans/" class="dropdown-item">Kmeans: k-means clustering</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">data</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../data/autompg_data/" class="dropdown-item">autompg_data: The Auto-MPG dataset for regression</a>
</li>
            
<li>
    <a href="../../data/boston_housing_data/" class="dropdown-item">boston_housing_data: The Boston housing dataset for regression</a>
</li>
            
<li>
    <a href="../../data/iris_data/" class="dropdown-item">iris_data: The 3-class iris dataset for classification</a>
</li>
            
<li>
    <a href="../../data/loadlocal_mnist/" class="dropdown-item">loadlocal_mnist: A function for loading MNIST from the original ubyte files</a>
</li>
            
<li>
    <a href="../../data/make_multiplexer_dataset/" class="dropdown-item">make_multiplexer_dataset: A function for creating multiplexer data</a>
</li>
            
<li>
    <a href="../../data/mnist_data/" class="dropdown-item">mnist_data: A subset of the MNIST dataset for classification</a>
</li>
            
<li>
    <a href="../../data/three_blobs_data/" class="dropdown-item">three_blobs_data: The synthetic blobs for classification</a>
</li>
            
<li>
    <a href="../../data/wine_data/" class="dropdown-item">wine_data: A 3-class wine dataset for classification</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">evaluate</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../accuracy_score/" class="dropdown-item">accuracy_score: Computing standard, balanced, and per-class accuracy</a>
</li>
            
<li>
    <a href="../bias_variance_decomp/" class="dropdown-item">bias_variance_decomp: Bias-variance decomposition for classification and regression losses</a>
</li>
            
<li>
    <a href="../bootstrap/" class="dropdown-item">bootstrap: The ordinary nonparametric boostrap for arbitrary parameters</a>
</li>
            
<li>
    <a href="../bootstrap_point632_score/" class="dropdown-item">bootstrap_point632_score: The .632 and .632+ boostrap for classifier evaluation</a>
</li>
            
<li>
    <a href="../BootstrapOutOfBag/" class="dropdown-item">BootstrapOutOfBag: A scikit-learn compatible version of the out-of-bag bootstrap</a>
</li>
            
<li>
    <a href="../cochrans_q/" class="dropdown-item">cochrans_q: Cochran's Q test for comparing multiple classifiers</a>
</li>
            
<li>
    <a href="../combined_ftest_5x2cv/" class="dropdown-item">combined_ftest_5x2cv: 5x2cv combined F test for classifier comparisons</a>
</li>
            
<li>
    <a href="../confusion_matrix/" class="dropdown-item">confusion_matrix: creating a confusion matrix for model evaluation</a>
</li>
            
<li>
    <a href="../create_counterfactual/" class="dropdown-item">create_counterfactual: Interpreting models via counterfactuals</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active" aria-current="page">feature_importance_permutation: Estimate feature importance via feature permutation.</a>
</li>
            
<li>
    <a href="../ftest/" class="dropdown-item">ftest: F-test for classifier comparisons</a>
</li>
            
<li>
    <a href="../GroupTimeSeriesSplit/" class="dropdown-item">GroupTimeSeriesSplit: A scikit-learn compatible version of the time series validation with groups</a>
</li>
            
<li>
    <a href="../lift_score/" class="dropdown-item">lift_score: Lift score for classification and association rule mining</a>
</li>
            
<li>
    <a href="../mcnemar_table/" class="dropdown-item">mcnemar_table: Contingency table for McNemar's test</a>
</li>
            
<li>
    <a href="../mcnemar_tables/" class="dropdown-item">mcnemar_tables: contingency tables for McNemar's test and Cochran's Q test</a>
</li>
            
<li>
    <a href="../mcnemar/" class="dropdown-item">mcnemar: McNemar's test for classifier comparisons</a>
</li>
            
<li>
    <a href="../paired_ttest_5x2cv/" class="dropdown-item">paired_ttest_5x2cv: 5x2cv paired t test for classifier comparisons</a>
</li>
            
<li>
    <a href="../paired_ttest_kfold_cv/" class="dropdown-item">paired_ttest_kfold_cv: K-fold cross-validated paired t test</a>
</li>
            
<li>
    <a href="../paired_ttest_resampled/" class="dropdown-item">paired_ttest_resample: Resampled paired t test</a>
</li>
            
<li>
    <a href="../permutation_test/" class="dropdown-item">permutation_test: Permutation test for hypothesis testing</a>
</li>
            
<li>
    <a href="../PredefinedHoldoutSplit/" class="dropdown-item">PredefinedHoldoutSplit: Utility for the holdout method compatible with scikit-learn</a>
</li>
            
<li>
    <a href="../RandomHoldoutSplit/" class="dropdown-item">RandomHoldoutSplit: split a dataset into a train and validation subset for validation</a>
</li>
            
<li>
    <a href="../scoring/" class="dropdown-item">scoring: computing various performance metrics</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">feature_extraction</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/" class="dropdown-item">LinearDiscriminantAnalysis: Linear discriminant analysis for dimensionality reduction</a>
</li>
            
<li>
    <a href="../../feature_extraction/PrincipalComponentAnalysis/" class="dropdown-item">PrincipalComponentAnalysis: Principal component analysis (PCA) for dimensionality reduction</a>
</li>
            
<li>
    <a href="../../feature_extraction/RBFKernelPCA/" class="dropdown-item">RBFKernelPCA</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">feature_selection</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../feature_selection/ColumnSelector/" class="dropdown-item">ColumnSelector: Scikit-learn utility function to select specific columns in a pipeline</a>
</li>
            
<li>
    <a href="../../feature_selection/ExhaustiveFeatureSelector/" class="dropdown-item">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</a>
</li>
            
<li>
    <a href="../../feature_selection/SequentialFeatureSelector/" class="dropdown-item">SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants)</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">file_io</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../file_io/find_filegroups/" class="dropdown-item">find_filegroups: Find files that only differ via their file extensions</a>
</li>
            
<li>
    <a href="../../file_io/find_files/" class="dropdown-item">find_files: Find files based on substring matches</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">frequent_patterns</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../frequent_patterns/apriori/" class="dropdown-item">Apriori</a>
</li>
            
<li>
    <a href="../../frequent_patterns/association_rules/" class="dropdown-item">Association rules</a>
</li>
            
<li>
    <a href="../../frequent_patterns/fpgrowth/" class="dropdown-item">Fpgrowth</a>
</li>
            
<li>
    <a href="../../frequent_patterns/fpmax/" class="dropdown-item">Fpmax</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">math</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../math/num_combinations/" class="dropdown-item">num_combinations: combinations for creating subsequences of k elements</a>
</li>
            
<li>
    <a href="../../math/num_permutations/" class="dropdown-item">num_permutations: number of permutations for creating subsequences of k elements</a>
</li>
            
<li>
    <a href="../../math/vectorspace_dimensionality/" class="dropdown-item">vectorspace_dimensionality: compute the number of dimensions that a set of vectors spans</a>
</li>
            
<li>
    <a href="../../math/vectorspace_orthonormalization/" class="dropdown-item">vectorspace_orthonormalization: Converts a set of linearly independent vectors to a set of orthonormal basis vectors</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">plotting</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../plotting/category_scatter/" class="dropdown-item">Scategory_scatter: Create a scatterplot with categories in different colors</a>
</li>
            
<li>
    <a href="../../plotting/checkerboard_plot/" class="dropdown-item">checkerboard_plot: Create a checkerboard plot in matplotlib</a>
</li>
            
<li>
    <a href="../../plotting/plot_pca_correlation_graph/" class="dropdown-item">plot_pca_correlation_graph: plot correlations between original features and principal components</a>
</li>
            
<li>
    <a href="../../plotting/ecdf/" class="dropdown-item">ecdf: Create an empirical cumulative distribution function plot</a>
</li>
            
<li>
    <a href="../../plotting/enrichment_plot/" class="dropdown-item">enrichment_plot: create an enrichment plot for cumulative counts</a>
</li>
            
<li>
    <a href="../../plotting/heatmap/" class="dropdown-item">heatmap: Create a heatmap in matplotlib</a>
</li>
            
<li>
    <a href="../../plotting/plot_confusion_matrix/" class="dropdown-item">plot_confusion_matrix: Visualize confusion matrices</a>
</li>
            
<li>
    <a href="../../plotting/plot_decision_regions/" class="dropdown-item">plot_decision_regions: Visualize the decision regions of a classifier</a>
</li>
            
<li>
    <a href="../../plotting/plot_learning_curves/" class="dropdown-item">plot_learning_curves: Plot learning curves from training and test sets</a>
</li>
            
<li>
    <a href="../../plotting/plot_linear_regression/" class="dropdown-item">plot_linear_regression: A quick way for plotting linear regression fits</a>
</li>
            
<li>
    <a href="../../plotting/plot_sequential_feature_selection/" class="dropdown-item">plot_sequential_feature_selection: Visualize selected feature subset performances from the SequentialFeatureSelector</a>
</li>
            
<li>
    <a href="../../plotting/scatterplotmatrix/" class="dropdown-item">scatterplotmatrix: visualize datasets via a scatter plot matrix</a>
</li>
            
<li>
    <a href="../../plotting/scatter_hist/" class="dropdown-item">scatter_hist: create a scatter histogram plot</a>
</li>
            
<li>
    <a href="../../plotting/stacked_barplot/" class="dropdown-item">stacked_barplot: Plot stacked bar plots in matplotlib</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">preprocessing</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../preprocessing/CopyTransformer/" class="dropdown-item">CopyTransformer: A function that creates a copy of the input array in a scikit-learn pipeline</a>
</li>
            
<li>
    <a href="../../preprocessing/DenseTransformer/" class="dropdown-item">DenseTransformer: Transforms a sparse into a dense NumPy array, e.g., in a scikit-learn pipeline</a>
</li>
            
<li>
    <a href="../../preprocessing/MeanCenterer/" class="dropdown-item">MeanCenterer: column-based mean centering on a NumPy array</a>
</li>
            
<li>
    <a href="../../preprocessing/minmax_scaling/" class="dropdown-item">MinMaxScaling: Min-max scaling fpr pandas DataFrames and NumPy arrays</a>
</li>
            
<li>
    <a href="../../preprocessing/one-hot_encoding/" class="dropdown-item">One hot encoding</a>
</li>
            
<li>
    <a href="../../preprocessing/shuffle_arrays_unison/" class="dropdown-item">shuffle_arrays_unison: shuffle arrays in a consistent fashion</a>
</li>
            
<li>
    <a href="../../preprocessing/standardize/" class="dropdown-item">standardize: A function to standardize columns in a 2D NumPy array</a>
</li>
            
<li>
    <a href="../../preprocessing/TransactionEncoder/" class="dropdown-item">TransactionEncoder</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">regressor</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../regressor/LinearRegression/" class="dropdown-item">LinearRegression: An implementation of ordinary least-squares linear regression</a>
</li>
            
<li>
    <a href="../../regressor/StackingCVRegressor/" class="dropdown-item">StackingCVRegressor: stacking with cross-validation for regression</a>
</li>
            
<li>
    <a href="../../regressor/StackingRegressor/" class="dropdown-item">StackingRegressor: a simple stacking implementation for regression</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">text</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../text/generalize_names/" class="dropdown-item">generalize_names: convert names into a generalized format</a>
</li>
            
<li>
    <a href="../../text/generalize_names_duplcheck/" class="dropdown-item">generalize_names_duplcheck: Generalize names while preventing duplicates among different names</a>
</li>
            
<li>
    <a href="../../text/tokenizer/" class="dropdown-item">tokenizer_emoticons: tokenizers for emoticons</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">utils</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../utils/Counter/" class="dropdown-item">Counter: A simple progress counter</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">API</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.classifier/" class="dropdown-item">Mlxtend.classifier</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.cluster/" class="dropdown-item">Mlxtend.cluster</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.data/" class="dropdown-item">Mlxtend.data</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.evaluate/" class="dropdown-item">Mlxtend.evaluate</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.feature_extraction/" class="dropdown-item">Mlxtend.feature extraction</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.feature_selection/" class="dropdown-item">Mlxtend.feature selection</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.file_io/" class="dropdown-item">Mlxtend.file io</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/" class="dropdown-item">Mlxtend.frequent patterns</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.plotting/" class="dropdown-item">Mlxtend.plotting</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.preprocessing/" class="dropdown-item">Mlxtend.preprocessing</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.regressor/" class="dropdown-item">Mlxtend.regressor</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.text/" class="dropdown-item">Mlxtend.text</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.utils/" class="dropdown-item">Mlxtend.utils</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item">
                                <a href="../../../installation/" class="nav-link">Installation</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">About</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../CHANGELOG/" class="dropdown-item">Release Notes</a>
</li>
                                    
<li>
    <a href="../../../Code-of-Conduct/" class="dropdown-item">Code of Conduct</a>
</li>
                                    
<li>
    <a href="../../../CONTRIBUTING/" class="dropdown-item">How To Contribute</a>
</li>
                                    
<li>
    <a href="../../../contributors/" class="dropdown-item">Contributors</a>
</li>
                                    
<li>
    <a href="../../../license/" class="dropdown-item">License</a>
</li>
                                    
<li>
    <a href="../../../cite/" class="dropdown-item">Citing Mlxtend</a>
</li>
                                    
<li>
    <a href="../../../discuss/" class="dropdown-item">Discuss</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../create_counterfactual/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../ftest/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/rasbt/mlxtend/tree/master/docs/sources/user_guide/evaluate/feature_importance_permutation.md" class="nav-link"><i class="fa-brands fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#feature_importance_permutation-estimate-feature-importance-via-feature-permutation" class="nav-link">feature_importance_permutation: Estimate feature importance via feature permutation.</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#overview" class="nav-link">Overview</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#example-1-feature-importance-for-classifiers" class="nav-link">Example 1 -- Feature Importance for Classifiers</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#example-2-feature-importance-for-regressors" class="nav-link">Example 2 -- Feature Importance for Regressors</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#example-3-feature-importance-with-one-hot-encoded-features" class="nav-link">Example 3 -- Feature Importance With One-Hot-Encoded Features</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#api" class="nav-link">API</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="feature_importance_permutation-estimate-feature-importance-via-feature-permutation">feature_importance_permutation: Estimate feature importance via feature permutation.</h1>
<p>A function to estimate the feature importance of classifiers and regressors based on <em>permutation importance</em>.</p>
<blockquote>
<p><code>from mlxtend.evaluate import feature_importance_permutation</code>    </p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>The <em>permutation importance</em> is an intuitive, model-agnostic method to estimate the feature importance for classifier and regression models. The approach is relatively simple and straight-forward:</p>
<ol>
<li>Take a model that was fit to the training dataset</li>
<li>Estimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance</li>
<li>For each feature <em>i</em>:</li>
<li>randomly permute feature column <em>i</em> in the original dataset</li>
<li>record the predictive performance of the model on the dataset with the permuted column </li>
<li>compute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset</li>
</ol>
<p>Permutation importance is generally considered as a relatively efficient technique that works well in practice [1], while a drawback is that the importance of correlated features may be overestimated [2].</p>
<h3 id="references">References</h3>
<ul>
<li>[1] Terence Parr, Kerem Turgutlu, Christopher Csiszar, and Jeremy Howard. <em>Beware Default Random Forest Importances</em> (https://parrt.cs.usfca.edu/doc/rf-importance/index.html)</li>
<li>[2] Strobl, C., Boulesteix, A. L., Kneib, T., Augustin, T., &amp; Zeileis, A. (2008). Conditional variable importance for random forests. BMC bioinformatics, 9(1), 307.</li>
</ul>
<h2 id="example-1-feature-importance-for-classifiers">Example 1 -- Feature Importance for Classifiers</h2>
<p>The following example illustrates the feature importance estimation via permutation importance based for classification models.</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from mlxtend.evaluate import feature_importance_permutation
</code></pre>
<h4 id="generate-a-toy-dataset">Generate a toy dataset</h4>
<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

# Build a classification task using 3 informative features
X, y = make_classification(n_samples=10000,
                           n_features=10,
                           n_informative=3,
                           n_redundant=0,
                           n_repeated=0,
                           n_classes=2,
                           random_state=0,
                           shuffle=False)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1, stratify=y)
</code></pre>
<h4 id="feature-importance-via-random-forest">Feature importance via random forest</h4>
<p>First, we compute the feature importance directly from the random forest via <em>mean impurity decrease</em> (described after the code section):</p>
<pre><code class="language-python">forest = RandomForestClassifier(n_estimators=250,
                                random_state=0)

forest.fit(X_train, y_train)

print('Training accuracy:', np.mean(forest.predict(X_train) == y_train)*100)
print('Test accuracy:', np.mean(forest.predict(X_test) == y_test)*100)

importance_vals = forest.feature_importances_
print(importance_vals)
</code></pre>
<pre><code>Training accuracy: 100.0
Test accuracy: 95.06666666666666
[0.283357   0.30846795 0.24204291 0.02229767 0.02364941 0.02390578
 0.02501543 0.0234225  0.02370816 0.0241332 ]
</code></pre>
<p>There are several strategies for computing the feature importance in random forest. The method implemented in scikit-learn (used in the next code example) is based on the Breiman and Friedman's CART (Breiman, Friedman, "Classification and regression trees", 1984), the so-called <em>mean impurity decrease</em>. Here, the importance value of a features is computed by averaging the impurity decrease for that feature, when splitting a parent node into two child nodes, across all the trees in the ensemble. Note that the impurity decrease values are weighted by the number of samples that are in the respective nodes. This process is repeated for all features in the dataset, and the feature importance values are then normalized so that they sum up to 1. In CART, the authors also note that this fast way of computing feature importance values is relatively consistent with the permutation importance.</p>
<p>Next, let's visualize the feature importance values from the random forest including a measure of the <em>mean impurity decrease</em> variability (here: standard deviation):</p>
<pre><code class="language-python">std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importance_vals)[::-1]

# Plot the feature importances of the forest
plt.figure()
plt.title(&quot;Random Forest feature importance&quot;)
plt.bar(range(X.shape[1]), importance_vals[indices],
        yerr=std[indices], align=&quot;center&quot;)
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.ylim([0, 0.5])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_17_0.png" /></p>
<p>As we can see, the features 1, 0, and 2 are estimated to be the most informative ones for the random forest classier. Next, let's compute the feature importance via the permutation importance approach.</p>
<h4 id="permutation-importance">Permutation Importance</h4>
<pre><code class="language-python">imp_vals, _ = feature_importance_permutation(
    predict_method=forest.predict, 
    X=X_test,
    y=y_test,
    metric='accuracy',
    num_rounds=1,
    seed=1)

imp_vals
</code></pre>
<pre><code>array([ 0.26833333,  0.26733333,  0.261     , -0.002     , -0.00033333,
        0.00066667,  0.00233333,  0.00066667,  0.00066667, -0.00233333])
</code></pre>
<p>Note that the <code>feature_importance_permutation</code> returns two arrays. The first array (here: <code>imp_vals</code>) contains the actual importance values we are interested in. If <code>num_rounds &gt; 1</code>, the permutation is repeated multiple times (with different random seeds), and in this case the first array contains the average value of the importance computed from the different runs. The second array (here, assigned to <code>_</code>, because we are not using it) then contains all individual values from these runs (more about that later).</p>
<p>Now, let's also visualize the importance values in a barplot:</p>
<pre><code class="language-python">indices = np.argsort(imp_vals)[::-1]
plt.figure()
plt.title(&quot;Random Forest feature importance via permutation importance&quot;)
plt.bar(range(X.shape[1]), imp_vals[indices])
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.ylim([0, 0.5])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_23_0.png" /></p>
<p>As we can see, also here, features 1, 0, and 2 are predicted to be the most important ones, which is consistent with the feature importance values that we computed via the <em>mean impurity decrease</em> method earlier.</p>
<p>(Note that in the context of random forests, the feature importance via permutation importance is typically computed using the out-of-bag samples of a random forest, whereas in this implementation, an independent dataset is used.)</p>
<p>Previously, it was mentioned that the permutation is repeated multiple times if <code>num_rounds &gt; 1</code>. In this case, the second array returned by the <code>feature_importance_permutation</code> contains the importance values for these individual runs (the array has shape [num_features, num_rounds), which we can use to compute some sort of variability between these runs. </p>
<pre><code class="language-python">imp_vals, imp_all = feature_importance_permutation(
    predict_method=forest.predict, 
    X=X_test,
    y=y_test,
    metric='accuracy',
    num_rounds=10,
    seed=1)


std = np.std(imp_all, axis=1)
indices = np.argsort(imp_vals)[::-1]

plt.figure()
plt.title(&quot;Random Forest feature importance via permutation importance w. std. dev.&quot;)
plt.bar(range(X.shape[1]), imp_vals[indices],
        yerr=std[indices])
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_27_0.png" /></p>
<p>It shall be noted that the feature importance values do not sum up to one, since they are not normalized (you can normalize them if you'd like, by dividing these by the sum of importance values). Here, the main point is to look at the importance values relative to each other and not to over-interpret the absolute values.</p>
<h4 id="support-vector-machines">Support Vector Machines</h4>
<p>While the permutation importance approach yields results that are generally consistent with the <em>mean impurity decrease</em> feature importance values from a random forest, it's a method that is model-agnostic and can be used with any kind of classifier or regressor. The example below applies the <code>feature_importance_permutation</code> function to a support vector machine:</p>
<pre><code class="language-python">from sklearn.svm import SVC


svm = SVC(C=1.0, kernel='rbf')
svm.fit(X_train, y_train)

print('Training accuracy', np.mean(svm.predict(X_train) == y_train)*100)
print('Test accuracy', np.mean(svm.predict(X_test) == y_test)*100)
</code></pre>
<pre><code>Training accuracy 94.87142857142857
Test accuracy 94.89999999999999
</code></pre>
<pre><code class="language-python">imp_vals, imp_all = feature_importance_permutation(
    predict_method=svm.predict, 
    X=X_test,
    y=y_test,
    metric='accuracy',
    num_rounds=10,
    seed=1)


std = np.std(imp_all, axis=1)
indices = np.argsort(imp_vals)[::-1]

plt.figure()
plt.title(&quot;SVM feature importance via permutation importance&quot;)
plt.bar(range(X.shape[1]), imp_vals[indices],
        yerr=std[indices])
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_32_0.png" /></p>
<h2 id="example-2-feature-importance-for-regressors">Example 2 -- Feature Importance for Regressors</h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from mlxtend.evaluate import feature_importance_permutation
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
from sklearn.svm import SVR


X, y = make_regression(n_samples=1000,
                       n_features=5,
                       n_informative=2,
                       n_targets=1,
                       random_state=123,
                       shuffle=False)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123)    

svm = SVR(kernel='rbf')
svm.fit(X_train, y_train)

imp_vals, _ = feature_importance_permutation(
    predict_method=svm.predict, 
    X=X_test,
    y=y_test,
    metric='r2',
    num_rounds=1,
    seed=1)

imp_vals
</code></pre>
<pre><code>array([ 0.43309137,  0.22058866,  0.00148447,  0.01613953, -0.00529505])
</code></pre>
<pre><code class="language-python">plt.figure()
plt.bar(range(X.shape[1]), imp_vals)
plt.xticks(range(X.shape[1]))
plt.xlim([-1, X.shape[1]])
plt.ylim([0, 0.5])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_35_0.png" /></p>
<h2 id="example-3-feature-importance-with-one-hot-encoded-features">Example 3 -- Feature Importance With One-Hot-Encoded Features</h2>
<p>Upon one-hot encoding, a feature variable with 10 distinct categories will be split into 10 new feature columns (or 9, if you drop a redundant column). If we want to treat each new feature column as an individual feature variable, we can use the <code>feature_importance_permutation</code> as usual. </p>
<p>This is illustrated in the example below.</p>
<h3 id="preparing-the-dataset">Preparing the Dataset</h3>
<p>Here, we look at a dataset that consists of one categorical feature (<code>'categorical'</code>) and 3 numerical features (<code>'measurement1'</code>, <code>'measurement2'</code>, and <code>'measurement3'</code>).</p>
<pre><code class="language-python">import pandas as pd


df_data = pd.read_csv('https://gist.githubusercontent.com/rasbt/b99bf69079bc0d601eeae8a49248d358/raw/a114be9801647ec5460089f3a9576713dabf5f1f/onehot-numeric-mixed-data.csv')
df_data.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>categorical</th>
      <th>measurement1</th>
      <th>measurement2</th>
      <th>label</th>
      <th>measurement3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>F</td>
      <td>1.428571</td>
      <td>2.721313</td>
      <td>0</td>
      <td>2.089</td>
    </tr>
    <tr>
      <th>1</th>
      <td>R</td>
      <td>0.685939</td>
      <td>0.982976</td>
      <td>0</td>
      <td>0.637</td>
    </tr>
    <tr>
      <th>2</th>
      <td>P</td>
      <td>1.055817</td>
      <td>0.624210</td>
      <td>0</td>
      <td>0.226</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S</td>
      <td>0.995956</td>
      <td>0.321101</td>
      <td>0</td>
      <td>0.138</td>
    </tr>
    <tr>
      <th>4</th>
      <td>R</td>
      <td>1.376773</td>
      <td>1.578309</td>
      <td>0</td>
      <td>0.478</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">from sklearn.model_selection import train_test_split


df_X = df_data[['measurement1', 'measurement2', 'measurement3', 'categorical']]
df_y = df_data['label']


df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(
     df_X, df_y, test_size=0.33, random_state=42, stratify=df_y)
</code></pre>
<p>Here, we do the one-hot encoding on the categorical feature and merge it back with the numerical columns:</p>
<pre><code class="language-python">from sklearn.preprocessing import OneHotEncoder
import numpy as np


ohe = OneHotEncoder(drop='first')
ohe.fit(df_X_train[['categorical']])

df_X_train_ohe = df_X_train.drop(columns=['categorical'])
df_X_test_ohe = df_X_test.drop(columns=['categorical'])

ohe_train = np.asarray(ohe.transform(df_X_train[['categorical']]).todense())
ohe_test = np.asarray(ohe.transform(df_X_test[['categorical']]).todense())

X_train_ohe = np.hstack((df_X_train_ohe.values, ohe_train))
X_test_ohe = np.hstack((df_X_test_ohe.values, ohe_test))
</code></pre>
<pre><code class="language-python"># look at first 3 rows
print(X_train_ohe[:3])
</code></pre>
<pre><code>[[0.65747208 0.95105388 0.36       0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         1.         0.         0.
  0.         0.         0.        ]
 [1.17503636 1.01094494 0.653      0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         1.         0.
  0.         0.         0.        ]
 [1.25516647 0.67575824 0.176      0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         1.         0.         0.         0.
  0.         0.         0.        ]]
</code></pre>
<h3 id="fitting-a-baseline-model-for-analysis">Fitting a Baseline Model for Analysis</h3>
<pre><code class="language-python">from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV


pipe = make_pipeline(StandardScaler(),
                     MLPClassifier(max_iter=10000, random_state=123))

params = {
    'mlpclassifier__hidden_layer_sizes': [(30, 20, 10), 
                                          (20, 10), 
                                          (20,),
                                          (10,)],
    'mlpclassifier__activation': ['tanh', 'relu'],
    'mlpclassifier__solver': ['sgd'],
    'mlpclassifier__alpha': [0.0001],
    'mlpclassifier__learning_rate': ['adaptive'],
}

gs = GridSearchCV(estimator=pipe, 
                  param_grid=params, 
                  scoring='accuracy', 
                  refit=True,
                  n_jobs=-1,
                  cv=10)

gs = gs.fit(X_train_ohe, df_y_train.values)
model = gs.best_estimator_
</code></pre>
<h3 id="regular-permutation-importance">Regular Permutation Importance</h3>
<p>Here, we compute the feature importance the regular way were each one-hot encoded feature is treated as an individiual variable.</p>
<pre><code class="language-python">from mlxtend.evaluate import feature_importance_permutation
</code></pre>
<pre><code class="language-python">imp_vals, imp_all = feature_importance_permutation(
    predict_method=model.predict, 
    X=X_test_ohe,
    y=df_y_test.values,
    metric='accuracy',
    num_rounds=50,
    seed=1)
</code></pre>
<pre><code class="language-python">feat_names_with_ohe = ['measurement1', 'measurement2', 'measurement3'] \
    + [f'categorical_ohe_{i}' for i in range(2, 20)]
</code></pre>
<pre><code class="language-python">%matplotlib inline
import matplotlib.pyplot as plt


std = np.std(imp_all, axis=1)
indices = np.argsort(imp_vals)[::-1]

plt.figure()
#plt.title(&quot;Feature importance via permutation importance w. std. dev.&quot;)
plt.bar(range(len(feat_names_with_ohe)), imp_vals[indices],
        yerr=std[indices])
plt.xticks(range(len(feat_names_with_ohe)),
           np.array(feat_names_with_ohe)[indices], rotation=90)
plt.xlim([-1, len(feat_names_with_ohe)])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_52_0.png" /></p>
<p>However, notice that if we have a lot of categorical values, the feature importance of the individual binary features (after one hot encoding) are now hard to interpret. In certain cases, it would be desireable to treat the one-hot encoded binary features as a single variable in feature permutation importance evaluation. We can achieve this using feature groups.</p>
<h3 id="with-feature-groups">With Feature Groups</h3>
<p>In the example below, all the one-hot encoded variables are treated as a feature group. This means, they are are all shuffled and analyzed as a single feature inside the feature permutation importance analysis.</p>
<pre><code class="language-python">feature_groups = [0, 1, 2, range(3, 21)]

imp_vals, imp_all = feature_importance_permutation(
    predict_method=model.predict, 
    X=X_test_ohe,
    y=df_y_test.values,
    metric='accuracy',
    num_rounds=50,
    feature_groups=feature_groups,
    seed=1)
</code></pre>
<pre><code class="language-python">feature_names = ['measurement1', 'measurement2', 'measurement3', 'categorical']

std = np.std(imp_all, axis=1)
indices = np.argsort(imp_vals)[::-1]

plt.figure()
plt.bar(range(len(feature_names)), imp_vals[indices],
        yerr=std[indices])
plt.xticks(range(len(feature_names)),
           np.array(feature_names)[indices], rotation=90)
plt.xlim([-1, len(feature_names)])
plt.show()
</code></pre>
<p><img alt="png" src="../feature_importance_permutation_files/feature_importance_permutation_57_0.png" /></p>
<h2 id="api">API</h2>
<p><em>feature_importance_permutation(X, y, predict_method, metric, num_rounds=1, feature_groups=None, seed=None)</em></p>
<p>Feature importance imputation via permutation importance</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : NumPy array, shape = [n_samples, n_features]</p>
<p>Dataset, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : NumPy array, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>predict_method</code> : prediction function</p>
<p>A callable function that predicts the target values
from X.</p>
</li>
<li>
<p><code>metric</code> : str, callable</p>
<p>The metric for evaluating the feature importance through
permutation. By default, the strings 'accuracy' is
recommended for classifiers and the string 'r2' is
recommended for regressors. Optionally, a custom
scoring function (e.g., <code>metric=scoring_func</code>) that
accepts two arguments, y_true and y_pred, which have
similar shape to the <code>y</code> array.</p>
</li>
<li>
<p><code>num_rounds</code> : int (default=1)</p>
<p>Number of rounds the feature columns are permuted to
compute the permutation importance.</p>
</li>
<li>
<p><code>feature_groups</code> : list or None (default=None)</p>
<p>Optional argument for treating certain features as a group.
For example <code>[1, 2, [3, 4, 5]]</code>, which can be useful for
interpretability, for example, if features 3, 4, 5 are one-hot
encoded features.</p>
</li>
<li>
<p><code>seed</code> : int or None (default=None)</p>
<p>Random seed for permuting the feature columns.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>mean_importance_vals, all_importance_vals</code> : NumPy arrays.</p>
<p>The first array, mean_importance_vals has shape [n_features, ] and
contains the importance values for all features.
The shape of the second array is [n_features, num_rounds] and contains
the feature importance for each repetition. If num_rounds=1,
it contains the same values as the first array, mean_importance_vals.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    https://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/</p>
<p>ython
ython</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2014-2023 <a href="https://sebastianraschka.com">Sebastian Raschka</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../../../mathjaxhelper.js"></script>
        <script src="../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
