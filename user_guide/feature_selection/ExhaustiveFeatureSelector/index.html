<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Sebastian Raschka">
        <link rel="canonical" href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/">
        <link rel="shortcut icon" href="../../../favicon.ico">
        
        <title>Exhaustive Feature Selector - mlxtend</title>
        <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="../../../cinder/css/base.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../cinder/css/cinder.css" rel="stylesheet">
        <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../../../cinder/css/highlight.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">mlxtend</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../../..">Home</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../USER_GUIDE_INDEX/">User Guide Index</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">classifier</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../classifier/Adaline/">Adaptive Linear Neuron -- Adaline</a>
</li>
            
<li >
    <a href="../../classifier/EnsembleVoteClassifier/">EnsembleVoteClassifier</a>
</li>
            
<li >
    <a href="../../classifier/LogisticRegression/">Logistic Regression</a>
</li>
            
<li >
    <a href="../../classifier/MultiLayerPerceptron/">Neural Network - Multilayer Perceptron</a>
</li>
            
<li >
    <a href="../../classifier/Perceptron/">Perceptron</a>
</li>
            
<li >
    <a href="../../classifier/SoftmaxRegression/">Softmax Regression</a>
</li>
            
<li >
    <a href="../../classifier/StackingClassifier/">StackingClassifier</a>
</li>
            
<li >
    <a href="../../classifier/StackingCVClassifier/">StackingCVClassifier</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">cluster</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../cluster/Kmeans/">Kmeans</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">data</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../data/autompg_data/">Auto MPG</a>
</li>
            
<li >
    <a href="../../data/boston_housing_data/">Boston Housing Data</a>
</li>
            
<li >
    <a href="../../data/iris_data/">Iris Dataset</a>
</li>
            
<li >
    <a href="../../data/loadlocal_mnist/">Load the MNIST Dataset from Local Files</a>
</li>
            
<li >
    <a href="../../data/make_multiplexer_dataset/">Make Multiplexer Dataset</a>
</li>
            
<li >
    <a href="../../data/mnist_data/">MNIST Dataset</a>
</li>
            
<li >
    <a href="../../data/three_blobs_data/">Three Blobs Dataset</a>
</li>
            
<li >
    <a href="../../data/wine_data/">Wine Dataset</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">evaluate</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../evaluate/bootstrap/">Bootstrap</a>
</li>
            
<li >
    <a href="../../evaluate/bootstrap_point632_score/">bootstrap_point632_score</a>
</li>
            
<li >
    <a href="../../evaluate/BootstrapOutOfBag/">BootstrapOutOfBag</a>
</li>
            
<li >
    <a href="../../evaluate/cochrans_q/">Cochran's Q Test</a>
</li>
            
<li >
    <a href="../../evaluate/combined_ftest_5x2cv/">5x2cv combined *F* test</a>
</li>
            
<li >
    <a href="../../evaluate/confusion_matrix/">Confusion Matrix</a>
</li>
            
<li >
    <a href="../../evaluate/feature_importance_permutation/">Feature Importance Permutation</a>
</li>
            
<li >
    <a href="../../evaluate/ftest/">F-Test</a>
</li>
            
<li >
    <a href="../../evaluate/lift_score/">Lift Score</a>
</li>
            
<li >
    <a href="../../evaluate/mcnemar_table/">Contigency Table for McNemar's Test</a>
</li>
            
<li >
    <a href="../../evaluate/mcnemar_tables/">Contigency Tables for McNemar's Test and Cochran's Q Test</a>
</li>
            
<li >
    <a href="../../evaluate/mcnemar/">McNemar's Test</a>
</li>
            
<li >
    <a href="../../evaluate/paired_ttest_5x2cv/">5x2cv paired *t* test</a>
</li>
            
<li >
    <a href="../../evaluate/paired_ttest_kfold_cv/">K-fold cross-validated paired *t* test</a>
</li>
            
<li >
    <a href="../../evaluate/paired_ttest_resampled/">Resampled paired *t* test</a>
</li>
            
<li >
    <a href="../../evaluate/permutation_test/">Permutation Test</a>
</li>
            
<li >
    <a href="../../evaluate/PredefinedHoldoutSplit/">PredefinedHoldoutSplit</a>
</li>
            
<li >
    <a href="../../evaluate/RandomHoldoutSplit/">RandomHoldoutSplit</a>
</li>
            
<li >
    <a href="../../evaluate/scoring/">Scoring</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">feature_extraction</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/">Linear Discriminant Analysis</a>
</li>
            
<li >
    <a href="../../feature_extraction/PrincipalComponentAnalysis/">Principal Component Analysis</a>
</li>
            
<li >
    <a href="../../feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">feature_selection</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../ColumnSelector/">ColumnSelector</a>
</li>
            
<li class="active">
    <a href="./">Exhaustive Feature Selector</a>
</li>
            
<li >
    <a href="../SequentialFeatureSelector/">Sequential Feature Selector</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">file_io</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../file_io/find_filegroups/">Find Filegroups</a>
</li>
            
<li >
    <a href="../../file_io/find_files/">Find Files</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">frequent_patterns</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../frequent_patterns/apriori/">Apriori</a>
</li>
            
<li >
    <a href="../../frequent_patterns/association_rules/">Association rules</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">general concepts</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../general_concepts/activation-functions/">Activation Functions for Artificial Neural Networks</a>
</li>
            
<li >
    <a href="../../general_concepts/gradient-optimization/">Gradient Descent and Stochastic Gradient Descent</a>
</li>
            
<li >
    <a href="../../general_concepts/linear-gradient-derivative/">Deriving the Gradient Descent Rule for Linear Regression and Adaline</a>
</li>
            
<li >
    <a href="../../general_concepts/regularization-linear/">Regularization of Generalized Linear Models</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">image</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../image/extract_face_landmarks/">Extract Face Landmarks</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">math</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../math/num_combinations/">Compute the Number of Combinations</a>
</li>
            
<li >
    <a href="../../math/num_permutations/">Compute the Number of Permutations</a>
</li>
            
<li >
    <a href="../../math/vectorspace_dimensionality/">Vectorspace Dimensionality</a>
</li>
            
<li >
    <a href="../../math/vectorspace_orthonormalization/">Vectorspace Orthonormalization</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">plotting</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../plotting/category_scatter/">Scatterplot with Categories</a>
</li>
            
<li >
    <a href="../../plotting/checkerboard_plot/">Checkerboard Plot</a>
</li>
            
<li >
    <a href="../../plotting/ecdf/">Empirical Cumulative Distribution Function Plot</a>
</li>
            
<li >
    <a href="../../plotting/enrichment_plot/">Enrichment Plot</a>
</li>
            
<li >
    <a href="../../plotting/plot_confusion_matrix/">Confusion Matrix</a>
</li>
            
<li >
    <a href="../../plotting/plot_decision_regions/">Plotting Decision Regions</a>
</li>
            
<li >
    <a href="../../plotting/plot_learning_curves/">Plotting Learning Curves</a>
</li>
            
<li >
    <a href="../../plotting/plot_linear_regression/">Linear Regression Plot</a>
</li>
            
<li >
    <a href="../../plotting/plot_sequential_feature_selection/">Plot Sequential Feature Selection</a>
</li>
            
<li >
    <a href="../../plotting/scatterplotmatrix/">Scatter Plot Matrix</a>
</li>
            
<li >
    <a href="../../plotting/stacked_barplot/">Stacked Barplot</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">preprocessing</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../preprocessing/CopyTransformer/">CopyTransformer</a>
</li>
            
<li >
    <a href="../../preprocessing/DenseTransformer/">DenseTransformer</a>
</li>
            
<li >
    <a href="../../preprocessing/MeanCenterer/">Mean Centerer</a>
</li>
            
<li >
    <a href="../../preprocessing/minmax_scaling/">MinMax Scaling</a>
</li>
            
<li >
    <a href="../../preprocessing/one-hot_encoding/">One hot encoding</a>
</li>
            
<li >
    <a href="../../preprocessing/shuffle_arrays_unison/">Shuffle Arrays in Unison</a>
</li>
            
<li >
    <a href="../../preprocessing/standardize/">Standardize</a>
</li>
            
<li >
    <a href="../../preprocessing/TransactionEncoder/">TransactionEncoder</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">regressor</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../regressor/LinearRegression/">LinearRegression</a>
</li>
            
<li >
    <a href="../../regressor/StackingCVRegressor/">StackingCVRegressor</a>
</li>
            
<li >
    <a href="../../regressor/StackingRegressor/">StackingRegressor</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">text</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../text/generalize_names/">Generalize Names</a>
</li>
            
<li >
    <a href="../../text/generalize_names_duplcheck/">Generalize Names & Duplicate Checking</a>
</li>
            
<li >
    <a href="../../text/tokenizer/">Tokenizer</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">utils</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../utils/Counter/">Counter</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.classifier/">Mlxtend.classifier</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.cluster/">Mlxtend.cluster</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.data/">Mlxtend.data</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.feature_selection/">Mlxtend.feature selection</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.file_io/">Mlxtend.file io</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.plotting/">Mlxtend.plotting</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.regressor/">Mlxtend.regressor</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.text/">Mlxtend.text</a>
</li>
                                    
<li >
    <a href="../../../api_subpackages/mlxtend.utils/">Mlxtend.utils</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../../installation/">Installation</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../CHANGELOG/">Release Notes</a>
</li>
                                    
<li >
    <a href="../../../CONTRIBUTING/">How To Contribute</a>
</li>
                                    
<li >
    <a href="../../../contributors/">Contributors</a>
</li>
                                    
<li >
    <a href="../../../license/">License</a>
</li>
                                    
<li >
    <a href="../../../cite/">Citing Mlxtend</a>
</li>
                                    
<li >
    <a href="../../../discuss/">Discuss</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../ColumnSelector/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../SequentialFeatureSelector/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/rasbt/mlxtend/edit/master/docs/user_guide/feature_selection/ExhaustiveFeatureSelector.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#exhaustive-feature-selector">Exhaustive Feature Selector</a></li>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#example-1-a-simple-iris-example">Example 1 - A simple Iris Example</a></li>
            <li><a href="#example-2-visualizing-the-feature-selection-results">Example 2 - Visualizing the feature selection results</a></li>
            <li><a href="#example-3-exhaustive-feature-selection-for-regression">Example 3 - Exhaustive Feature Selection for Regression</a></li>
            <li><a href="#example-4-using-the-selected-feature-subset-for-making-new-predictions">Example 4 - Using the Selected Feature Subset For Making New Predictions</a></li>
            <li><a href="#example-5-exhaustive-feature-selection-and-gridsearch">Example 5 - Exhaustive Feature Selection and GridSearch</a></li>
            <li><a href="#example-6-working-with-pandas-dataframes">Example 6 - Working with pandas DataFrames</a></li>
            <li><a href="#api">API</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="exhaustive-feature-selector">Exhaustive Feature Selector</h1>
<p>Implementation of an <em>exhaustive feature selector</em> for sampling and evaluating all possible feature combinations in a specified range.</p>
<blockquote>
<p>from mlxtend.feature_selection import ExhaustiveFeatureSelector</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>This exhaustive feature selection algorithm is a wrapper approach for brute-force evaluation of feature subsets; the best subset is selected by optimizing a specified performance metric given an arbitrary regressor or classifier. For instance, if the classifier is a logistic regression and the dataset consists of 4 features, the alogorithm will evaluate all 15 feature combinations (if <code>min_features=1</code> and <code>max_features=4</code>)</p>
<ul>
<li>{0}</li>
<li>{1}</li>
<li>{2}</li>
<li>{3}</li>
<li>{0, 1}</li>
<li>{0, 2}</li>
<li>{0, 3}</li>
<li>{1, 2}</li>
<li>{1, 3}</li>
<li>{2, 3}</li>
<li>{0, 1, 2}</li>
<li>{0, 1, 3}</li>
<li>{0, 2, 3}</li>
<li>{1, 2, 3}</li>
<li>{0, 1, 2, 3}</li>
</ul>
<p>and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.</p>
<h2 id="example-1-a-simple-iris-example">Example 1 - A simple Iris Example</h2>
<p>Initializing a simple classifier from scikit-learn:</p>
<pre><code class="python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

efs1 = efs1.fit(X, y)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>

<pre><code>Features: 15/15

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('0', '2', '3')
</code></pre>
<p>Note that in the example above, the  'best_feature_names_' are simply a string equivalent of the feature indices. However, we can provide custom feature names to the <code>fit</code> function for this mapping:</p>
<pre><code class="python">feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width')
efs1 = efs1.fit(X, y, custom_feature_names=feature_names)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>

<pre><code>Features: 15/15

Best subset (corresponding names): ('sepal length', 'petal length', 'petal width')
</code></pre>
<p>Via the <code>subsets_</code> attribute, we can take a look at the selected feature indices at each step:</p>
<pre><code class="python">efs1.subsets_
</code></pre>

<pre><code>{0: {'avg_score': 0.65999999999999992,
  'cv_scores': array([ 0.53333333,  0.63333333,  0.73333333,  0.76666667,  0.63333333]),
  'feature_idx': (0,),
  'feature_names': ('sepal length',)},
 1: {'avg_score': 0.56666666666666665,
  'cv_scores': array([ 0.53333333,  0.63333333,  0.6       ,  0.5       ,  0.56666667]),
  'feature_idx': (1,),
  'feature_names': ('sepal width',)},
 2: {'avg_score': 0.95333333333333337,
  'cv_scores': array([ 0.93333333,  1.        ,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (2,),
  'feature_names': ('petal length',)},
 3: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.86666667,  1.        ]),
  'feature_idx': (3,),
  'feature_names': ('petal width',)},
 4: {'avg_score': 0.72666666666666668,
  'cv_scores': array([ 0.66666667,  0.8       ,  0.63333333,  0.86666667,  0.66666667]),
  'feature_idx': (0, 1),
  'feature_names': ('sepal length', 'sepal width')},
 5: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.96666667,  1.        ,  0.86666667,  0.93333333,  0.96666667]),
  'feature_idx': (0, 2),
  'feature_names': ('sepal length', 'petal length')},
 6: {'avg_score': 0.95333333333333337,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (0, 3),
  'feature_names': ('sepal length', 'petal width')},
 7: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.96666667,  1.        ,  0.9       ,  0.93333333,  0.93333333]),
  'feature_idx': (1, 2),
  'feature_names': ('sepal width', 'petal length')},
 8: {'avg_score': 0.94000000000000006,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.86666667,  0.93333333,  0.96666667]),
  'feature_idx': (1, 3),
  'feature_names': ('sepal width', 'petal width')},
 9: {'avg_score': 0.95333333333333337,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (2, 3),
  'feature_names': ('petal length', 'petal width')},
 10: {'avg_score': 0.94000000000000006,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.86666667,  0.93333333,  0.96666667]),
  'feature_idx': (0, 1, 2),
  'feature_names': ('sepal length', 'sepal width', 'petal length')},
 11: {'avg_score': 0.94666666666666666,
  'cv_scores': array([ 0.93333333,  0.96666667,  0.9       ,  0.93333333,  1.        ]),
  'feature_idx': (0, 1, 3),
  'feature_names': ('sepal length', 'sepal width', 'petal width')},
 12: {'avg_score': 0.97333333333333338,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.96666667,  0.96666667,  1.        ]),
  'feature_idx': (0, 2, 3),
  'feature_names': ('sepal length', 'petal length', 'petal width')},
 13: {'avg_score': 0.95999999999999996,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.93333333,  1.        ]),
  'feature_idx': (1, 2, 3),
  'feature_names': ('sepal width', 'petal length', 'petal width')},
 14: {'avg_score': 0.96666666666666679,
  'cv_scores': array([ 0.96666667,  0.96666667,  0.93333333,  0.96666667,  1.        ]),
  'feature_idx': (0, 1, 2, 3),
  'feature_names': ('sepal length',
   'sepal width',
   'petal length',
   'petal width')}}
</code></pre>
<h2 id="example-2-visualizing-the-feature-selection-results">Example 2 - Visualizing the feature selection results</h2>
<p>For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the <code>get_metric_dict</code> method of the <code>ExhaustiveFeatureSelector</code> object. The columns <code>std_dev</code> and <code>std_err</code> represent the standard deviation and standard errors of the cross-validation scores, respectively.</p>
<p>Below, we see the DataFrame of the Sequential Forward Selector from Example 2:</p>
<pre><code class="python">import pandas as pd

iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

feature_names = ('sepal length', 'sepal width',
                 'petal length', 'petal width')
efs1 = efs1.fit(X, y, custom_feature_names=feature_names)

df = pd.DataFrame.from_dict(efs1.get_metric_dict()).T
df.sort_values('avg_score', inplace=True, ascending=False)
df
</code></pre>

<pre><code>Features: 15/15
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>avg_score</th>
      <th>ci_bound</th>
      <th>cv_scores</th>
      <th>feature_idx</th>
      <th>feature_names</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>0.973333</td>
      <td>0.0171372</td>
      <td>[0.966666666667, 0.966666666667, 0.96666666666...</td>
      <td>(0, 2, 3)</td>
      <td>(sepal length, petal length, petal width)</td>
      <td>0.0133333</td>
      <td>0.00666667</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.966667</td>
      <td>0.0270963</td>
      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>
      <td>(0, 1, 2, 3)</td>
      <td>(sepal length, sepal width, petal length, peta...</td>
      <td>0.0210819</td>
      <td>0.0105409</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.96</td>
      <td>0.0320608</td>
      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>
      <td>(1, 2, 3)</td>
      <td>(sepal width, petal length, petal width)</td>
      <td>0.0249444</td>
      <td>0.0124722</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.953333</td>
      <td>0.0514116</td>
      <td>[0.933333333333, 1.0, 0.9, 0.933333333333, 1.0]</td>
      <td>(2,)</td>
      <td>(petal length,)</td>
      <td>0.04</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.953333</td>
      <td>0.0436915</td>
      <td>[0.966666666667, 0.966666666667, 0.9, 0.933333...</td>
      <td>(0, 3)</td>
      <td>(sepal length, petal width)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.953333</td>
      <td>0.0436915</td>
      <td>[0.966666666667, 0.966666666667, 0.9, 0.933333...</td>
      <td>(2, 3)</td>
      <td>(petal length, petal width)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.946667</td>
      <td>0.0581151</td>
      <td>[0.966666666667, 0.966666666667, 0.93333333333...</td>
      <td>(3,)</td>
      <td>(petal width,)</td>
      <td>0.0452155</td>
      <td>0.0226078</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.946667</td>
      <td>0.0581151</td>
      <td>[0.966666666667, 1.0, 0.866666666667, 0.933333...</td>
      <td>(0, 2)</td>
      <td>(sepal length, petal length)</td>
      <td>0.0452155</td>
      <td>0.0226078</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.946667</td>
      <td>0.0436915</td>
      <td>[0.966666666667, 1.0, 0.9, 0.933333333333, 0.9...</td>
      <td>(1, 2)</td>
      <td>(sepal width, petal length)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.946667</td>
      <td>0.0436915</td>
      <td>[0.933333333333, 0.966666666667, 0.9, 0.933333...</td>
      <td>(0, 1, 3)</td>
      <td>(sepal length, sepal width, petal width)</td>
      <td>0.0339935</td>
      <td>0.0169967</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.94</td>
      <td>0.0499631</td>
      <td>[0.966666666667, 0.966666666667, 0.86666666666...</td>
      <td>(1, 3)</td>
      <td>(sepal width, petal width)</td>
      <td>0.038873</td>
      <td>0.0194365</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.94</td>
      <td>0.0499631</td>
      <td>[0.966666666667, 0.966666666667, 0.86666666666...</td>
      <td>(0, 1, 2)</td>
      <td>(sepal length, sepal width, petal length)</td>
      <td>0.038873</td>
      <td>0.0194365</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.726667</td>
      <td>0.11623</td>
      <td>[0.666666666667, 0.8, 0.633333333333, 0.866666...</td>
      <td>(0, 1)</td>
      <td>(sepal length, sepal width)</td>
      <td>0.0904311</td>
      <td>0.0452155</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.66</td>
      <td>0.106334</td>
      <td>[0.533333333333, 0.633333333333, 0.73333333333...</td>
      <td>(0,)</td>
      <td>(sepal length,)</td>
      <td>0.0827312</td>
      <td>0.0413656</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.566667</td>
      <td>0.0605892</td>
      <td>[0.533333333333, 0.633333333333, 0.6, 0.5, 0.5...</td>
      <td>(1,)</td>
      <td>(sepal width,)</td>
      <td>0.0471405</td>
      <td>0.0235702</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">import matplotlib.pyplot as plt

metric_dict = efs1.get_metric_dict()

fig = plt.figure()
k_feat = sorted(metric_dict.keys())
avg = [metric_dict[k]['avg_score'] for k in k_feat]

upper, lower = [], []
for k in k_feat:
    upper.append(metric_dict[k]['avg_score'] +
                 metric_dict[k]['std_dev'])
    lower.append(metric_dict[k]['avg_score'] -
                 metric_dict[k]['std_dev'])

plt.fill_between(k_feat,
                 upper,
                 lower,
                 alpha=0.2,
                 color='blue',
                 lw=1)

plt.plot(k_feat, avg, color='blue', marker='o')
plt.ylabel('Accuracy +/- Standard Deviation')
plt.xlabel('Number of Features')
feature_min = len(metric_dict[k_feat[0]]['feature_idx'])
feature_max = len(metric_dict[k_feat[-1]]['feature_idx'])
plt.xticks(k_feat, 
           [str(metric_dict[k]['feature_names']) for k in k_feat], 
           rotation=90)
plt.show()
</code></pre>

<p><img alt="png" src="../ExhaustiveFeatureSelector_files/ExhaustiveFeatureSelector_19_0.png" /></p>
<h2 id="example-3-exhaustive-feature-selection-for-regression">Example 3 - Exhaustive Feature Selection for Regression</h2>
<p>Similar to the classification examples above, the <code>SequentialFeatureSelector</code> also supports scikit-learn's estimators
for regression.</p>
<pre><code class="python">from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston.data, boston.target

lr = LinearRegression()

efs = EFS(lr, 
          min_features=10,
          max_features=12,
          scoring='neg_mean_squared_error',
          cv=10)

efs.fit(X, y)

print('Best MSE score: %.2f' % efs.best_score_ * (-1))
print('Best subset:', efs.best_idx_)
</code></pre>

<pre><code>Features: 377/377


Best subset: (0, 1, 4, 6, 7, 8, 9, 10, 11, 12)
</code></pre>
<h2 id="example-4-using-the-selected-feature-subset-for-making-new-predictions">Example 4 - Using the Selected Feature Subset For Making New Predictions</h2>
<pre><code class="python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)

knn = KNeighborsClassifier(n_neighbors=3)
</code></pre>

<pre><code class="python"># Select the &quot;best&quot; three features via
# 5-fold cross-validation on the training set.

from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           cv=5)
efs1 = efs1.fit(X_train, y_train)
</code></pre>

<pre><code>Features: 15/15
</code></pre>
<pre><code class="python">print('Selected features:', efs1.best_idx_)
</code></pre>

<pre><code>Selected features: (2, 3)
</code></pre>
<pre><code class="python"># Generate the new subsets based on the selected features
# Note that the transform call is equivalent to
# X_train[:, efs1.k_feature_idx_]

X_train_efs = efs1.transform(X_train)
X_test_efs = efs1.transform(X_test)

# Fit the estimator using the new feature subset
# and make a prediction on the test data
knn.fit(X_train_efs, y_train)
y_pred = knn.predict(X_test_efs)

# Compute the accuracy of the prediction
acc = float((y_test == y_pred).sum()) / y_pred.shape[0]
print('Test set accuracy: %.2f %%' % (acc*100))
</code></pre>

<pre><code>Test set accuracy: 96.00 %
</code></pre>
<h2 id="example-5-exhaustive-feature-selection-and-gridsearch">Example 5 - Exhaustive Feature Selection and GridSearch</h2>
<pre><code class="python"># Initialize the dataset

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)
</code></pre>

<p>Use scikit-learn's <code>GridSearch</code> to tune the hyperparameters of the <code>LogisticRegression</code> estimator inside the <code>ExhaustiveFeatureSelector</code> and use it for prediction in the pipeline. <strong>Note that the <code>clone_estimator</code> attribute needs to be set to <code>False</code>.</strong></p>
<pre><code class="python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

lr = LogisticRegression(multi_class='multinomial', 
                        solver='lbfgs', 
                        random_state=123)

efs1 = EFS(estimator=lr, 
           min_features=2,
           max_features=3,
           scoring='accuracy',
           print_progress=False,
           clone_estimator=False,
           cv=5,
           n_jobs=1)

pipe = make_pipeline(efs1, lr)

param_grid = {'exhaustivefeatureselector__estimator__C': [0.1, 1.0, 10.0]}

gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=2, 
                  verbose=1, 
                  refit=False)

# run gridearch
gs = gs.fit(X_train, y_train)
</code></pre>

<pre><code>Fitting 2 folds for each of 3 candidates, totalling 6 fits


[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.7s finished
</code></pre>
<p>... and the "best" parameters determined by GridSearch are ...</p>
<pre><code class="python">print(&quot;Best parameters via GridSearch&quot;, gs.best_params_)
</code></pre>

<pre><code>Best parameters via GridSearch {'exhaustivefeatureselector__estimator__C': 1.0}
</code></pre>
<h4 id="obtaining-the-best-k-feature-indices-after-gridsearch">Obtaining the best <em>k</em> feature indices after GridSearch</h4>
<p>If we are interested in the best <em>k</em> best feature indices via <code>SequentialFeatureSelection.best_idx_</code>, we have to initialize a <code>GridSearchCV</code> object with <code>refit=True</code>. Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline.</p>
<pre><code class="python">gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=2, 
                  verbose=1, 
                  refit=True)
</code></pre>

<p>After running the grid search, we can access the individual pipeline objects of the <code>best_estimator_</code> via the <code>steps</code> attribute.</p>
<pre><code class="python">gs = gs.fit(X_train, y_train)
gs.best_estimator_.steps
</code></pre>

<pre><code>Fitting 2 folds for each of 3 candidates, totalling 6 fits


[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.9s finished





[('exhaustivefeatureselector',
  ExhaustiveFeatureSelector(clone_estimator=False, cv=5,
               estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
            intercept_scaling=1, max_iter=100, multi_class='multinomial',
            n_jobs=1, penalty='l2', random_state=123, solver='lbfgs',
            tol=0.0001, verbose=0, warm_start=False),
               max_features=3, min_features=2, n_jobs=1,
               pre_dispatch='2*n_jobs', print_progress=False,
               scoring='accuracy')),
 ('logisticregression',
  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
            intercept_scaling=1, max_iter=100, multi_class='multinomial',
            n_jobs=1, penalty='l2', random_state=123, solver='lbfgs',
            tol=0.0001, verbose=0, warm_start=False))]
</code></pre>
<p>Via sub-indexing, we can then obtain the best-selected feature subset:</p>
<pre><code class="python">print('Best features:', gs.best_estimator_.steps[0][1].best_idx_)
</code></pre>

<pre><code>Best features: (2, 3)
</code></pre>
<p>During cross-validation, this feature combination had a CV accuracy of:</p>
<pre><code class="python">print('Best score:', gs.best_score_)
</code></pre>

<pre><code>Best score: 0.97
</code></pre>
<pre><code class="python">gs.best_params_
</code></pre>

<pre><code>{'exhaustivefeatureselector__estimator__C': 1.0}
</code></pre>
<p><strong>Alternatively</strong>, if we can set the "best grid search parameters" in our pipeline manually if we ran <code>GridSearchCV</code> with <code>refit=False</code>. It should yield the same results:</p>
<pre><code class="python">pipe.set_params(**gs.best_params_).fit(X_train, y_train)
print('Best features:', pipe.steps[0][1].best_idx_)
</code></pre>

<pre><code>Best features: (2, 3)
</code></pre>
<h2 id="example-6-working-with-pandas-dataframes">Example 6 - Working with pandas DataFrames</h2>
<p>Optionally, we can also use pandas DataFrames and pandas Series as input to the <code>fit</code> function. In this case, the column names of the pandas DataFrame will be used as feature names. However, note that if <code>custom_feature_names</code> are provided in the fit function, these <code>custom_feature_names</code> take precedence over the DataFrame column-based feature names.</p>
<pre><code class="python">import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

iris = load_iris()
col_names = ('sepal length', 'sepal width',
             'petal length', 'petal width')
X_df = pd.DataFrame(iris.data, columns=col_names)
y_series = pd.Series(iris.target)
knn = KNeighborsClassifier(n_neighbors=4)
</code></pre>

<pre><code class="python">from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

efs1 = efs1.fit(X_df, y_series)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>

<pre><code>Features: 15/15

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('sepal length', 'petal length', 'petal width')
</code></pre>
<h2 id="api">API</h2>
<p><em>ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True)*</p>
<p>Exhaustive Feature Selection for Classification and Regression.
(new in v0.4.3)</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>min_features</code> : int (default: 1)</p>
<p>Minumum number of features to select</p>
</li>
<li>
<p><code>max_features</code> : int (default: 1)</p>
<p>Maximum number of features to select</p>
</li>
<li>
<p><code>print_progress</code> : bool (default: True)</p>
<p>Prints progress as the number of epochs
to stderr.</p>
</li>
<li>
<p><code>scoring</code> : str, (default='accuracy')</p>
<p>Scoring metric in {accuracy, f1, precision, recall, roc_auc}
for classifiers,
{'mean_absolute_error', 'mean_squared_error',
'median_absolute_error', 'r2'} for regressors,
or a callable object or function with
signature <code>scorer(estimator, X, y)</code>.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Scikit-learn cross-validation generator or <code>int</code>.
If estimator is a classifier (or y consists of integer class labels),
stratified k-fold is performed, and regular k-fold cross-validation
otherwise.
No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>best_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>best_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>best_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
exhaustive selection, where the dictionary keys are
the lengths k of these feature subsets. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v 0.13.0.</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    <a href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/">http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/</a></p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, custom_feature_names=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>custom_feature_names</code> : None or tuple (default: tuple)</p>
<p>Custom feature names for <code>self.k_feature_names</code> and
<code>self.subsets_[i]['feature_names']</code>.
(new in v 0.13.0)</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, </em><em>fit_params)</em></p>
<p>Fit to training data and return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : boolean, optional</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : mapping of string to any</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2014-2018 <a href="http://sebastianraschka.com">Sebastian Raschka</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../mathjaxhelper.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
