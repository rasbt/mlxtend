<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Sebastian Raschka">
        <link rel="canonical" href="https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations - mlxtend</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/color-brewer.min.css">
        <link href="../../../cinder/css/base.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../cinder/css/cinder.css" rel="stylesheet">
        <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../../../cinder/css/highlight.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">mlxtend</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../../.." class="nav-link">Home</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../USER_GUIDE_INDEX/" class="dropdown-item">User Guide Index</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">classifier</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../classifier/Adaline/" class="dropdown-item">Adaline: Adaptive Linear Neuron Classifier</a>
</li>
            
<li>
    <a href="../../classifier/EnsembleVoteClassifier/" class="dropdown-item">EnsembleVoteClassifier: A majority voting classifier</a>
</li>
            
<li>
    <a href="../../classifier/LogisticRegression/" class="dropdown-item">LogisticRegression: A binary classifier</a>
</li>
            
<li>
    <a href="../../classifier/MultiLayerPerceptron/" class="dropdown-item">MultilayerPerceptron: A simple multilayer neural network</a>
</li>
            
<li>
    <a href="../../classifier/OneRClassifier/" class="dropdown-item">OneRClassifier: One Rule (OneR) method for classfication</a>
</li>
            
<li>
    <a href="../../classifier/Perceptron/" class="dropdown-item">Perceptron: A simple binary classifier</a>
</li>
            
<li>
    <a href="../../classifier/SoftmaxRegression/" class="dropdown-item">SoftmaxRegression: Multiclass version of logistic regression</a>
</li>
            
<li>
    <a href="../../classifier/StackingClassifier/" class="dropdown-item">StackingClassifier: Simple stacking</a>
</li>
            
<li>
    <a href="../../classifier/StackingCVClassifier/" class="dropdown-item">StackingCVClassifier: Stacking with cross-validation</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">cluster</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../cluster/Kmeans/" class="dropdown-item">Kmeans: k-means clustering</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">data</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../data/autompg_data/" class="dropdown-item">autompg_data: The Auto-MPG dataset for regression</a>
</li>
            
<li>
    <a href="../../data/boston_housing_data/" class="dropdown-item">boston_housing_data: The Boston housing dataset for regression</a>
</li>
            
<li>
    <a href="../../data/iris_data/" class="dropdown-item">iris_data: The 3-class iris dataset for classification</a>
</li>
            
<li>
    <a href="../../data/loadlocal_mnist/" class="dropdown-item">loadlocal_mnist: A function for loading MNIST from the original ubyte files</a>
</li>
            
<li>
    <a href="../../data/make_multiplexer_dataset/" class="dropdown-item">make_multiplexer_dataset: A function for creating multiplexer data</a>
</li>
            
<li>
    <a href="../../data/mnist_data/" class="dropdown-item">mnist_data: A subset of the MNIST dataset for classification</a>
</li>
            
<li>
    <a href="../../data/three_blobs_data/" class="dropdown-item">three_blobs_data: The synthetic blobs for classification</a>
</li>
            
<li>
    <a href="../../data/wine_data/" class="dropdown-item">wine_data: A 3-class wine dataset for classification</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">evaluate</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../evaluate/accuracy_score/" class="dropdown-item">accuracy_score: Computing standard, balanced, and per-class accuracy</a>
</li>
            
<li>
    <a href="../../evaluate/bias_variance_decomp/" class="dropdown-item">bias_variance_decomp: Bias-variance decomposition for classification and regression losses</a>
</li>
            
<li>
    <a href="../../evaluate/bootstrap/" class="dropdown-item">bootstrap: The ordinary nonparametric boostrap for arbitrary parameters</a>
</li>
            
<li>
    <a href="../../evaluate/bootstrap_point632_score/" class="dropdown-item">bootstrap_point632_score: The .632 and .632+ boostrap for classifier evaluation</a>
</li>
            
<li>
    <a href="../../evaluate/BootstrapOutOfBag/" class="dropdown-item">BootstrapOutOfBag: A scikit-learn compatible version of the out-of-bag bootstrap</a>
</li>
            
<li>
    <a href="../../evaluate/cochrans_q/" class="dropdown-item">cochrans_q: Cochran's Q test for comparing multiple classifiers</a>
</li>
            
<li>
    <a href="../../evaluate/combined_ftest_5x2cv/" class="dropdown-item">combined_ftest_5x2cv: 5x2cv combined *F* test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/confusion_matrix/" class="dropdown-item">confusion_matrix: creating a confusion matrix for model evaluation</a>
</li>
            
<li>
    <a href="../../evaluate/create_counterfactual/" class="dropdown-item">create_counterfactual: Interpreting models via counterfactuals</a>
</li>
            
<li>
    <a href="../../evaluate/feature_importance_permutation/" class="dropdown-item">feature_importance_permutation: Estimate feature importance via feature permutation.</a>
</li>
            
<li>
    <a href="../../evaluate/ftest/" class="dropdown-item">ftest: F-test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/GroupTimeSeriesSplit/" class="dropdown-item">GroupTimeSeriesSplit: A scikit-learn compatible version of the time series validation with groups</a>
</li>
            
<li>
    <a href="../../evaluate/lift_score/" class="dropdown-item">lift_score: Lift score for classification and association rule mining</a>
</li>
            
<li>
    <a href="../../evaluate/mcnemar_table/" class="dropdown-item">mcnemar_table: Contingency table for McNemar's test</a>
</li>
            
<li>
    <a href="../../evaluate/mcnemar_tables/" class="dropdown-item">mcnemar_tables: contingency tables for McNemar's test and Cochran's Q test</a>
</li>
            
<li>
    <a href="../../evaluate/mcnemar/" class="dropdown-item">mcnemar: McNemar's test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/paired_ttest_5x2cv/" class="dropdown-item">paired_ttest_5x2cv: 5x2cv paired *t* test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/paired_ttest_kfold_cv/" class="dropdown-item">paired_ttest_kfold_cv: K-fold cross-validated paired *t* test</a>
</li>
            
<li>
    <a href="../../evaluate/paired_ttest_resampled/" class="dropdown-item">paired_ttest_resample: Resampled paired *t* test</a>
</li>
            
<li>
    <a href="../../evaluate/permutation_test/" class="dropdown-item">permutation_test: Permutation test for hypothesis testing</a>
</li>
            
<li>
    <a href="../../evaluate/PredefinedHoldoutSplit/" class="dropdown-item">PredefinedHoldoutSplit: Utility for the holdout method compatible with scikit-learn</a>
</li>
            
<li>
    <a href="../../evaluate/RandomHoldoutSplit/" class="dropdown-item">RandomHoldoutSplit: split a dataset into a train and validation subset for validation</a>
</li>
            
<li>
    <a href="../../evaluate/scoring/" class="dropdown-item">scoring: computing various performance metrics</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">feature_extraction</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/" class="dropdown-item">LinearDiscriminantAnalysis: Linear discriminant analysis for dimensionality reduction</a>
</li>
            
<li>
    <a href="../../feature_extraction/PrincipalComponentAnalysis/" class="dropdown-item">PrincipalComponentAnalysis: Principal component analysis (PCA) for dimensionality reduction</a>
</li>
            
<li>
    <a href="../../feature_extraction/RBFKernelPCA/" class="dropdown-item">RBFKernelPCA</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">feature_selection</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../ColumnSelector/" class="dropdown-item">ColumnSelector: Scikit-learn utility function to select specific columns in a pipeline</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</a>
</li>
            
<li>
    <a href="../SequentialFeatureSelector/" class="dropdown-item">SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants)</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">file_io</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../file_io/find_filegroups/" class="dropdown-item">find_filegroups: Find files that only differ via their file extensions</a>
</li>
            
<li>
    <a href="../../file_io/find_files/" class="dropdown-item">find_files: Find files based on substring matches</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">frequent_patterns</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../frequent_patterns/apriori/" class="dropdown-item">Apriori</a>
</li>
            
<li>
    <a href="../../frequent_patterns/association_rules/" class="dropdown-item">Association rules</a>
</li>
            
<li>
    <a href="../../frequent_patterns/fpgrowth/" class="dropdown-item">Fpgrowth</a>
</li>
            
<li>
    <a href="../../frequent_patterns/fpmax/" class="dropdown-item">Fpmax</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">image</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../image/extract_face_landmarks/" class="dropdown-item">extract_face_landmarks: extract 68 landmark features from face images</a>
</li>
            
<li>
    <a href="../../image/eyepad_align/" class="dropdown-item">EyepadAlign:  align face images based on eye location</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">math</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../math/num_combinations/" class="dropdown-item">num_combinations: combinations for creating subsequences of *k* elements</a>
</li>
            
<li>
    <a href="../../math/num_permutations/" class="dropdown-item">num_permutations: number of permutations for creating subsequences of *k* elements</a>
</li>
            
<li>
    <a href="../../math/vectorspace_dimensionality/" class="dropdown-item">vectorspace_dimensionality: compute the number of dimensions that a set of vectors spans</a>
</li>
            
<li>
    <a href="../../math/vectorspace_orthonormalization/" class="dropdown-item">vectorspace_orthonormalization: Converts a set of linearly independent vectors to a set of orthonormal basis vectors</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">plotting</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../plotting/category_scatter/" class="dropdown-item">Scategory_scatter: Create a scatterplot with categories in different colors</a>
</li>
            
<li>
    <a href="../../plotting/checkerboard_plot/" class="dropdown-item">checkerboard_plot: Create a checkerboard plot in matplotlib</a>
</li>
            
<li>
    <a href="../../plotting/plot_pca_correlation_graph/" class="dropdown-item">plot_pca_correlation_graph: plot correlations between original features and principal components</a>
</li>
            
<li>
    <a href="../../plotting/ecdf/" class="dropdown-item">ecdf: Create an empirical cumulative distribution function plot</a>
</li>
            
<li>
    <a href="../../plotting/enrichment_plot/" class="dropdown-item">enrichment_plot: create an enrichment plot for cumulative counts</a>
</li>
            
<li>
    <a href="../../plotting/heatmap/" class="dropdown-item">heatmap: Create a heatmap in matplotlib</a>
</li>
            
<li>
    <a href="../../plotting/plot_confusion_matrix/" class="dropdown-item">plot_confusion_matrix: Visualize confusion matrices</a>
</li>
            
<li>
    <a href="../../plotting/plot_decision_regions/" class="dropdown-item">plot_decision_regions: Visualize the decision regions of a classifier</a>
</li>
            
<li>
    <a href="../../plotting/plot_learning_curves/" class="dropdown-item">plot_learning_curves: Plot learning curves from training and test sets</a>
</li>
            
<li>
    <a href="../../plotting/plot_linear_regression/" class="dropdown-item">plot_linear_regression: A quick way for plotting linear regression fits</a>
</li>
            
<li>
    <a href="../../plotting/plot_sequential_feature_selection/" class="dropdown-item">plot_sequential_feature_selection: Visualize selected feature subset performances from the SequentialFeatureSelector</a>
</li>
            
<li>
    <a href="../../plotting/scatterplotmatrix/" class="dropdown-item">scatterplotmatrix: visualize datasets via a scatter plot matrix</a>
</li>
            
<li>
    <a href="../../plotting/scatter_hist/" class="dropdown-item">scatter_hist: create a scatter histogram plot</a>
</li>
            
<li>
    <a href="../../plotting/stacked_barplot/" class="dropdown-item">stacked_barplot: Plot stacked bar plots in matplotlib</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">preprocessing</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../preprocessing/CopyTransformer/" class="dropdown-item">CopyTransformer: A function that creates a copy of the input array in a scikit-learn pipeline</a>
</li>
            
<li>
    <a href="../../preprocessing/DenseTransformer/" class="dropdown-item">DenseTransformer: Transforms a sparse into a dense NumPy array, e.g., in a scikit-learn pipeline</a>
</li>
            
<li>
    <a href="../../preprocessing/MeanCenterer/" class="dropdown-item">MeanCenterer: column-based mean centering on a NumPy array</a>
</li>
            
<li>
    <a href="../../preprocessing/minmax_scaling/" class="dropdown-item">MinMaxScaling: Min-max scaling fpr pandas DataFrames and NumPy arrays</a>
</li>
            
<li>
    <a href="../../preprocessing/one-hot_encoding/" class="dropdown-item">One hot encoding</a>
</li>
            
<li>
    <a href="../../preprocessing/shuffle_arrays_unison/" class="dropdown-item">shuffle_arrays_unison: shuffle arrays in a consistent fashion</a>
</li>
            
<li>
    <a href="../../preprocessing/standardize/" class="dropdown-item">standardize: A function to standardize columns in a 2D NumPy array</a>
</li>
            
<li>
    <a href="../../preprocessing/TransactionEncoder/" class="dropdown-item">TransactionEncoder</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">regressor</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../regressor/LinearRegression/" class="dropdown-item">LinearRegression: An implementation of ordinary least-squares linear regression</a>
</li>
            
<li>
    <a href="../../regressor/StackingCVRegressor/" class="dropdown-item">StackingCVRegressor: stacking with cross-validation for regression</a>
</li>
            
<li>
    <a href="../../regressor/StackingRegressor/" class="dropdown-item">StackingRegressor: a simple stacking implementation for regression</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">text</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../text/generalize_names/" class="dropdown-item">generalize_names: convert names into a generalized format</a>
</li>
            
<li>
    <a href="../../text/generalize_names_duplcheck/" class="dropdown-item">generalize_names_duplcheck: Generalize names while preventing duplicates among different names</a>
</li>
            
<li>
    <a href="../../text/tokenizer/" class="dropdown-item">tokenizer_emoticons: tokenizers for emoticons</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">utils</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../utils/Counter/" class="dropdown-item">Counter: A simple progress counter</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.classifier/" class="dropdown-item">Mlxtend.classifier</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.cluster/" class="dropdown-item">Mlxtend.cluster</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.data/" class="dropdown-item">Mlxtend.data</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.evaluate/" class="dropdown-item">Mlxtend.evaluate</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.feature_extraction/" class="dropdown-item">Mlxtend.feature extraction</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.feature_selection/" class="dropdown-item">Mlxtend.feature selection</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.file_io/" class="dropdown-item">Mlxtend.file io</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/" class="dropdown-item">Mlxtend.frequent patterns</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.image/" class="dropdown-item">Mlxtend.image</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.plotting/" class="dropdown-item">Mlxtend.plotting</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.preprocessing/" class="dropdown-item">Mlxtend.preprocessing</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.regressor/" class="dropdown-item">Mlxtend.regressor</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.text/" class="dropdown-item">Mlxtend.text</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.utils/" class="dropdown-item">Mlxtend.utils</a>
</li>
                                </ul>
                            </li>
                            <li class="navitem">
                                <a href="../../../installation/" class="nav-link">Installation</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../CHANGELOG/" class="dropdown-item">Release Notes</a>
</li>
                                    
<li>
    <a href="../../../Code-of-Conduct/" class="dropdown-item">Code of Conduct</a>
</li>
                                    
<li>
    <a href="../../../CONTRIBUTING/" class="dropdown-item">How To Contribute</a>
</li>
                                    
<li>
    <a href="../../../contributors/" class="dropdown-item">Contributors</a>
</li>
                                    
<li>
    <a href="../../../license/" class="dropdown-item">License</a>
</li>
                                    
<li>
    <a href="../../../cite/" class="dropdown-item">Citing Mlxtend</a>
</li>
                                    
<li>
    <a href="../../../discuss/" class="dropdown-item">Discuss</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../ColumnSelector/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../SequentialFeatureSelector/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/rasbt/mlxtend/tree/master/docs/sources/user_guide/feature_selection/ExhaustiveFeatureSelector.md" class="nav-link">Edit on mlxtend
                                    </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#exhaustivefeatureselector-optimal-feature-sets-by-considering-all-possible-feature-combinations" class="nav-link">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#overview" class="nav-link">Overview</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-1-a-simple-iris-example" class="nav-link">Example 1 - A simple Iris example</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-2-visualizing-the-feature-selection-results" class="nav-link">Example 2 - Visualizing the feature selection results</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-3-exhaustive-feature-selection-for-regression-analysis" class="nav-link">Example 3 - Exhaustive feature selection for regression analysis</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-4-regression-and-adjusted-r2" class="nav-link">Example 4 - Regression and adjusted R2</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-5-using-the-selected-feature-subset-for-making-new-predictions" class="nav-link">Example 5 - Using the selected feature subset For making new predictions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-6-exhaustive-feature-selection-and-gridsearch" class="nav-link">Example 6 - Exhaustive feature selection and GridSearch</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-7-exhaustive-feature-selection-with-loocv" class="nav-link">Example 7 - Exhaustive Feature Selection with LOOCV</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-8-interrupting-long-runs-for-intermediate-results" class="nav-link">Example 8 - Interrupting Long Runs for Intermediate Results</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-9-working-with-feature-groups" class="nav-link">Example 9 - Working with Feature Groups</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#api" class="nav-link">API</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="exhaustivefeatureselector-optimal-feature-sets-by-considering-all-possible-feature-combinations">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</h1>
<p>Implementation of an <em>exhaustive feature selector</em> for sampling and evaluating all possible feature combinations in a specified range.</p>
<blockquote>
<p>from mlxtend.feature_selection import ExhaustiveFeatureSelector</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>This exhaustive feature selection algorithm is a wrapper approach for brute-force evaluation of feature subsets; the best subset is selected by optimizing a specified performance metric given an arbitrary regressor or classifier. For instance, if the classifier is a logistic regression and the dataset consists of 4 features, the alogorithm will evaluate all 15 feature combinations (if <code>min_features=1</code> and <code>max_features=4</code>)</p>
<ul>
<li>{0}</li>
<li>{1}</li>
<li>{2}</li>
<li>{3}</li>
<li>{0, 1}</li>
<li>{0, 2}</li>
<li>{0, 3}</li>
<li>{1, 2}</li>
<li>{1, 3}</li>
<li>{2, 3}</li>
<li>{0, 1, 2}</li>
<li>{0, 1, 3}</li>
<li>{0, 2, 3}</li>
<li>{1, 2, 3}</li>
<li>{0, 1, 2, 3}</li>
</ul>
<p>and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.</p>
<h2 id="example-1-a-simple-iris-example">Example 1 - A simple Iris example</h2>
<p>Initializing a simple classifier from scikit-learn:</p>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

efs1 = efs1.fit(X, y)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>
<pre><code>Features: 15/15

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('0', '2', '3')
</code></pre>
<h3 id="feature-names">Feature Names</h3>
<p>When working with large datasets, the feature indices might be hard to interpret. In this case, we recommend using pandas DataFrames with distinct column names as input:</p>
<pre><code class="language-python">import pandas as pd

df_X = pd.DataFrame(X, columns=[&quot;Sepal length&quot;, &quot;Sepal width&quot;, &quot;Petal length&quot;, &quot;Petal width&quot;])
df_X.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal length</th>
      <th>Sepal width</th>
      <th>Petal length</th>
      <th>Petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">efs1 = efs1.fit(df_X, y)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>
<pre><code>Features: 15/15

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('Sepal length', 'Petal length', 'Petal width')
</code></pre>
<h3 id="detailed-outputs">Detailed Outputs</h3>
<p>Via the <code>subsets_</code> attribute, we can take a look at the selected feature indices at each step:</p>
<pre><code class="language-python">efs1.subsets_
</code></pre>
<pre><code>{0: {'feature_idx': (0,),
  'cv_scores': array([0.53333333, 0.63333333, 0.7       , 0.8       , 0.56666667]),
  'avg_score': 0.6466666666666667,
  'feature_names': ('Sepal length',)},
 1: {'feature_idx': (1,),
  'cv_scores': array([0.43333333, 0.63333333, 0.53333333, 0.43333333, 0.5       ]),
  'avg_score': 0.5066666666666666,
  'feature_names': ('Sepal width',)},
 2: {'feature_idx': (2,),
  'cv_scores': array([0.93333333, 0.93333333, 0.9       , 0.93333333, 1.        ]),
  'avg_score': 0.9400000000000001,
  'feature_names': ('Petal length',)},
 3: {'feature_idx': (3,),
  'cv_scores': array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ]),
  'avg_score': 0.96,
  'feature_names': ('Petal width',)},
 4: {'feature_idx': (0, 1),
  'cv_scores': array([0.66666667, 0.8       , 0.7       , 0.86666667, 0.66666667]),
  'avg_score': 0.74,
  'feature_names': ('Sepal length', 'Sepal width')},
 5: {'feature_idx': (0, 2),
  'cv_scores': array([0.96666667, 1.        , 0.86666667, 0.93333333, 0.96666667]),
  'avg_score': 0.9466666666666667,
  'feature_names': ('Sepal length', 'Petal length')},
 6: {'feature_idx': (0, 3),
  'cv_scores': array([0.96666667, 0.96666667, 0.9       , 0.93333333, 1.        ]),
  'avg_score': 0.9533333333333334,
  'feature_names': ('Sepal length', 'Petal width')},
 7: {'feature_idx': (1, 2),
  'cv_scores': array([0.93333333, 0.93333333, 0.9       , 0.93333333, 0.93333333]),
  'avg_score': 0.9266666666666667,
  'feature_names': ('Sepal width', 'Petal length')},
 8: {'feature_idx': (1, 3),
  'cv_scores': array([0.96666667, 0.96666667, 0.86666667, 0.93333333, 0.96666667]),
  'avg_score': 0.9400000000000001,
  'feature_names': ('Sepal width', 'Petal width')},
 9: {'feature_idx': (2, 3),
  'cv_scores': array([0.96666667, 0.96666667, 0.9       , 0.93333333, 1.        ]),
  'avg_score': 0.9533333333333334,
  'feature_names': ('Petal length', 'Petal width')},
 10: {'feature_idx': (0, 1, 2),
  'cv_scores': array([0.96666667, 0.96666667, 0.86666667, 0.93333333, 0.96666667]),
  'avg_score': 0.9400000000000001,
  'feature_names': ('Sepal length', 'Sepal width', 'Petal length')},
 11: {'feature_idx': (0, 1, 3),
  'cv_scores': array([0.93333333, 0.96666667, 0.9       , 0.93333333, 1.        ]),
  'avg_score': 0.9466666666666667,
  'feature_names': ('Sepal length', 'Sepal width', 'Petal width')},
 12: {'feature_idx': (0, 2, 3),
  'cv_scores': array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 1.        ]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('Sepal length', 'Petal length', 'Petal width')},
 13: {'feature_idx': (1, 2, 3),
  'cv_scores': array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ]),
  'avg_score': 0.96,
  'feature_names': ('Sepal width', 'Petal length', 'Petal width')},
 14: {'feature_idx': (0, 1, 2, 3),
  'cv_scores': array([0.96666667, 0.96666667, 0.93333333, 0.96666667, 1.        ]),
  'avg_score': 0.9666666666666668,
  'feature_names': ('Sepal length',
   'Sepal width',
   'Petal length',
   'Petal width')}}
</code></pre>
<h2 id="example-2-visualizing-the-feature-selection-results">Example 2 - Visualizing the feature selection results</h2>
<p>For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the <code>get_metric_dict</code> method of the <code>ExhaustiveFeatureSelector</code> object. The columns <code>std_dev</code> and <code>std_err</code> represent the standard deviation and standard errors of the cross-validation scores, respectively.</p>
<p>Below, we see the DataFrame of the Sequential Forward Selector from Example 2:</p>
<pre><code class="language-python">import pandas as pd

iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=5)

feature_names = ('sepal length', 'sepal width',
                 'petal length', 'petal width')

df_X = pd.DataFrame(
    X, columns=[&quot;Sepal length&quot;, &quot;Sepal width&quot;, &quot;Petal length&quot;, &quot;Petal width&quot;])
efs1 = efs1.fit(df_X, y)

df = pd.DataFrame.from_dict(efs1.get_metric_dict()).T
df.sort_values('avg_score', inplace=True, ascending=False)
df
</code></pre>
<pre><code>Features: 15/15
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_idx</th>
      <th>cv_scores</th>
      <th>avg_score</th>
      <th>feature_names</th>
      <th>ci_bound</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>(0, 2, 3)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.966...</td>
      <td>0.973333</td>
      <td>(Sepal length, Petal length, Petal width)</td>
      <td>0.017137</td>
      <td>0.013333</td>
      <td>0.006667</td>
    </tr>
    <tr>
      <th>14</th>
      <td>(0, 1, 2, 3)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.933...</td>
      <td>0.966667</td>
      <td>(Sepal length, Sepal width, Petal length, Peta...</td>
      <td>0.027096</td>
      <td>0.021082</td>
      <td>0.010541</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(3,)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.933...</td>
      <td>0.96</td>
      <td>(Petal width,)</td>
      <td>0.032061</td>
      <td>0.024944</td>
      <td>0.012472</td>
    </tr>
    <tr>
      <th>13</th>
      <td>(1, 2, 3)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.933...</td>
      <td>0.96</td>
      <td>(Sepal width, Petal length, Petal width)</td>
      <td>0.032061</td>
      <td>0.024944</td>
      <td>0.012472</td>
    </tr>
    <tr>
      <th>6</th>
      <td>(0, 3)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.9, ...</td>
      <td>0.953333</td>
      <td>(Sepal length, Petal width)</td>
      <td>0.043691</td>
      <td>0.033993</td>
      <td>0.016997</td>
    </tr>
    <tr>
      <th>9</th>
      <td>(2, 3)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.9, ...</td>
      <td>0.953333</td>
      <td>(Petal length, Petal width)</td>
      <td>0.043691</td>
      <td>0.033993</td>
      <td>0.016997</td>
    </tr>
    <tr>
      <th>5</th>
      <td>(0, 2)</td>
      <td>[0.9666666666666667, 1.0, 0.8666666666666667, ...</td>
      <td>0.946667</td>
      <td>(Sepal length, Petal length)</td>
      <td>0.058115</td>
      <td>0.045216</td>
      <td>0.022608</td>
    </tr>
    <tr>
      <th>11</th>
      <td>(0, 1, 3)</td>
      <td>[0.9333333333333333, 0.9666666666666667, 0.9, ...</td>
      <td>0.946667</td>
      <td>(Sepal length, Sepal width, Petal width)</td>
      <td>0.043691</td>
      <td>0.033993</td>
      <td>0.016997</td>
    </tr>
    <tr>
      <th>2</th>
      <td>(2,)</td>
      <td>[0.9333333333333333, 0.9333333333333333, 0.9, ...</td>
      <td>0.94</td>
      <td>(Petal length,)</td>
      <td>0.041977</td>
      <td>0.03266</td>
      <td>0.01633</td>
    </tr>
    <tr>
      <th>8</th>
      <td>(1, 3)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.866...</td>
      <td>0.94</td>
      <td>(Sepal width, Petal width)</td>
      <td>0.049963</td>
      <td>0.038873</td>
      <td>0.019437</td>
    </tr>
    <tr>
      <th>10</th>
      <td>(0, 1, 2)</td>
      <td>[0.9666666666666667, 0.9666666666666667, 0.866...</td>
      <td>0.94</td>
      <td>(Sepal length, Sepal width, Petal length)</td>
      <td>0.049963</td>
      <td>0.038873</td>
      <td>0.019437</td>
    </tr>
    <tr>
      <th>7</th>
      <td>(1, 2)</td>
      <td>[0.9333333333333333, 0.9333333333333333, 0.9, ...</td>
      <td>0.926667</td>
      <td>(Sepal width, Petal length)</td>
      <td>0.017137</td>
      <td>0.013333</td>
      <td>0.006667</td>
    </tr>
    <tr>
      <th>4</th>
      <td>(0, 1)</td>
      <td>[0.6666666666666666, 0.8, 0.7, 0.8666666666666...</td>
      <td>0.74</td>
      <td>(Sepal length, Sepal width)</td>
      <td>0.102823</td>
      <td>0.08</td>
      <td>0.04</td>
    </tr>
    <tr>
      <th>0</th>
      <td>(0,)</td>
      <td>[0.5333333333333333, 0.6333333333333333, 0.7, ...</td>
      <td>0.646667</td>
      <td>(Sepal length,)</td>
      <td>0.122983</td>
      <td>0.095685</td>
      <td>0.047842</td>
    </tr>
    <tr>
      <th>1</th>
      <td>(1,)</td>
      <td>[0.43333333333333335, 0.6333333333333333, 0.53...</td>
      <td>0.506667</td>
      <td>(Sepal width,)</td>
      <td>0.095416</td>
      <td>0.074237</td>
      <td>0.037118</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">import matplotlib.pyplot as plt

metric_dict = efs1.get_metric_dict()

fig = plt.figure()
k_feat = sorted(metric_dict.keys())
avg = [metric_dict[k]['avg_score'] for k in k_feat]

upper, lower = [], []
for k in k_feat:
    upper.append(metric_dict[k]['avg_score'] +
                 metric_dict[k]['std_dev'])
    lower.append(metric_dict[k]['avg_score'] -
                 metric_dict[k]['std_dev'])

plt.fill_between(k_feat,
                 upper,
                 lower,
                 alpha=0.2,
                 color='blue',
                 lw=1)

plt.plot(k_feat, avg, color='blue', marker='o')
plt.ylabel('Accuracy +/- Standard Deviation')
plt.xlabel('Number of Features')
feature_min = len(metric_dict[k_feat[0]]['feature_idx'])
feature_max = len(metric_dict[k_feat[-1]]['feature_idx'])
plt.xticks(k_feat, 
           [str(metric_dict[k]['feature_names']) for k in k_feat], 
           rotation=90)
plt.show()
</code></pre>
<p><img alt="png" src="../ExhaustiveFeatureSelector_files/ExhaustiveFeatureSelector_22_0.png" /></p>
<h2 id="example-3-exhaustive-feature-selection-for-regression-analysis">Example 3 - Exhaustive feature selection for regression analysis</h2>
<p>Similar to the classification examples above, the <code>SequentialFeatureSelector</code> also supports scikit-learn's estimators
for regression.</p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston.data, boston.target

lr = LinearRegression()

efs = EFS(lr, 
          min_features=10,
          max_features=12,
          scoring='neg_mean_squared_error',
          cv=10)

efs.fit(X, y)

print('Best MSE score: %.2f' % efs.best_score_ * (-1))
print('Best subset:', efs.best_idx_)
</code></pre>
<pre><code>/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.

    The Boston housing prices dataset has an ethical problem. You can refer to
    the documentation of this function for further details.

    The scikit-learn maintainers therefore strongly discourage the use of this
    dataset unless the purpose of the code is to study and educate about
    ethical issues in data science and machine learning.

    In this special case, you can fetch the dataset from the original
    source::

        import pandas as pd
        import numpy as np


        data_url = "https://lib.stat.cmu.edu/datasets/boston"
        raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
        target = raw_df.values[1::2, 2]

    Alternative datasets include the California housing dataset (i.e.
    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing
    dataset. You can load the datasets as follows::

        from sklearn.datasets import fetch_california_housing
        housing = fetch_california_housing()

    for the California housing dataset and::

        from sklearn.datasets import fetch_openml
        housing = fetch_openml(name="house_prices", as_frame=True)

    for the Ames housing dataset.

  warnings.warn(msg, category=FutureWarning)
Features: 377/377


Best subset: (0, 1, 4, 6, 7, 8, 9, 10, 11, 12)
</code></pre>
<h2 id="example-4-regression-and-adjusted-r2">Example 4 - Regression and adjusted R2</h2>
<p>As shown in Example 3, the exhaustive feature selector can be used for selecting features via a regression model. In regression analysis, there exists the common phenomenon that the <script type="math/tex">R^2</script> score can become spuriously inflated the more features we choose. Hence, and this is especially true for feature selection, it is useful to make model comparisons based on the adjusted <script type="math/tex">R^2</script> value rather than the regular <script type="math/tex">R^2</script>. The adjusted <script type="math/tex">R^2</script>, <script type="math/tex">\bar{R}^{2}</script>, accounts for the number of features and examples as follows:</p>
<p>
<script type="math/tex; mode=display">\bar{R}^{2}=1-\left(1-R^{2}\right) \frac{n-1}{n-p-1},</script>
</p>
<p>where <script type="math/tex">n</script> is the number of examples and <script type="math/tex">p</script> is the number of features.</p>
<p>One of the advantages of scikit-learn's API is that it's consistent, intuitive, and simple to use. However, one downside of this API design is that it can be a bit restrictive for certain scenarios. For instance, scikit-learn scoring function only take two inputs, the predicted and the true target values. Hence, we cannot use scikit-learn's scoring API to compute the adjusted <script type="math/tex">R^2</script>, which also requires the number of features.</p>
<p>However, as a workaround, we can compute the <script type="math/tex">R^2</script> for the different feature subsets and then do a posthoc computation to obtain the adjusted <script type="math/tex">R^2</script>.</p>
<p><strong>Step 1: Compute <script type="math/tex">R^2</script>:</strong></p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston = load_boston()
X, y = boston.data, boston.target

lr = LinearRegression()

efs = EFS(lr, 
          min_features=10,
          max_features=12,
          scoring='r2',
          cv=10)

efs.fit(X, y)

print('Best R2 score: %.2f' % efs.best_score_ * (-1))
print('Best subset:', efs.best_idx_)
</code></pre>
<pre><code>/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.

    The Boston housing prices dataset has an ethical problem. You can refer to
    the documentation of this function for further details.

    The scikit-learn maintainers therefore strongly discourage the use of this
    dataset unless the purpose of the code is to study and educate about
    ethical issues in data science and machine learning.

    In this special case, you can fetch the dataset from the original
    source::

        import pandas as pd
        import numpy as np


        data_url = "https://lib.stat.cmu.edu/datasets/boston"
        raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
        target = raw_df.values[1::2, 2]

    Alternative datasets include the California housing dataset (i.e.
    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing
    dataset. You can load the datasets as follows::

        from sklearn.datasets import fetch_california_housing
        housing = fetch_california_housing()

    for the California housing dataset and::

        from sklearn.datasets import fetch_openml
        housing = fetch_openml(name="house_prices", as_frame=True)

    for the Ames housing dataset.

  warnings.warn(msg, category=FutureWarning)
Features: 377/377


Best subset: (1, 3, 5, 6, 7, 8, 9, 10, 11, 12)
</code></pre>
<p><strong>Step 2: Compute adjusted <script type="math/tex">R^2</script>:</strong></p>
<pre><code class="language-python">def adjust_r2(r2, num_examples, num_features):
    coef = (num_examples - 1) / (num_examples - num_features - 1) 
    return 1 - (1 - r2) * coef
</code></pre>
<pre><code class="language-python">for i in efs.subsets_:
    efs.subsets_[i]['adjusted_avg_score'] = (
        adjust_r2(r2=efs.subsets_[i]['avg_score'],
                  num_examples=X.shape[0]/10,
                  num_features=len(efs.subsets_[i]['feature_idx']))
    )
</code></pre>
<p><strong>Step 3: Select best subset based on adjusted <script type="math/tex">R^2</script>:</strong></p>
<pre><code class="language-python">score = -99e10

for i in efs.subsets_:
    score = efs.subsets_[i]['adjusted_avg_score']
    if ( efs.subsets_[i]['adjusted_avg_score'] == score and
        len(efs.subsets_[i]['feature_idx']) &lt; len(efs.best_idx_) )\
      or efs.subsets_[i]['adjusted_avg_score'] &gt; score:
        efs.best_idx_ = efs.subsets_[i]['feature_idx']
</code></pre>
<pre><code class="language-python">print('Best adjusted R2 score: %.2f' % efs.best_score_ * (-1))
print('Best subset:', efs.best_idx_)
</code></pre>
<pre><code>Best subset: (1, 3, 5, 6, 7, 8, 9, 10, 11, 12)
</code></pre>
<h2 id="example-5-using-the-selected-feature-subset-for-making-new-predictions">Example 5 - Using the selected feature subset For making new predictions</h2>
<pre><code class="language-python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)

knn = KNeighborsClassifier(n_neighbors=3)
</code></pre>
<pre><code class="language-python"># Select the &quot;best&quot; three features via
# 5-fold cross-validation on the training set.

from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           cv=5)
efs1 = efs1.fit(X_train, y_train)
</code></pre>
<pre><code>Features: 15/15
</code></pre>
<pre><code class="language-python">print('Selected features:', efs1.best_idx_)
</code></pre>
<pre><code>Selected features: (2, 3)
</code></pre>
<pre><code class="language-python"># Generate the new subsets based on the selected features
# Note that the transform call is equivalent to
# X_train[:, efs1.k_feature_idx_]

X_train_efs = efs1.transform(X_train)
X_test_efs = efs1.transform(X_test)

# Fit the estimator using the new feature subset
# and make a prediction on the test data
knn.fit(X_train_efs, y_train)
y_pred = knn.predict(X_test_efs)

# Compute the accuracy of the prediction
acc = float((y_test == y_pred).sum()) / y_pred.shape[0]
print('Test set accuracy: %.2f %%' % (acc*100))
</code></pre>
<pre><code>Test set accuracy: 96.00 %
</code></pre>
<h2 id="example-6-exhaustive-feature-selection-and-gridsearch">Example 6 - Exhaustive feature selection and GridSearch</h2>
<pre><code class="language-python"># Initialize the dataset

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)
</code></pre>
<p>Use scikit-learn's <code>GridSearch</code> to tune the hyperparameters of the <code>LogisticRegression</code> estimator inside the <code>ExhaustiveFeatureSelector</code> and use it for prediction in the pipeline. <strong>Note that the <code>clone_estimator</code> attribute needs to be set to <code>False</code>.</strong></p>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

lr = LogisticRegression(multi_class='multinomial', 
                        solver='newton-cg', 
                        random_state=123)

efs1 = EFS(estimator=lr, 
           min_features=2,
           max_features=3,
           scoring='accuracy',
           print_progress=False,
           clone_estimator=False,
           cv=5,
           n_jobs=1)

pipe = make_pipeline(efs1, lr)

param_grid = {'exhaustivefeatureselector__estimator__C': [0.1, 1.0, 10.0]}

gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=2, 
                  verbose=1, 
                  refit=False)

# run gridearch
gs = gs.fit(X_train, y_train)
</code></pre>
<pre><code>Fitting 2 folds for each of 3 candidates, totalling 6 fits
</code></pre>
<p>... and the "best" parameters determined by GridSearch are ...</p>
<pre><code class="language-python">print(&quot;Best parameters via GridSearch&quot;, gs.best_params_)
</code></pre>
<pre><code>Best parameters via GridSearch {'exhaustivefeatureselector__estimator__C': 0.1}
</code></pre>
<h4 id="obtaining-the-best-k-feature-indices-after-gridsearch">Obtaining the best <em>k</em> feature indices after GridSearch</h4>
<p>If we are interested in the best <em>k</em> best feature indices via <code>SequentialFeatureSelection.best_idx_</code>, we have to initialize a <code>GridSearchCV</code> object with <code>refit=True</code>. Now, the grid search object will take the complete training dataset and the best parameters, which it found via cross-validation, to train the estimator pipeline.</p>
<pre><code class="language-python">gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=2, 
                  verbose=1, 
                  refit=True)
</code></pre>
<p>After running the grid search, we can access the individual pipeline objects of the <code>best_estimator_</code> via the <code>steps</code> attribute.</p>
<pre><code class="language-python">gs = gs.fit(X_train, y_train)
gs.best_estimator_.steps
</code></pre>
<pre><code>Fitting 2 folds for each of 3 candidates, totalling 6 fits





[('exhaustivefeatureselector',
  ExhaustiveFeatureSelector(clone_estimator=False,
                            estimator=LogisticRegression(C=0.1,
                                                         multi_class='multinomial',
                                                         random_state=123,
                                                         solver='newton-cg'),
                            feature_groups=[[0], [1], [2], [3]], max_features=3,
                            min_features=2, print_progress=False)),
 ('logisticregression',
  LogisticRegression(multi_class='multinomial', random_state=123,
                     solver='newton-cg'))]
</code></pre>
<p>Via sub-indexing, we can then obtain the best-selected feature subset:</p>
<pre><code class="language-python">print('Best features:', gs.best_estimator_.steps[0][1].best_idx_)
</code></pre>
<pre><code>Best features: (2, 3)
</code></pre>
<p>During cross-validation, this feature combination had a CV accuracy of:</p>
<pre><code class="language-python">print('Best score:', gs.best_score_)
</code></pre>
<pre><code>Best score: 0.96
</code></pre>
<pre><code class="language-python">gs.best_params_
</code></pre>
<pre><code>{'exhaustivefeatureselector__estimator__C': 0.1}
</code></pre>
<p><strong>Alternatively</strong>, if we can set the "best grid search parameters" in our pipeline manually if we ran <code>GridSearchCV</code> with <code>refit=False</code>. It should yield the same results:</p>
<pre><code class="language-python">pipe.set_params(**gs.best_params_).fit(X_train, y_train)
print('Best features:', pipe.steps[0][1].best_idx_)
</code></pre>
<pre><code>Best features: (2, 3)
</code></pre>
<h2 id="example-7-exhaustive-feature-selection-with-loocv">Example 7 - Exhaustive Feature Selection with LOOCV</h2>
<p>The <code>ExhaustiveFeatureSelector</code> is not restricted to k-fold cross-validation. You can use any type of cross-validation method that supports the general scikit-learn cross-validation API. </p>
<p>The following example illustrates the use of scikit-learn's <code>LeaveOneOut</code> cross-validation method in combination with the exhaustive feature selector.</p>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
from sklearn.model_selection import LeaveOneOut


iris = load_iris()
X = iris.data
y = iris.target

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=1,
           max_features=4,
           scoring='accuracy',
           print_progress=True,
           cv=LeaveOneOut()) ### Use cross-validation generator here

efs1 = efs1.fit(X, y)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>
<pre><code>Features: 15/15

Best accuracy score: 0.96
Best subset (indices): (3,)
Best subset (corresponding names): ('3',)
</code></pre>
<h2 id="example-8-interrupting-long-runs-for-intermediate-results">Example 8 - Interrupting Long Runs for Intermediate Results</h2>
<p>If your run is taking too long, it is possible to trigger a <code>KeyboardInterrupt</code> (e.g., ctrl+c on a Mac, or interrupting the cell in a Jupyter notebook) to obtain temporary results.</p>
<p><strong>Toy dataset</strong></p>
<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split


X, y = make_classification(
    n_samples=200000,
    n_features=6,
    n_informative=2,
    n_redundant=1,
    n_repeated=1,
    n_clusters_per_class=2,
    flip_y=0.05,
    class_sep=0.5,
    random_state=123,
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)
</code></pre>
<p><strong>Long run with interruption</strong></p>
<pre><code class="language-python">from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=10000)

efs1 = EFS(model, 
           min_features=1, 
           max_features=4,
           print_progress=True,
           scoring='accuracy')

efs1 = efs1.fit(X_train, y_train)
</code></pre>
<pre><code>Features: 56/56
</code></pre>
<p><strong>Finalizing the fit</strong></p>
<p>Note that the feature selection run hasn't finished, so certain attributes may not be available. In order to use the EFS instance, it is recommended to call <code>finalize_fit</code>, which will make EFS estimator appear as "fitted" process the temporary results:</p>
<pre><code class="language-python">efs1.finalize_fit()
</code></pre>
<pre><code class="language-python">print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
</code></pre>
<pre><code>Best accuracy score: 0.73
Best subset (indices): (1, 2)
</code></pre>
<h2 id="example-9-working-with-feature-groups">Example 9 - Working with Feature Groups</h2>
<p>Since mlxtend v0.21.0, it is possible to specify feature groups. Feature groups allow you to group certain features together, such that they are always selected as a group. This can be very useful in contexts similar to one-hot encoding -- if you want to treat the one-hot encoded feature as a single feature:</p>
<p><img alt="" src="../SequentialFeatureSelector_files/feature_groups.jpeg" /></p>
<p>In the following example, we specify sepal length and sepal width as a feature group so that they are always selected together:</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
X = iris.data
y = iris.target

X_df = pd.DataFrame(X, columns=['sepal len', 'petal len',
                                'sepal wid', 'petal wid'])
X_df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal len</th>
      <th>petal len</th>
      <th>sepal wid</th>
      <th>petal wid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

knn = KNeighborsClassifier(n_neighbors=3)

efs1 = EFS(knn, 
           min_features=2,
           max_features=2,
           scoring='accuracy',
           feature_groups=[['sepal len', 'sepal wid'], ['petal len'], ['petal wid']],
           cv=3)

efs1 = efs1.fit(X_df, y)

print('Best accuracy score: %.2f' % efs1.best_score_)
print('Best subset (indices):', efs1.best_idx_)
print('Best subset (corresponding names):', efs1.best_feature_names_)
</code></pre>
<pre><code>Features: 3/3

Best accuracy score: 0.97
Best subset (indices): (0, 2, 3)
Best subset (corresponding names): ('sepal len', 'sepal wid', 'petal wid')
</code></pre>
<p>Notice that the returned number of features is 3, since the number of <code>min_features</code> and <code>max_features</code> corresponds to the number of feature groups. I.e., we have 2 feature groups in <code>['sepal len', 'sepal wid'], ['petal wid']</code>, but it expands to 3 features.</p>
<h2 id="api">API</h2>
<p><em>ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)*</p>
<p>Exhaustive Feature Selection for Classification and Regression.
    (new in v0.4.3)</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>min_features</code> : int (default: 1)</p>
<p>Minumum number of features to select</p>
</li>
<li>
<p><code>max_features</code> : int (default: 1)</p>
<p>Maximum number of features to select. If parameter <code>feature_groups</code> is not
None, the number of features is equal to the number of feature groups, i.e.
<code>len(feature_groups)</code>. For  example, if <code>feature_groups = [[0], [1], [2, 3],
[4]]</code>, then the <code>max_features</code> value cannot exceed 4.</p>
</li>
<li>
<p><code>print_progress</code> : bool (default: True)</p>
<p>Prints progress as the number of epochs
to stderr.</p>
</li>
<li>
<p><code>scoring</code> : str, (default='accuracy')</p>
<p>Scoring metric in {accuracy, f1, precision, recall, roc_auc}
for classifiers,
{'mean_absolute_error', 'mean_squared_error',
'median_absolute_error', 'r2'} for regressors,
or a callable object or function with
signature <code>scorer(estimator, X, y)</code>.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Scikit-learn cross-validation generator or <code>int</code>.
If estimator is a classifier (or y consists of integer class labels),
stratified k-fold is performed, and regular k-fold cross-validation
otherwise.
No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
<li>
<p><code>fixed_features</code> : tuple (default: None)</p>
<p>If not <code>None</code>, the feature indices provided as a tuple will be
regarded as fixed by the feature selector. For example, if
<code>fixed_features=(1, 3, 7)</code>, the 2nd, 4th, and 8th feature are
guaranteed to be present in the solution. Note that if
<code>fixed_features</code> is not <code>None</code>, make sure that the number of
features to be selected is greater than <code>len(fixed_features)</code>.
In other words, ensure that <code>k_features &gt; len(fixed_features)</code>.</p>
</li>
<li>
<p><code>feature_groups</code> : list or None (default: None)</p>
<p>Optional argument for treating certain features as a group.
This means, the features within a group are always selected together,
never split.
For example, <code>feature_groups=[[1], [2], [3, 4, 5]]</code>
specifies 3 feature groups.In this case,
possible feature selection results with <code>k_features=2</code>
are <code>[[1], [2]</code>, <code>[[1], [3, 4, 5]]</code>, or <code>[[2], [3, 4, 5]]</code>.
Feature groups can be useful for
interpretability, for example, if features 3, 4, 5 are one-hot
encoded features.  (For  more details, please read the notes at the
bottom of this docstring).  New in mlxtend v. 0.21.0.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>best_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>best_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>best_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
exhaustive selection, where the dictionary keys are
the lengths k of these feature subsets. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v. 0.13.0.</p>
</li>
</ul>
<p><strong>Notes</strong></p>
<p>(1) If parameter <code>feature_groups</code> is not None, the
    number of features is equal to the number of feature groups, i.e.
    <code>len(feature_groups)</code>. For  example, if <code>feature_groups = [[0], [1], [2, 3],
    [4]]</code>, then the <code>max_features</code> value cannot exceed 4.</p>
<pre><code>(2) Although two or more individual features may be considered as one group
throughout the feature-selection process, it does not mean the individual
features of that group have the same impact on the outcome. For instance, in
linear regression, the coefficient of the feature 2 and 3 can be different
even if they are considered as one group in feature_groups.

(3) If both fixed_features and feature_groups are specified, ensure that each
feature group contains the fixed_features selection. E.g., for a 3-feature set
fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid;
fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid.
</code></pre>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/</p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Fit to training data and return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : bool, default=True</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : dict</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<pre><code>The method works on simple estimators as well as on nested objects
(such as :class:`~sklearn.pipeline.Pipeline`). The latter have
parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
possible to update each component of a nested object.
</code></pre>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>**params</code> : dict</p>
<p>Estimator parameters.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>self</code> : estimator instance</p>
<p>Estimator instance.</p>
</li>
</ul>
<hr>

<p><em>transform(X)</em></p>
<p>Return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2014-2023 <a href="https://sebastianraschka.com">Sebastian Raschka</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../mathjaxhelper.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
