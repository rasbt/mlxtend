<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Sebastian Raschka">
        <link rel="canonical" href="https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants) - mlxtend</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/color-brewer.min.css">
        <link href="../../../cinder/css/base.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.css" rel="stylesheet">
        <link href="../../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../cinder/css/cinder.css" rel="stylesheet">
        <link href="../../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../../../cinder/css/highlight.css" rel="stylesheet">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">mlxtend</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../../.." class="nav-link">Home</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../USER_GUIDE_INDEX/" class="dropdown-item">User Guide Index</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">classifier</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../classifier/Adaline/" class="dropdown-item">Adaline: Adaptive Linear Neuron Classifier</a>
</li>
            
<li>
    <a href="../../classifier/EnsembleVoteClassifier/" class="dropdown-item">EnsembleVoteClassifier: A majority voting classifier</a>
</li>
            
<li>
    <a href="../../classifier/LogisticRegression/" class="dropdown-item">LogisticRegression: A binary classifier</a>
</li>
            
<li>
    <a href="../../classifier/MultiLayerPerceptron/" class="dropdown-item">MultilayerPerceptron: A simple multilayer neural network</a>
</li>
            
<li>
    <a href="../../classifier/OneRClassifier/" class="dropdown-item">OneRClassifier: One Rule (OneR) method for classfication</a>
</li>
            
<li>
    <a href="../../classifier/Perceptron/" class="dropdown-item">Perceptron: A simple binary classifier</a>
</li>
            
<li>
    <a href="../../classifier/SoftmaxRegression/" class="dropdown-item">SoftmaxRegression: Multiclass version of logistic regression</a>
</li>
            
<li>
    <a href="../../classifier/StackingClassifier/" class="dropdown-item">StackingClassifier: Simple stacking</a>
</li>
            
<li>
    <a href="../../classifier/StackingCVClassifier/" class="dropdown-item">StackingCVClassifier: Stacking with cross-validation</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">cluster</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../cluster/Kmeans/" class="dropdown-item">Kmeans: k-means clustering</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">data</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../data/autompg_data/" class="dropdown-item">autompg_data: The Auto-MPG dataset for regression</a>
</li>
            
<li>
    <a href="../../data/boston_housing_data/" class="dropdown-item">boston_housing_data: The Boston housing dataset for regression</a>
</li>
            
<li>
    <a href="../../data/iris_data/" class="dropdown-item">iris_data: The 3-class iris dataset for classification</a>
</li>
            
<li>
    <a href="../../data/loadlocal_mnist/" class="dropdown-item">loadlocal_mnist: A function for loading MNIST from the original ubyte files</a>
</li>
            
<li>
    <a href="../../data/make_multiplexer_dataset/" class="dropdown-item">make_multiplexer_dataset: A function for creating multiplexer data</a>
</li>
            
<li>
    <a href="../../data/mnist_data/" class="dropdown-item">mnist_data: A subset of the MNIST dataset for classification</a>
</li>
            
<li>
    <a href="../../data/three_blobs_data/" class="dropdown-item">three_blobs_data: The synthetic blobs for classification</a>
</li>
            
<li>
    <a href="../../data/wine_data/" class="dropdown-item">wine_data: A 3-class wine dataset for classification</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">evaluate</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../evaluate/accuracy_score/" class="dropdown-item">accuracy_score: Computing standard, balanced, and per-class accuracy</a>
</li>
            
<li>
    <a href="../../evaluate/bias_variance_decomp/" class="dropdown-item">bias_variance_decomp: Bias-variance decomposition for classification and regression losses</a>
</li>
            
<li>
    <a href="../../evaluate/bootstrap/" class="dropdown-item">bootstrap: The ordinary nonparametric boostrap for arbitrary parameters</a>
</li>
            
<li>
    <a href="../../evaluate/bootstrap_point632_score/" class="dropdown-item">bootstrap_point632_score: The .632 and .632+ boostrap for classifier evaluation</a>
</li>
            
<li>
    <a href="../../evaluate/BootstrapOutOfBag/" class="dropdown-item">BootstrapOutOfBag: A scikit-learn compatible version of the out-of-bag bootstrap</a>
</li>
            
<li>
    <a href="../../evaluate/cochrans_q/" class="dropdown-item">cochrans_q: Cochran's Q test for comparing multiple classifiers</a>
</li>
            
<li>
    <a href="../../evaluate/combined_ftest_5x2cv/" class="dropdown-item">combined_ftest_5x2cv: 5x2cv combined *F* test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/confusion_matrix/" class="dropdown-item">confusion_matrix: creating a confusion matrix for model evaluation</a>
</li>
            
<li>
    <a href="../../evaluate/create_counterfactual/" class="dropdown-item">create_counterfactual: Interpreting models via counterfactuals</a>
</li>
            
<li>
    <a href="../../evaluate/feature_importance_permutation/" class="dropdown-item">feature_importance_permutation: Estimate feature importance via feature permutation.</a>
</li>
            
<li>
    <a href="../../evaluate/ftest/" class="dropdown-item">ftest: F-test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/GroupTimeSeriesSplit/" class="dropdown-item">GroupTimeSeriesSplit: A scikit-learn compatible version of the time series validation with groups</a>
</li>
            
<li>
    <a href="../../evaluate/lift_score/" class="dropdown-item">lift_score: Lift score for classification and association rule mining</a>
</li>
            
<li>
    <a href="../../evaluate/mcnemar_table/" class="dropdown-item">mcnemar_table: Contingency table for McNemar's test</a>
</li>
            
<li>
    <a href="../../evaluate/mcnemar_tables/" class="dropdown-item">mcnemar_tables: contingency tables for McNemar's test and Cochran's Q test</a>
</li>
            
<li>
    <a href="../../evaluate/mcnemar/" class="dropdown-item">mcnemar: McNemar's test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/paired_ttest_5x2cv/" class="dropdown-item">paired_ttest_5x2cv: 5x2cv paired *t* test for classifier comparisons</a>
</li>
            
<li>
    <a href="../../evaluate/paired_ttest_kfold_cv/" class="dropdown-item">paired_ttest_kfold_cv: K-fold cross-validated paired *t* test</a>
</li>
            
<li>
    <a href="../../evaluate/paired_ttest_resampled/" class="dropdown-item">paired_ttest_resample: Resampled paired *t* test</a>
</li>
            
<li>
    <a href="../../evaluate/permutation_test/" class="dropdown-item">permutation_test: Permutation test for hypothesis testing</a>
</li>
            
<li>
    <a href="../../evaluate/PredefinedHoldoutSplit/" class="dropdown-item">PredefinedHoldoutSplit: Utility for the holdout method compatible with scikit-learn</a>
</li>
            
<li>
    <a href="../../evaluate/RandomHoldoutSplit/" class="dropdown-item">RandomHoldoutSplit: split a dataset into a train and validation subset for validation</a>
</li>
            
<li>
    <a href="../../evaluate/scoring/" class="dropdown-item">scoring: computing various performance metrics</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">feature_extraction</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../feature_extraction/LinearDiscriminantAnalysis/" class="dropdown-item">LinearDiscriminantAnalysis: Linear discriminant analysis for dimensionality reduction</a>
</li>
            
<li>
    <a href="../../feature_extraction/PrincipalComponentAnalysis/" class="dropdown-item">PrincipalComponentAnalysis: Principal component analysis (PCA) for dimensionality reduction</a>
</li>
            
<li>
    <a href="../../feature_extraction/RBFKernelPCA/" class="dropdown-item">RBFKernelPCA</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">feature_selection</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../ColumnSelector/" class="dropdown-item">ColumnSelector: Scikit-learn utility function to select specific columns in a pipeline</a>
</li>
            
<li>
    <a href="../ExhaustiveFeatureSelector/" class="dropdown-item">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</a>
</li>
            
<li>
    <a href="./" class="dropdown-item active">SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants)</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">file_io</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../file_io/find_filegroups/" class="dropdown-item">find_filegroups: Find files that only differ via their file extensions</a>
</li>
            
<li>
    <a href="../../file_io/find_files/" class="dropdown-item">find_files: Find files based on substring matches</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">frequent_patterns</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../frequent_patterns/apriori/" class="dropdown-item">Apriori</a>
</li>
            
<li>
    <a href="../../frequent_patterns/association_rules/" class="dropdown-item">Association rules</a>
</li>
            
<li>
    <a href="../../frequent_patterns/fpgrowth/" class="dropdown-item">Fpgrowth</a>
</li>
            
<li>
    <a href="../../frequent_patterns/fpmax/" class="dropdown-item">Fpmax</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">image</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../image/extract_face_landmarks/" class="dropdown-item">extract_face_landmarks: extract 68 landmark features from face images</a>
</li>
            
<li>
    <a href="../../image/eyepad_align/" class="dropdown-item">EyepadAlign:  align face images based on eye location</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">math</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../math/num_combinations/" class="dropdown-item">num_combinations: combinations for creating subsequences of *k* elements</a>
</li>
            
<li>
    <a href="../../math/num_permutations/" class="dropdown-item">num_permutations: number of permutations for creating subsequences of *k* elements</a>
</li>
            
<li>
    <a href="../../math/vectorspace_dimensionality/" class="dropdown-item">vectorspace_dimensionality: compute the number of dimensions that a set of vectors spans</a>
</li>
            
<li>
    <a href="../../math/vectorspace_orthonormalization/" class="dropdown-item">vectorspace_orthonormalization: Converts a set of linearly independent vectors to a set of orthonormal basis vectors</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">plotting</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../plotting/category_scatter/" class="dropdown-item">Scategory_scatter: Create a scatterplot with categories in different colors</a>
</li>
            
<li>
    <a href="../../plotting/checkerboard_plot/" class="dropdown-item">checkerboard_plot: Create a checkerboard plot in matplotlib</a>
</li>
            
<li>
    <a href="../../plotting/plot_pca_correlation_graph/" class="dropdown-item">plot_pca_correlation_graph: plot correlations between original features and principal components</a>
</li>
            
<li>
    <a href="../../plotting/ecdf/" class="dropdown-item">ecdf: Create an empirical cumulative distribution function plot</a>
</li>
            
<li>
    <a href="../../plotting/enrichment_plot/" class="dropdown-item">enrichment_plot: create an enrichment plot for cumulative counts</a>
</li>
            
<li>
    <a href="../../plotting/heatmap/" class="dropdown-item">heatmap: Create a heatmap in matplotlib</a>
</li>
            
<li>
    <a href="../../plotting/plot_confusion_matrix/" class="dropdown-item">plot_confusion_matrix: Visualize confusion matrices</a>
</li>
            
<li>
    <a href="../../plotting/plot_decision_regions/" class="dropdown-item">plot_decision_regions: Visualize the decision regions of a classifier</a>
</li>
            
<li>
    <a href="../../plotting/plot_learning_curves/" class="dropdown-item">plot_learning_curves: Plot learning curves from training and test sets</a>
</li>
            
<li>
    <a href="../../plotting/plot_linear_regression/" class="dropdown-item">plot_linear_regression: A quick way for plotting linear regression fits</a>
</li>
            
<li>
    <a href="../../plotting/plot_sequential_feature_selection/" class="dropdown-item">plot_sequential_feature_selection: Visualize selected feature subset performances from the SequentialFeatureSelector</a>
</li>
            
<li>
    <a href="../../plotting/scatterplotmatrix/" class="dropdown-item">scatterplotmatrix: visualize datasets via a scatter plot matrix</a>
</li>
            
<li>
    <a href="../../plotting/scatter_hist/" class="dropdown-item">scatter_hist: create a scatter histogram plot</a>
</li>
            
<li>
    <a href="../../plotting/stacked_barplot/" class="dropdown-item">stacked_barplot: Plot stacked bar plots in matplotlib</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">preprocessing</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../preprocessing/CopyTransformer/" class="dropdown-item">CopyTransformer: A function that creates a copy of the input array in a scikit-learn pipeline</a>
</li>
            
<li>
    <a href="../../preprocessing/DenseTransformer/" class="dropdown-item">DenseTransformer: Transforms a sparse into a dense NumPy array, e.g., in a scikit-learn pipeline</a>
</li>
            
<li>
    <a href="../../preprocessing/MeanCenterer/" class="dropdown-item">MeanCenterer: column-based mean centering on a NumPy array</a>
</li>
            
<li>
    <a href="../../preprocessing/minmax_scaling/" class="dropdown-item">MinMaxScaling: Min-max scaling fpr pandas DataFrames and NumPy arrays</a>
</li>
            
<li>
    <a href="../../preprocessing/one-hot_encoding/" class="dropdown-item">One hot encoding</a>
</li>
            
<li>
    <a href="../../preprocessing/shuffle_arrays_unison/" class="dropdown-item">shuffle_arrays_unison: shuffle arrays in a consistent fashion</a>
</li>
            
<li>
    <a href="../../preprocessing/standardize/" class="dropdown-item">standardize: A function to standardize columns in a 2D NumPy array</a>
</li>
            
<li>
    <a href="../../preprocessing/TransactionEncoder/" class="dropdown-item">TransactionEncoder</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">regressor</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../regressor/LinearRegression/" class="dropdown-item">LinearRegression: An implementation of ordinary least-squares linear regression</a>
</li>
            
<li>
    <a href="../../regressor/StackingCVRegressor/" class="dropdown-item">StackingCVRegressor: stacking with cross-validation for regression</a>
</li>
            
<li>
    <a href="../../regressor/StackingRegressor/" class="dropdown-item">StackingRegressor: a simple stacking implementation for regression</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">text</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../text/generalize_names/" class="dropdown-item">generalize_names: convert names into a generalized format</a>
</li>
            
<li>
    <a href="../../text/generalize_names_duplcheck/" class="dropdown-item">generalize_names_duplcheck: Generalize names while preventing duplicates among different names</a>
</li>
            
<li>
    <a href="../../text/tokenizer/" class="dropdown-item">tokenizer_emoticons: tokenizers for emoticons</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#" class="dropdown-item">utils</a>
    <ul class="dropdown-menu">
            
<li>
    <a href="../../utils/Counter/" class="dropdown-item">Counter: A simple progress counter</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.classifier/" class="dropdown-item">Mlxtend.classifier</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.cluster/" class="dropdown-item">Mlxtend.cluster</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.data/" class="dropdown-item">Mlxtend.data</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.evaluate/" class="dropdown-item">Mlxtend.evaluate</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.feature_extraction/" class="dropdown-item">Mlxtend.feature extraction</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.feature_selection/" class="dropdown-item">Mlxtend.feature selection</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.file_io/" class="dropdown-item">Mlxtend.file io</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.frequent_patterns/" class="dropdown-item">Mlxtend.frequent patterns</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.image/" class="dropdown-item">Mlxtend.image</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.plotting/" class="dropdown-item">Mlxtend.plotting</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.preprocessing/" class="dropdown-item">Mlxtend.preprocessing</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.regressor/" class="dropdown-item">Mlxtend.regressor</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.text/" class="dropdown-item">Mlxtend.text</a>
</li>
                                    
<li>
    <a href="../../../api_subpackages/mlxtend.utils/" class="dropdown-item">Mlxtend.utils</a>
</li>
                                </ul>
                            </li>
                            <li class="navitem">
                                <a href="../../../installation/" class="nav-link">Installation</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../CHANGELOG/" class="dropdown-item">Release Notes</a>
</li>
                                    
<li>
    <a href="../../../Code-of-Conduct/" class="dropdown-item">Code of Conduct</a>
</li>
                                    
<li>
    <a href="../../../CONTRIBUTING/" class="dropdown-item">How To Contribute</a>
</li>
                                    
<li>
    <a href="../../../contributors/" class="dropdown-item">Contributors</a>
</li>
                                    
<li>
    <a href="../../../license/" class="dropdown-item">License</a>
</li>
                                    
<li>
    <a href="../../../cite/" class="dropdown-item">Citing Mlxtend</a>
</li>
                                    
<li>
    <a href="../../../discuss/" class="dropdown-item">Discuss</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../ExhaustiveFeatureSelector/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../file_io/find_filegroups/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/rasbt/mlxtend/tree/master/docs/sources/user_guide/feature_selection/SequentialFeatureSelector.md" class="nav-link">Edit on mlxtend
                                    </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#sequentialfeatureselector-the-popular-forward-and-backward-feature-selection-approaches-including-floating-variants" class="nav-link">SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants)</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#overview" class="nav-link">Overview</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#tutorial-videos" class="nav-link">Tutorial Videos</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#visual-illustration" class="nav-link">Visual Illustration</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#algorithmic-details" class="nav-link">Algorithmic Details</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#example-1-a-simple-sequential-forward-selection-example" class="nav-link">Example 1 - A simple Sequential Forward Selection example</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-2-toggling-between-sfs-sbs-sffs-and-sbfs" class="nav-link">Example 2 - Toggling between SFS, SBS, SFFS, and SBFS</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-3-visualizing-the-results-in-dataframes" class="nav-link">Example 3 - Visualizing the results in DataFrames</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-4-plotting-the-results" class="nav-link">Example 4 - Plotting the results</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-5-sequential-feature-selection-for-regression" class="nav-link">Example 5 - Sequential Feature Selection for Regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-6-feature-selection-with-fixed-trainvalidation-splits" class="nav-link">Example 6 -- Feature Selection with Fixed Train/Validation Splits</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-7-using-the-selected-feature-subset-for-making-new-predictions" class="nav-link">Example 7 -- Using the Selected Feature Subset For Making New Predictions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-8-sequential-feature-selection-and-gridsearch" class="nav-link">Example 8 -- Sequential Feature Selection and GridSearch</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-9-selecting-the-best-feature-combination-in-a-k-range" class="nav-link">Example 9 -- Selecting the "best"  feature combination in a k-range</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-10-using-other-cross-validation-schemes" class="nav-link">Example 10 -- Using other cross-validation schemes</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-11-interrupting-long-runs-for-intermediate-results" class="nav-link">Example 11 - Interrupting Long Runs for Intermediate Results</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-12-using-pandas-dataframes" class="nav-link">Example 12 - Using Pandas DataFrames</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-13-specifying-fixed-feature-sets" class="nav-link">Example 13 - Specifying Fixed Feature Sets</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#example-13-working-with-feature-groups" class="nav-link">Example 13 - Working with Feature Groups</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#api" class="nav-link">API</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="sequentialfeatureselector-the-popular-forward-and-backward-feature-selection-approaches-including-floating-variants">SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants)</h1>
<p>Implementation of <em>sequential feature algorithms</em> (SFAs) -- greedy search algorithms -- that have been developed as a suboptimal solution to the computationally often not feasible exhaustive search.</p>
<blockquote>
<p>from mlxtend.feature_selection import SequentialFeatureSelector</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial <em>d</em>-dimensional feature space to a <em>k</em>-dimensional feature subspace where <em>k &lt; d</em>. The motivation behind feature selection algorithms is to automatically select a subset of features most relevant to the problem. The goal of feature selection is two-fold: We want to improve the computational efficiency and reduce the model's generalization error by removing irrelevant features or noise. In addition, a wrapper approach such as sequential feature selection is advantageous if embedded feature selection -- for example, a regularization penalty like LASSO -- is not applicable.</p>
<p>In a nutshell, SFAs remove or add one feature at a time based on the classifier performance until a feature subset of the desired size <em>k</em> is reached. There are four different flavors of SFAs available via the <code>SequentialFeatureSelector</code>:</p>
<ol>
<li>Sequential Forward Selection (SFS)</li>
<li>Sequential Backward Selection (SBS)</li>
<li>Sequential Forward Floating Selection (SFFS)</li>
<li>Sequential Backward Floating Selection (SBFS)</li>
</ol>
<p>The <strong><em>floating</em></strong> variants, SFFS and SBFS, can be considered extensions to the simpler SFS and SBS algorithms. The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded) so that a larger number of feature subset combinations can be sampled. It is important to emphasize that this step is conditional and only occurs if the resulting feature subset is assessed as "better" by the criterion function after the removal (or addition) of a particular feature. Furthermore, I added an optional check to skip the conditional exclusion steps if the algorithm gets stuck in cycles.  </p>
<hr />
<p>How is this different from <em>Recursive Feature Elimination</em> (RFE)  -- e.g., as implemented in <code>sklearn.feature_selection.RFE</code>? RFE is computationally less complex using the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.</p>
<hr />
<h1 id="tutorial-videos">Tutorial Videos</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/0vCXcGJg5Bo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/KYypVSwqqHI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h1 id="visual-illustration">Visual Illustration</h1>
<p>A visual illustration of the sequential backward selection process is provided below, from the paper</p>
<ul>
<li>Joe Bemister-Buffington, Alex J. Wolf, Sebastian Raschka, and Leslie A. Kuhn (2020)
Machine Learning to Identify Flexibility Signatures of Class A GPCR Inhibition
Biomolecules 2020, 10, 454. https://www.mdpi.com/2218-273X/10/3/454#</li>
</ul>
<p><img alt="" src="../SequentialFeatureSelector_files/sbs-gpcr2020.png" /></p>
<h1 id="algorithmic-details">Algorithmic Details</h1>
<h3 id="sequential-forward-selection-sfs">Sequential Forward Selection (SFS)</h3>
<p><strong>Input:</strong> <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The <strong><em>SFS</em></strong> algorithm takes the whole <script type="math/tex">d</script>-dimensional feature set as input.</li>
</ul>
<p><strong>Output:</strong> <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>SFS returns a subset of features; the number of selected features <script type="math/tex">k</script>, where <script type="math/tex">k < d</script>, has to be specified <em>a priori</em>.</li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = \emptyset</script>, <script type="math/tex">k = 0</script>
</p>
<ul>
<li>We initialize the algorithm with an empty set <script type="math/tex">\emptyset</script> ("null set") so that <script type="math/tex">k = 0</script> (where <script type="math/tex">k</script> is the size of the subset).</li>
</ul>
<p><strong>Step 1 (Inclusion):</strong>  </p>
<p>
<script type="math/tex">x^+ = \text{ arg max } J(X_k + x), \text{ where }  x \in Y - X_k</script>
<br />
<script type="math/tex">X_{k+1} = X_k + x^+</script>
<br />
<script type="math/tex">k = k + 1</script>
<br />
<em>Go to Step 1</em> </p>
<ul>
<li>in this step, we add an additional feature, <script type="math/tex">x^+</script>, to our feature subset <script type="math/tex">X_k</script>.</li>
<li>
<script type="math/tex">x^+</script> is the feature that maximizes our criterion function, that is, the feature that is associated with the best classifier performance if it is added to <script type="math/tex">X_k</script>.</li>
<li>We repeat this procedure until the termination criterion is satisfied.</li>
</ul>
<p><strong>Termination:</strong> <script type="math/tex">k = p</script>
</p>
<ul>
<li>We add features from the feature subset <script type="math/tex">X_k</script> until the feature subset of size <script type="math/tex">k</script> contains the number of desired features <script type="math/tex">p</script> that we specified <em>a priori</em>.</li>
</ul>
<h3 id="sequential-backward-selection-sbs">Sequential Backward Selection (SBS)</h3>
<p><strong>Input:</strong> the set of all features, <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The SBS algorithm takes the whole feature set as input.</li>
</ul>
<p><strong>Output:</strong> <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>SBS returns a subset of features; the number of selected features <script type="math/tex">k</script>, where <script type="math/tex">k < d</script>, has to be specified <em>a priori</em>.</li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = Y</script>, <script type="math/tex">k = d</script>
</p>
<ul>
<li>We initialize the algorithm with the given feature set so that the <script type="math/tex">k = d</script>.</li>
</ul>
<p><strong>Step 1 (Exclusion):</strong>  </p>
<p>
<script type="math/tex">x^- = \text{ arg max } J(X_k - x), \text{  where } x \in X_k</script>
<br />
<script type="math/tex">X_{k-1} = X_k - x^-</script>
<br />
<script type="math/tex">k = k - 1</script>
<br />
<em>Go to Step 1</em>  </p>
<ul>
<li>In this step, we remove a feature, <script type="math/tex">x^-</script> from our feature subset <script type="math/tex">X_k</script>.</li>
<li>
<script type="math/tex">x^-</script> is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classifier performance if it is removed from <script type="math/tex">X_k</script>.</li>
<li>We repeat this procedure until the termination criterion is satisfied.</li>
</ul>
<p><strong>Termination:</strong> <script type="math/tex">k = p</script>
</p>
<ul>
<li>We add features from the feature subset <script type="math/tex">X_k</script> until the feature subset of size <script type="math/tex">k</script> contains the number of desired features <script type="math/tex">p</script> that we specified <em>a priori</em>.</li>
</ul>
<h3 id="sequential-backward-floating-selection-sbfs">Sequential Backward Floating Selection (SBFS)</h3>
<p><strong>Input:</strong> the set of all features, <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The SBFS algorithm takes the whole feature set as input.</li>
</ul>
<p><strong>Output:</strong> <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>SBFS returns a subset of features; the number of selected features <script type="math/tex">k</script>, where <script type="math/tex">k < d</script>, has to be specified <em>a priori</em>.</li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = Y</script>, <script type="math/tex">k = d</script>
</p>
<ul>
<li>We initialize the algorithm with the given feature set so that the <script type="math/tex">k = d</script>.</li>
</ul>
<p><strong>Step 1 (Exclusion):</strong>  </p>
<p>
<script type="math/tex">x^- = \text{ arg max } J(X_k - x), \text{  where } x \in X_k</script>
<br />
<script type="math/tex">X_{k-1} = X_k - x^-</script>
<br />
<script type="math/tex">k = k - 1</script>
<br />
<em>Go to Step 2</em>  </p>
<ul>
<li>In this step, we remove a feature, <script type="math/tex">x^-</script> from our feature subset <script type="math/tex">X_k</script>.</li>
<li>
<script type="math/tex">x^-</script> is the feature that maximizes our criterion function upon removal, that is, the feature that is associated with the best classifier performance if it is removed from <script type="math/tex">X_k</script>.</li>
</ul>
<p><strong>Step 2 (Conditional Inclusion):</strong><br />
<br>
<script type="math/tex">x^+ = \text{ arg max } J(X_k + x), \text{ where } x \in Y - X_k</script>
<br />
<em>if J(X_k + x) &gt; J(X_k)</em>:  <br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">X_{k+1} = X_k + x^+</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">k = k + 1</script>
<br />
<em>Go to Step 1</em>  </p>
<ul>
<li>In Step 2, we search for features that improve the classifier performance if they are added back to the feature subset. If such features exist, we add the feature <script type="math/tex">x^+</script> for which the performance improvement is maximized. If <script type="math/tex">k = 2</script> or an improvement cannot be made (i.e., such feature <script type="math/tex">x^+</script> cannot be found), go back to step 1; else, repeat this step.</li>
</ul>
<p><strong>Termination:</strong> <script type="math/tex">k = p</script>
</p>
<ul>
<li>We add features from the feature subset <script type="math/tex">X_k</script> until the feature subset of size <script type="math/tex">k</script> contains the number of desired features <script type="math/tex">p</script> that we specified <em>a priori</em>.</li>
</ul>
<h3 id="sequential-forward-floating-selection-sffs">Sequential Forward Floating Selection (SFFS)</h3>
<p><strong>Input:</strong> the set of all features, <script type="math/tex">Y = \{y_1, y_2, ..., y_d\}</script>
</p>
<ul>
<li>The <strong><em>SFFS</em></strong> algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions (<strong><em>d = 10</em></strong>).
<br><br></li>
</ul>
<p><strong>Output:</strong> a subset of features, <script type="math/tex">X_k = \{x_j \; | \;j = 1, 2, ..., k; \; x_j \in Y\}</script>, where <script type="math/tex">k = (0, 1, 2, ..., d)</script>
</p>
<ul>
<li>The returned output of the algorithm is a subset of the feature space of a specified size. E.g., a subset of 5 features from a 10-dimensional feature space (<strong><em>k = 5, d = 10</em></strong>).
<br><br></li>
</ul>
<p><strong>Initialization:</strong> <script type="math/tex">X_0 = \emptyset</script>, <script type="math/tex">k = 0</script>
</p>
<ul>
<li>We initialize the algorithm with an empty set ("null set") so that the <strong><em>k = 0</em></strong> (where <strong><em>k</em></strong> is the size of the subset)
<br><br></li>
</ul>
<p><strong>Step 1 (Inclusion):</strong><br />
<br>
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">x^+ = \text{ arg max } J(X_k + x), \text{ where }  x \in Y - X_k</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">X_{k+1} = X_k + x^+</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">k = k + 1</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;<em>Go to Step 2</em><br />
<br> <br>
<strong>Step 2 (Conditional Exclusion):</strong><br />
<br>
&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">x^- = \text{ arg max } J(X_k - x), \text{ where } x \in X_k</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;<script type="math/tex">if \; J(X_k - x) > J(X_k)</script>:  <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">X_{k-1} = X_k - x^- </script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <script type="math/tex">k = k - 1</script>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;<em>Go to Step 1</em>  </p>
<ul>
<li>In step 1, we include the feature from the <strong><em>feature space</em></strong> that leads to the best performance increase for our <strong><em>feature subset</em></strong> (assessed by the <strong><em>criterion function</em></strong>). Then, we go over to step 2</li>
<li>
<p>In step 2, we only remove a feature if the resulting subset would gain an increase in performance. If <script type="math/tex">k = 2</script> or an improvement cannot be made (i.e., such feature <script type="math/tex">x^+</script> cannot be found), go back to step 1; else, repeat this step.</p>
</li>
<li>
<p>Steps 1 and 2 are repeated until the <strong>Termination</strong> criterion is reached.
<br><br></p>
</li>
</ul>
<p><strong>Termination:</strong> stop when <strong><em>k</em></strong> equals the number of desired features</p>
<h3 id="references">References</h3>
<ul>
<li>
<p>Ferri, F. J., Pudil P., Hatef, M., Kittler, J. (1994). <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=sbajBQAAQBAJ&amp;oi=fnd&amp;pg=PA403&amp;dq=comparative+study+of+techniques+for+large+scale&amp;ots=KdIOYpA8wj&amp;sig=hdOsBP1HX4hcDjx4RLg_chheojc#v=onepage&amp;q=comparative%20study%20of%20techniques%20for%20large%20scale&amp;f=false"><em>"Comparative study of techniques for large-scale feature selection."</em></a> Pattern Recognition in Practice IV : 403-413.</p>
</li>
<li>
<p>Pudil, P., Novovičová, J., &amp; Kittler, J. (1994). <a href="https://www.sciencedirect.com/science/article/pii/0167865594901279"><em>"Floating search methods in feature selection."</em></a> Pattern recognition letters 15.11 (1994): 1119-1125.</p>
</li>
</ul>
<h2 id="example-1-a-simple-sequential-forward-selection-example">Example 1 - A simple Sequential Forward Selection example</h2>
<p>Initializing a simple classifier from scikit-learn:</p>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target
knn = KNeighborsClassifier(n_neighbors=4)
</code></pre>
<p>We start by selection the "best" 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we set <code>forward=True</code> and <code>floating=False</code>. By choosing <code>cv=0</code>, we don't perform any cross-validation, therefore, the performance (here: <code>'accuracy'</code>) is computed entirely on the training set. </p>
<pre><code class="language-python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           cv=0)

sfs1 = sfs1.fit(X, y)
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2022-09-06 20:51:22] Features: 1/3 -- score: 0.96[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2022-09-06 20:51:22] Features: 2/3 -- score: 0.9733333333333334[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2022-09-06 20:51:22] Features: 3/3 -- score: 0.9733333333333334
</code></pre>
<p>Via the <code>subsets_</code> attribute, we can take a look at the selected feature indices at each step:</p>
<pre><code class="language-python">sfs1.subsets_
</code></pre>
<pre><code>{1: {'feature_idx': (3,),
  'cv_scores': array([0.96]),
  'avg_score': 0.96,
  'feature_names': ('3',)},
 2: {'feature_idx': (2, 3),
  'cv_scores': array([0.97333333]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('2', '3')},
 3: {'feature_idx': (1, 2, 3),
  'cv_scores': array([0.97333333]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('1', '2', '3')}}
</code></pre>
<p>Note that the 'feature_names' entry is simply a string representation of the 'feature_idx' in this case. Optionally, we can provide custom feature names via the <code>fit</code> method's <code>custom_feature_names</code> parameter:</p>
<pre><code class="language-python">feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width')
sfs1 = sfs1.fit(X, y, custom_feature_names=feature_names)
sfs1.subsets_
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2022-09-06 20:51:22] Features: 1/3 -- score: 0.96[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2022-09-06 20:51:22] Features: 2/3 -- score: 0.9733333333333334[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2022-09-06 20:51:22] Features: 3/3 -- score: 0.9733333333333334




{1: {'feature_idx': (3,),
  'cv_scores': array([0.96]),
  'avg_score': 0.96,
  'feature_names': ('petal width',)},
 2: {'feature_idx': (2, 3),
  'cv_scores': array([0.97333333]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('petal length', 'petal width')},
 3: {'feature_idx': (1, 2, 3),
  'cv_scores': array([0.97333333]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('sepal width', 'petal length', 'petal width')}}
</code></pre>
<p>Furthermore, we can access the indices of the 3 best features directly via the <code>k_feature_idx_</code> attribute:</p>
<pre><code class="language-python">sfs1.k_feature_idx_
</code></pre>
<pre><code>(1, 2, 3)
</code></pre>
<p>And similarly, to obtain the names of these features, given that we provided an argument to the <code>custom_feature_names</code> parameter, we can refer to the <code>sfs1.k_feature_names_</code> attribute:</p>
<pre><code class="language-python">sfs1.k_feature_names_
</code></pre>
<pre><code>('sepal width', 'petal length', 'petal width')
</code></pre>
<p>Finally, the prediction score for these 3 features can be accesses via <code>k_score_</code>:</p>
<pre><code class="language-python">sfs1.k_score_
</code></pre>
<pre><code>0.9733333333333334
</code></pre>
<h2 id="example-2-toggling-between-sfs-sbs-sffs-and-sbfs">Example 2 - Toggling between SFS, SBS, SFFS, and SBFS</h2>
<p>Using the <code>forward</code> and <code>floating</code> parameters, we can toggle between SFS, SBS, SFFS, and SBFS as shown below. Note that we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via <code>n_jobs=-1</code>, we choose to run the cross-validation on all our available CPU cores.</p>
<pre><code class="language-python"># Sequential Forward Selection
sfs = SFS(knn, 
          k_features=3, 
          forward=True, 
          floating=False, 
          scoring='accuracy',
          cv=4,
          n_jobs=-1)
sfs = sfs.fit(X, y)

print('\nSequential Forward Selection (k=3):')
print(sfs.k_feature_idx_)
print('CV Score:')
print(sfs.k_score_)

###################################################

# Sequential Backward Selection
sbs = SFS(knn, 
          k_features=3, 
          forward=False, 
          floating=False, 
          scoring='accuracy',
          cv=4,
          n_jobs=-1)
sbs = sbs.fit(X, y)

print('\nSequential Backward Selection (k=3):')
print(sbs.k_feature_idx_)
print('CV Score:')
print(sbs.k_score_)

###################################################

# Sequential Forward Floating Selection
sffs = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=True, 
           scoring='accuracy',
           cv=4,
           n_jobs=-1)
sffs = sffs.fit(X, y)

print('\nSequential Forward Floating Selection (k=3):')
print(sffs.k_feature_idx_)
print('CV Score:')
print(sffs.k_score_)

###################################################

# Sequential Backward Floating Selection
sbfs = SFS(knn, 
           k_features=3, 
           forward=False, 
           floating=True, 
           scoring='accuracy',
           cv=4,
           n_jobs=-1)
sbfs = sbfs.fit(X, y)

print('\nSequential Backward Floating Selection (k=3):')
print(sbfs.k_feature_idx_)
print('CV Score:')
print(sbfs.k_score_)
</code></pre>
<pre><code>/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"



Sequential Forward Selection (k=3):
(1, 2, 3)
CV Score:
0.9731507823613088

Sequential Backward Selection (k=3):
(1, 2, 3)
CV Score:
0.9731507823613088

Sequential Forward Floating Selection (k=3):
(1, 2, 3)
CV Score:
0.9731507823613088


/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"



Sequential Backward Floating Selection (k=3):
(1, 2, 3)
CV Score:
0.9731507823613088
</code></pre>
<p>In this simple scenario, selecting the best 3 features out of the 4 available features in the Iris set, we end up with similar results regardless of which sequential selection algorithms we used.</p>
<h2 id="example-3-visualizing-the-results-in-dataframes">Example 3 - Visualizing the results in DataFrames</h2>
<p>For our convenience, we can visualize the output from the feature selection in a pandas DataFrame format using the <code>get_metric_dict</code> method of the SequentialFeatureSelector object. The columns <code>std_dev</code> and <code>std_err</code> represent the standard deviation and standard errors of the cross-validation scores, respectively.</p>
<p>Below, we see the DataFrame of the Sequential Forward Selector from Example 2:</p>
<pre><code class="language-python">import pandas as pd
pd.DataFrame.from_dict(sfs.get_metric_dict()).T
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_idx</th>
      <th>cv_scores</th>
      <th>avg_score</th>
      <th>feature_names</th>
      <th>ci_bound</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>(3,)</td>
      <td>[0.9736842105263158, 0.9473684210526315, 0.918...</td>
      <td>0.959993</td>
      <td>(3,)</td>
      <td>0.048319</td>
      <td>0.030143</td>
      <td>0.017403</td>
    </tr>
    <tr>
      <th>2</th>
      <td>(2, 3)</td>
      <td>[0.9736842105263158, 0.9473684210526315, 0.918...</td>
      <td>0.959993</td>
      <td>(2, 3)</td>
      <td>0.048319</td>
      <td>0.030143</td>
      <td>0.017403</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(1, 2, 3)</td>
      <td>[0.9736842105263158, 1.0, 0.9459459459459459, ...</td>
      <td>0.973151</td>
      <td>(1, 2, 3)</td>
      <td>0.030639</td>
      <td>0.019113</td>
      <td>0.011035</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now, let's compare it to the Sequential Backward Selector:</p>
<pre><code class="language-python">pd.DataFrame.from_dict(sbs.get_metric_dict()).T
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_idx</th>
      <th>cv_scores</th>
      <th>avg_score</th>
      <th>feature_names</th>
      <th>ci_bound</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4</th>
      <td>(0, 1, 2, 3)</td>
      <td>[0.9736842105263158, 0.9473684210526315, 0.918...</td>
      <td>0.953236</td>
      <td>(0, 1, 2, 3)</td>
      <td>0.03602</td>
      <td>0.022471</td>
      <td>0.012974</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(1, 2, 3)</td>
      <td>[0.9736842105263158, 1.0, 0.9459459459459459, ...</td>
      <td>0.973151</td>
      <td>(1, 2, 3)</td>
      <td>0.030639</td>
      <td>0.019113</td>
      <td>0.011035</td>
    </tr>
  </tbody>
</table>
</div>

<p>We can see that both SFS and SBFS found the same "best" 3 features, however, the intermediate steps where obviously different.</p>
<p>The <code>ci_bound</code> column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the <code>confidence_interval</code> parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:</p>
<pre><code class="language-python">pd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_idx</th>
      <th>cv_scores</th>
      <th>avg_score</th>
      <th>feature_names</th>
      <th>ci_bound</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4</th>
      <td>(0, 1, 2, 3)</td>
      <td>[0.9736842105263158, 0.9473684210526315, 0.918...</td>
      <td>0.953236</td>
      <td>(0, 1, 2, 3)</td>
      <td>0.027658</td>
      <td>0.022471</td>
      <td>0.012974</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(1, 2, 3)</td>
      <td>[0.9736842105263158, 1.0, 0.9459459459459459, ...</td>
      <td>0.973151</td>
      <td>(1, 2, 3)</td>
      <td>0.023525</td>
      <td>0.019113</td>
      <td>0.011035</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="example-4-plotting-the-results">Example 4 - Plotting the results</h2>
<p>After importing the little helper function <a href="../../plotting/plot_sequential_feature_selection/"><code>plotting.plot_sequential_feature_selection</code></a>, we can also visualize the results using matplotlib figures.</p>
<pre><code class="language-python">from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt

sfs = SFS(knn, 
          k_features=4, 
          forward=True, 
          floating=False, 
          scoring='accuracy',
          verbose=2,
          cv=5)

sfs = sfs.fit(X, y)

fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')

plt.ylim([0.8, 1])
plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2022-09-06 20:51:24] Features: 1/4 -- score: 0.96[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2022-09-06 20:51:24] Features: 2/4 -- score: 0.9666666666666668[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2022-09-06 20:51:24] Features: 3/4 -- score: 0.9533333333333334[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished

[2022-09-06 20:51:24] Features: 4/4 -- score: 0.9733333333333334
</code></pre>
<p><img alt="png" src="../SequentialFeatureSelector_files/SequentialFeatureSelector_49_1.png" /></p>
<h2 id="example-5-sequential-feature-selection-for-regression">Example 5 - Sequential Feature Selection for Regression</h2>
<p>Similar to the classification examples above, the <code>SequentialFeatureSelector</code> also supports scikit-learn's estimators
for regression.</p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
X, y = data.data, data.target

lr = LinearRegression()

sfs = SFS(lr, 
          k_features=8, 
          forward=True, 
          floating=False, 
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(X, y)
fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')

plt.title('Sequential Forward Selection (w. StdErr)')
plt.grid()
plt.show()
</code></pre>
<p><img alt="png" src="../SequentialFeatureSelector_files/SequentialFeatureSelector_52_0.png" /></p>
<h2 id="example-6-feature-selection-with-fixed-trainvalidation-splits">Example 6 -- Feature Selection with Fixed Train/Validation Splits</h2>
<p>If you do not wish to use cross-validation (here: k-fold cross-validation, i.e., rotating training and validation folds), you can use the <code>PredefinedHoldoutSplit</code> class to specify your own, fixed training and validation split.</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
from mlxtend.evaluate import PredefinedHoldoutSplit
import numpy as np


iris = load_iris()
X = iris.data
y = iris.target

rng = np.random.RandomState(123)
my_validation_indices = rng.permutation(np.arange(150))[:30]
print(my_validation_indices)
</code></pre>
<pre><code>[ 72 112 132  88  37 138  87  42   8  90 141  33  59 116 135 104  36  13
  63  45  28 133  24 127  46  20  31 121 117   4]
</code></pre>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as SFS



knn = KNeighborsClassifier(n_neighbors=4)
piter = PredefinedHoldoutSplit(my_validation_indices)

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           cv=piter)

sfs1 = sfs1.fit(X, y)
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished

[2022-09-06 20:51:25] Features: 1/3 -- score: 0.9666666666666667[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished

[2022-09-06 20:51:25] Features: 2/3 -- score: 0.9666666666666667[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2022-09-06 20:51:25] Features: 3/3 -- score: 0.9666666666666667
</code></pre>
<h2 id="example-7-using-the-selected-feature-subset-for-making-new-predictions">Example 7 -- Using the Selected Feature Subset For Making New Predictions</h2>
<pre><code class="language-python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.33, random_state=1)

knn = KNeighborsClassifier(n_neighbors=4)
</code></pre>
<pre><code class="language-python"># Select the &quot;best&quot; three features via
# 5-fold cross-validation on the training set.

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=5)
sfs1 = sfs1.fit(X_train, y_train)
</code></pre>
<pre><code class="language-python">print('Selected features:', sfs1.k_feature_idx_)
</code></pre>
<pre><code>Selected features: (1, 2, 3)
</code></pre>
<pre><code class="language-python"># Generate the new subsets based on the selected features
# Note that the transform call is equivalent to
# X_train[:, sfs1.k_feature_idx_]

X_train_sfs = sfs1.transform(X_train)
X_test_sfs = sfs1.transform(X_test)

# Fit the estimator using the new feature subset
# and make a prediction on the test data
knn.fit(X_train_sfs, y_train)
y_pred = knn.predict(X_test_sfs)

# Compute the accuracy of the prediction
acc = float((y_test == y_pred).sum()) / y_pred.shape[0]
print('Test set accuracy: %.2f %%' % (acc * 100))
</code></pre>
<pre><code>Test set accuracy: 96.00 %
</code></pre>
<h2 id="example-8-sequential-feature-selection-and-gridsearch">Example 8 -- Sequential Feature Selection and GridSearch</h2>
<p>In the following example, we are tuning the SFS's estimator using GridSearch. To avoid unwanted behavior or side-effects, it's recommended to use the estimator inside and outside of SFS as separate instances.</p>
<pre><code class="language-python"># Initialize the dataset

from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=0.2, random_state=123)
</code></pre>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import mlxtend

knn1 = KNeighborsClassifier()
knn2 = KNeighborsClassifier()

sfs1 = SFS(estimator=knn1, 
           k_features=3,
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=5)

pipe = Pipeline([('sfs', sfs1), 
                 ('knn2', knn2)])

param_grid = {
    'sfs__k_features': [1, 2, 3],
    'sfs__estimator__n_neighbors': [3, 4, 7], # inner knn
    'knn2__n_neighbors': [3, 4, 7] # outer knn
  }

gs = GridSearchCV(estimator=pipe, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  n_jobs=1, 
                  cv=5,
                  refit=False)

# run gridearch
gs = gs.fit(X_train, y_train)
</code></pre>
<p>Let's take a look at the suggested hyperparameters below:</p>
<p>for i in range(len(gs.cv_results_['params'])):
    print(gs.cv_results_['params'][i], 'test acc.:', gs.cv_results_['mean_test_score'][i])</p>
<p>The "best" parameters determined by GridSearch are ...</p>
<pre><code class="language-python">print(&quot;Best parameters via GridSearch&quot;, gs.best_params_)
</code></pre>
<pre><code>Best parameters via GridSearch {'knn2__n_neighbors': 7, 'sfs__estimator__n_neighbors': 3, 'sfs__k_features': 3}
</code></pre>
<pre><code class="language-python">pipe.set_params(**gs.best_params_).fit(X_train, y_train)
</code></pre>
<pre><code>Pipeline(steps=[('sfs',
                 SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),
                                           k_features=3, scoring='accuracy')),
                ('knn2', KNeighborsClassifier(n_neighbors=7))])
</code></pre>
<h2 id="example-9-selecting-the-best-feature-combination-in-a-k-range">Example 9 -- Selecting the "best"  feature combination in a k-range</h2>
<p>If <code>k_features</code> is set to to a tuple <code>(min_k, max_k)</code> (new in 0.4.2), the SFS will now select the best feature combination that it discovered by iterating from <code>k=1</code> to <code>max_k</code> (forward), or <code>max_k</code> to <code>min_k</code> (backward). The size of the returned feature subset is then within <code>max_k</code> to <code>min_k</code>, depending on which combination scored best during cross validation.</p>
<pre><code class="language-python">X.shape
</code></pre>
<pre><code>(150, 4)
</code></pre>
<pre><code class="language-python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier
from mlxtend.data import wine_data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

X, y = wine_data()
X_train, X_test, y_train, y_test= train_test_split(X, y, 
                                                   stratify=y,
                                                   test_size=0.3,
                                                   random_state=1)

knn = KNeighborsClassifier(n_neighbors=2)

sfs1 = SFS(estimator=knn, 
           k_features=(3, 10),
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=5)

pipe = make_pipeline(StandardScaler(), sfs1)

pipe.fit(X_train, y_train)

print('best combination (ACC: %.3f): %s\n' % (sfs1.k_score_, sfs1.k_feature_idx_))
print('all subsets:\n', sfs1.subsets_)
plot_sfs(sfs1.get_metric_dict(), kind='std_err');
</code></pre>
<pre><code>best combination (ACC: 0.992): (0, 1, 2, 3, 6, 8, 9, 10, 11, 12)

all subsets:
 {1: {'feature_idx': (6,), 'cv_scores': array([0.84 , 0.64 , 0.84 , 0.8  , 0.875]), 'avg_score': 0.799, 'feature_names': ('6',)}, 2: {'feature_idx': (6, 9), 'cv_scores': array([0.92      , 0.88      , 1.        , 0.96      , 0.91666667]), 'avg_score': 0.9353333333333333, 'feature_names': ('6', '9')}, 3: {'feature_idx': (6, 9, 12), 'cv_scores': array([0.92      , 0.92      , 0.96      , 1.        , 0.95833333]), 'avg_score': 0.9516666666666665, 'feature_names': ('6', '9', '12')}, 4: {'feature_idx': (3, 6, 9, 12), 'cv_scores': array([0.96      , 0.96      , 0.96      , 1.        , 0.95833333]), 'avg_score': 0.9676666666666666, 'feature_names': ('3', '6', '9', '12')}, 5: {'feature_idx': (3, 6, 9, 10, 12), 'cv_scores': array([0.92, 0.96, 1.  , 1.  , 1.  ]), 'avg_score': 0.976, 'feature_names': ('3', '6', '9', '10', '12')}, 6: {'feature_idx': (2, 3, 6, 9, 10, 12), 'cv_scores': array([0.92, 0.96, 1.  , 0.96, 1.  ]), 'avg_score': 0.968, 'feature_names': ('2', '3', '6', '9', '10', '12')}, 7: {'feature_idx': (0, 2, 3, 6, 9, 10, 12), 'cv_scores': array([0.92, 0.92, 1.  , 1.  , 1.  ]), 'avg_score': 0.968, 'feature_names': ('0', '2', '3', '6', '9', '10', '12')}, 8: {'feature_idx': (0, 2, 3, 6, 8, 9, 10, 12), 'cv_scores': array([1.  , 0.92, 1.  , 1.  , 1.  ]), 'avg_score': 0.984, 'feature_names': ('0', '2', '3', '6', '8', '9', '10', '12')}, 9: {'feature_idx': (0, 2, 3, 6, 8, 9, 10, 11, 12), 'cv_scores': array([1.  , 0.92, 1.  , 1.  , 1.  ]), 'avg_score': 0.984, 'feature_names': ('0', '2', '3', '6', '8', '9', '10', '11', '12')}, 10: {'feature_idx': (0, 1, 2, 3, 6, 8, 9, 10, 11, 12), 'cv_scores': array([1.  , 0.96, 1.  , 1.  , 1.  ]), 'avg_score': 0.992, 'feature_names': ('0', '1', '2', '3', '6', '8', '9', '10', '11', '12')}}
</code></pre>
<p><img alt="png" src="../SequentialFeatureSelector_files/SequentialFeatureSelector_74_1.png" /></p>
<h2 id="example-10-using-other-cross-validation-schemes">Example 10 -- Using other cross-validation schemes</h2>
<p>In addition to standard k-fold and stratified k-fold, other cross validation schemes can be used with <code>SequentialFeatureSelector</code>. For example, <code>GroupKFold</code> or <code>LeaveOneOut</code> cross-validation from scikit-learn. </p>
<h4 id="using-groupkfold-with-sequentialfeatureselector">Using GroupKFold with SequentialFeatureSelector</h4>
<pre><code class="language-python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.neighbors import KNeighborsClassifier
from mlxtend.data import iris_data
from sklearn.model_selection import GroupKFold
import numpy as np

X, y = iris_data()
groups = np.arange(len(y)) // 10
print('groups: {}'.format(groups))
</code></pre>
<pre><code>groups: [ 0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  2  2  2  2
  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4
  4  4  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  7  7
  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9
  9  9  9  9 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11
 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 14 14 14 14
 14 14 14 14 14 14]
</code></pre>
<p>Calling the <code>split()</code> method of a scikit-learn cross-validator object will return a generator that yields train, test splits.</p>
<pre><code class="language-python">cv_gen = GroupKFold(4).split(X, y, groups)
cv_gen
</code></pre>
<pre><code>&lt;generator object _BaseKFold.split at 0x2877b27b0&gt;
</code></pre>
<p>The <code>cv</code> parameter of <code>SequentialFeatureSelector</code> must be either an <code>int</code> or an iterable yielding train, test splits. This iterable can be constructed by passing the train, test split generator to the built-in <code>list()</code> function. </p>
<pre><code class="language-python">cv = list(cv_gen)
</code></pre>
<pre><code class="language-python">knn = KNeighborsClassifier(n_neighbors=2)
sfs = SFS(estimator=knn, 
          k_features=2,
          scoring='accuracy',
          cv=cv)

sfs.fit(X, y)

print('best combination (ACC: %.3f): %s\n' % (sfs.k_score_, sfs.k_feature_idx_))
</code></pre>
<pre><code>best combination (ACC: 0.940): (2, 3)
</code></pre>
<h2 id="example-11-interrupting-long-runs-for-intermediate-results">Example 11 - Interrupting Long Runs for Intermediate Results</h2>
<p>If your run is taking too long, it is possible to trigger a <code>KeyboardInterrupt</code> (e.g., ctrl+c on a Mac, or interrupting the cell in a Jupyter notebook) to obtain temporary results.</p>
<p><strong>Toy dataset</strong></p>
<pre><code class="language-python">from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split


X, y = make_classification(
    n_samples=20000,
    n_features=500,
    n_informative=10,
    n_redundant=40,
    n_repeated=25,
    n_clusters_per_class=5,
    flip_y=0.05,
    class_sep=0.5,
    random_state=123,
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)
</code></pre>
<pre><code>/Users/sebastianraschka/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.3
  warnings.warn(f"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}"
</code></pre>
<p><strong>Long run with interruption</strong></p>
<pre><code class="language-python">from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

sfs1 = SFS(model, 
           k_features=10, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           cv=5)

sfs1 = sfs1.fit(X_train, y_train)
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    7.8s finished

[2022-09-13 21:10:39] Features: 1/10 -- score: 0.5965[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s
[Parallel(n_jobs=1)]: Done 499 out of 499 | elapsed:   25.5s finished

[2022-09-13 21:11:04] Features: 2/10 -- score: 0.6256875000000001[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s

STOPPING EARLY DUE TO KEYBOARD INTERRUPT...
</code></pre>
<p><strong>Finalizing the fit</strong></p>
<p>Note that the feature selection run hasn't finished, so certain attributes may not be available. In order to use the SFS instance, it is recommended to call <code>finalize_fit</code>, which will make SFS estimator appear as "fitted" process the temporary results:</p>
<pre><code class="language-python">sfs1.finalize_fit()
</code></pre>
<pre><code class="language-python">print(sfs1.k_feature_idx_)
print(sfs1.k_score_)
</code></pre>
<pre><code>(128, 160)
0.6256875000000001
</code></pre>
<h2 id="example-12-using-pandas-dataframes">Example 12 - Using Pandas DataFrames</h2>
<p>Optionally, we can also use pandas DataFrames and pandas Series as input to the <code>fit</code> function. In this case, the column names of the pandas DataFrame will be used as feature names. However, note that if <code>custom_feature_names</code> are provided in the fit function, these <code>custom_feature_names</code> take precedence over the DataFrame column-based feature names.</p>
<pre><code class="language-python">import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from mlxtend.feature_selection import SequentialFeatureSelector as SFS


iris = load_iris()
X = iris.data
y = iris.target
knn = KNeighborsClassifier(n_neighbors=4)

sfs1 = SFS(knn, 
           k_features=3, 
           forward=True, 
           floating=False, 
           scoring='accuracy',
           cv=0)
</code></pre>
<pre><code class="language-python">X_df = pd.DataFrame(X, columns=['sepal len', 'petal len',
                                'sepal width', 'petal width'])
X_df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal len</th>
      <th>petal len</th>
      <th>sepal width</th>
      <th>petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<p>Also, the target array, <code>y</code>, can be optionally be cast as a Series:</p>
<pre><code class="language-python">y_series = pd.Series(y)
y_series.head()
</code></pre>
<pre><code>0    0
1    0
2    0
3    0
4    0
dtype: int64
</code></pre>
<pre><code class="language-python">sfs1 = sfs1.fit(X_df, y_series)
</code></pre>
<p>Note that the only difference of passing a pandas DataFrame as input is that the sfs1.subsets_ array will now contain a new column, </p>
<pre><code class="language-python">sfs1.subsets_
</code></pre>
<pre><code>{1: {'feature_idx': (3,),
  'cv_scores': array([0.96]),
  'avg_score': 0.96,
  'feature_names': ('petal width',)},
 2: {'feature_idx': (2, 3),
  'cv_scores': array([0.97333333]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('sepal width', 'petal width')},
 3: {'feature_idx': (1, 2, 3),
  'cv_scores': array([0.97333333]),
  'avg_score': 0.9733333333333334,
  'feature_names': ('petal len', 'sepal width', 'petal width')}}
</code></pre>
<p>In mlxtend version &gt;= 0.13 pandas DataFrames are supported as feature inputs to the <code>SequentianFeatureSelector</code> instead of NumPy arrays or other NumPy-like array types.</p>
<h2 id="example-13-specifying-fixed-feature-sets">Example 13 - Specifying Fixed Feature Sets</h2>
<p>Often, it may be useful to specify a fixed set of features we want to use for a given model (e.g., determined by prior knowledge or domain knowledge). Since MLxtend v 0.18.0, it is now possible to specify such features via the <code>fixed_features</code> attribute. This will mean that these features are guaranteed to be included in the selected subsets.</p>
<p>Note that this feature works for all options regarding forward and backward selection, and using floating selection or not.</p>
<p>The example below illustrates how we can set the features 0 and 2 in the dataset as fixed:</p>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target
knn = KNeighborsClassifier(n_neighbors=3)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(knn, 
           k_features=4, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           fixed_features=(0, 2),
           cv=3)

sfs1 = sfs1.fit(X, y)
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2022-09-13 21:17:21] Features: 3/4 -- score: 0.9733333333333333[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished

[2022-09-13 21:17:21] Features: 4/4 -- score: 0.9733333333333333
</code></pre>
<pre><code class="language-python">sfs1.subsets_
</code></pre>
<pre><code>{2: {'feature_idx': (0, 2),
  'cv_scores': array([0.98, 0.92, 0.94]),
  'avg_score': 0.9466666666666667,
  'feature_names': ('0', '2')},
 3: {'feature_idx': (0, 2, 3),
  'cv_scores': array([0.98, 0.96, 0.98]),
  'avg_score': 0.9733333333333333,
  'feature_names': ('0', '2', '3')},
 4: {'feature_idx': (0, 1, 2, 3),
  'cv_scores': array([0.98, 0.96, 0.98]),
  'avg_score': 0.9733333333333333,
  'feature_names': ('0', '1', '2', '3')}}
</code></pre>
<p>If the input dataset is a pandas DataFrame, we can also use the column names directly:</p>
<pre><code class="language-python">import pandas as pd
</code></pre>
<pre><code class="language-python">X_df = pd.DataFrame(X, columns=['sepal len', 'petal len',
                                'sepal width', 'petal width'])
X_df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal len</th>
      <th>petal len</th>
      <th>sepal width</th>
      <th>petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">sfs2 = SFS(knn, 
           k_features=4, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           fixed_features=('sepal len', 'petal len'),
           cv=3)

sfs2 = sfs2.fit(X_df, y_series)
</code></pre>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished

[2022-09-13 21:17:25] Features: 3/4 -- score: 0.9466666666666667[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished

[2022-09-13 21:17:25] Features: 4/4 -- score: 0.9733333333333333
</code></pre>
<pre><code class="language-python">sfs2.subsets_
</code></pre>
<pre><code>{2: {'feature_idx': (0, 1),
  'cv_scores': array([0.72, 0.74, 0.78]),
  'avg_score': 0.7466666666666667,
  'feature_names': ('sepal len', 'petal len')},
 3: {'feature_idx': (0, 1, 2),
  'cv_scores': array([0.98, 0.92, 0.94]),
  'avg_score': 0.9466666666666667,
  'feature_names': ('sepal len', 'petal len', 'sepal width')},
 4: {'feature_idx': (0, 1, 2, 3),
  'cv_scores': array([0.98, 0.96, 0.98]),
  'avg_score': 0.9733333333333333,
  'feature_names': ('sepal len', 'petal len', 'sepal width', 'petal width')}}
</code></pre>
<h2 id="example-13-working-with-feature-groups">Example 13 - Working with Feature Groups</h2>
<p>Since mlxtend v0.21.0, it is possible to specify feature groups. Feature groups allow you to group certain features together, such that they are always selected as a group. This can be very useful in contexts similar to one-hot encoding -- if you want to treat the one-hot encoded feature as a single feature:</p>
<p><img alt="" src="../SequentialFeatureSelector_files/feature_groups.jpeg" /></p>
<p>In the following example, we specify sepal length and sepal width as a feature group so that they are always selected together:</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
X = iris.data
y = iris.target

X_df = pd.DataFrame(X, columns=['sepal len', 'petal len',
                                'sepal wid', 'petal wid'])
X_df.head()

</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal len</th>
      <th>petal len</th>
      <th>sepal wid</th>
      <th>petal wid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

knn = KNeighborsClassifier(n_neighbors=3)

sfs1 = SFS(knn, 
           k_features=2, 
           scoring='accuracy',
           feature_groups=(['sepal len', 'sepal wid'], ['petal len'], ['petal wid']),
           cv=3)

sfs1 = sfs1.fit(X_df, y)
</code></pre>
<pre><code class="language-python">sfs1 = SFS(knn, 
           k_features=2, 
           scoring='accuracy',
           feature_groups=[[0, 2], [1], [3]],
           cv=3)

sfs1 = sfs1.fit(X, y)
</code></pre>
<h1 id="api">API</h1>
<p><em>SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)*</p>
<p>Sequential Feature Selection for Classification and Regression.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>k_features</code> : int or tuple or str (default: 1)</p>
<p>Number of features to select,
where k_features &lt; the full feature set.
New in 0.4.2: A tuple containing a min and max value can be provided,
and the SFS will consider return any feature combination between
min and max that scored highest in cross-validation. For example,
the tuple (1, 4) will return any combination from
1 up to 4 features instead of a fixed number of features k.
New in 0.8.0: A string argument "best" or "parsimonious".
If "best" is provided, the feature selector will return the
feature subset with the best cross-validation performance.
If "parsimonious" is provided as an argument, the smallest
feature subset that is within one standard error of the
cross-validation performance will be selected.</p>
</li>
<li>
<p><code>forward</code> : bool (default: True)</p>
<p>Forward selection if True,
backward selection otherwise</p>
</li>
<li>
<p><code>floating</code> : bool (default: False)</p>
<p>Adds a conditional exclusion/inclusion if True.</p>
</li>
<li>
<p><code>verbose</code> : int (default: 0), level of verbosity to use in logging.</p>
<p>If 0, no output,
if 1 number of features in current set, if 2 detailed logging i
ncluding timestamp and cv scores at step.</p>
</li>
<li>
<p><code>scoring</code> : str, callable, or None (default: None)</p>
<p>If None (default), uses 'accuracy' for sklearn classifiers
and 'r2' for sklearn regressors.
If str, uses a sklearn scoring metric string identifier, for example
{accuracy, f1, precision, recall, roc_auc} for classifiers,
{'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',
'median_absolute_error', 'r2'} for regressors.
If a callable object or function is provided, it has to be conform with
sklearn's signature <code>scorer(estimator, X, y)</code>; see
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
for more information.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Integer or iterable yielding train, test splits. If cv is an integer
and <code>estimator</code> is a classifier (or y consists of integer class
labels) stratified k-fold. Otherwise regular k-fold cross-validation
is performed. No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
<li>
<p><code>fixed_features</code> : tuple (default: None)</p>
<p>If not <code>None</code>, the feature indices provided as a tuple will be
regarded as fixed by the feature selector. For example, if
<code>fixed_features=(1, 3, 7)</code>, the 2nd, 4th, and 8th feature are
guaranteed to be present in the solution. Note that if
<code>fixed_features</code> is not <code>None</code>, make sure that the number of
features to be selected is greater than <code>len(fixed_features)</code>.
In other words, ensure that <code>k_features &gt; len(fixed_features)</code>.
New in mlxtend v. 0.18.0.</p>
</li>
<li>
<p><code>feature_groups</code> : list or None (default: None)</p>
<p>Optional argument for treating certain features as a group.
This means, the features within a group are always selected together,
never split.
For example, <code>feature_groups=[[1], [2], [3, 4, 5]]</code>
specifies 3 feature groups.In this case,
possible feature selection results with <code>k_features=2</code>
are <code>[[1], [2]</code>, <code>[[1], [3, 4, 5]]</code>, or <code>[[2], [3, 4, 5]]</code>.
Feature groups can be useful for
interpretability, for example, if features 3, 4, 5 are one-hot
encoded features.  (For  more details, please read the notes at the
bottom of this docstring).  New in mlxtend v. 0.21.0.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>k_feature_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>k_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>k_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
sequential selection, where the dictionary keys are
the lengths k of these feature subsets. If the parameter
<code>feature_groups</code> is not None, the value of key indicates
the number of groups that are selected together. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v 0.13.0.</p>
</li>
</ul>
<p><strong>Notes</strong></p>
<p>(1) If parameter <code>feature_groups</code> is not None, the
    number of features is equal to the number of feature groups, i.e.
    <code>len(feature_groups)</code>. For  example, if <code>feature_groups = [[0], [1], [2, 3],
    [4]]</code>, then the <code>max_features</code> value cannot exceed 4.</p>
<pre><code>(2) Although two or more individual features may be considered as one group
throughout the feature-selection process, it does not mean the individual
features of that group have the same impact on the outcome. For instance, in
linear regression, the coefficient of the feature 2 and 3 can be different
even if they are considered as one group in feature_groups.

(3) If both fixed_features and feature_groups are specified, ensure that each
feature group contains the fixed_features selection. E.g., for a 3-feature set
fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid;
fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid.
</code></pre>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/</p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y, custom_feature_names=None, groups=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for y.</p>
</li>
<li>
<p><code>custom_feature_names</code> : None or tuple (default: tuple)</p>
<p>Custom feature names for <code>self.k_feature_names</code> and
<code>self.subsets_[i]['feature_names']</code>.
(new in v 0.13.0)</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : various, optional</p>
<p>Additional parameters that are being passed to the estimator.
For example, <code>sample_weights=weights</code>.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Fit to training data then reduce X to its most important features.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.
New in v 0.13.0: a pandas Series are now also accepted as
argument for y.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : various, optional</p>
<p>Additional parameters that are being passed to the estimator.
For example, <code>sample_weights=weights</code>.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Reduced feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : bool, default=True</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : dict</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
    Valid parameter keys can be listed with <code>get_params()</code>.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Reduce X to its most important features.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Reduced feature subset of X, shape={n_samples, k_features}</p>
<h3 id="properties">Properties</h3>
<hr>

<p><em>named_estimators</em></p>
<p><strong>Returns</strong></p>
<p>List of named estimator tuples, like [('svc', SVC(...))]</p>
<p>ython</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2014-2023 <a href="https://sebastianraschka.com">Sebastian Raschka</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../mathjaxhelper.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
